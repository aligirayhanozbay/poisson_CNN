{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from Poisson_CNN import *\n",
    "from generate_cholesky_soln import generate_dataset\n",
    "from generate_laplace_soln import generate_random_boundaries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_nn_params = {'x_output_resolution' : 64}\n",
    "mod = Poisson_CNN(n_quadpts = 27, bc_nn_parameters = bc_nn_params, mse_component_weight = 1e+1, bc_nn_weights = None, homogeneous_poisson_nn_weights = 'Homogeneous_Poisson_NN_2.h5', bc_nn_trainable=True, homogeneous_poisson_nn_trainable=False)\n",
    "from IPython.display import clear_output\n",
    "mod([tf.random.uniform((10,1,64,64), dtype = tf.keras.backend.floatx()), tf.random.uniform((10,1,64), dtype = tf.keras.backend.floatx()), tf.random.uniform((10,1,64), dtype = tf.keras.backend.floatx()), tf.random.uniform((10,1,64), dtype = tf.keras.backend.floatx()), tf.random.uniform((10,1,64), dtype = tf.keras.backend.floatx()), tf.random.uniform((10,1), dtype = tf.keras.backend.floatx())])\n",
    "clear_output()\n",
    "mod.load_weights('asd.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_generator():\n",
    "    batch_size = 75\n",
    "    while True:\n",
    "        nx = 64#np.random.randint(64,128)\n",
    "        ny = 64#np.random.randint(64,128)\n",
    "        dx = 0.1*(np.random.rand() + 0.01)\n",
    "        boundaries = generate_random_boundaries(nx, ny, batch_size=batch_size, return_with_expanded_dims=False)\n",
    "        soln, rhs = generate_dataset(batch_size, [nx,ny], dx, boundaries=boundaries, initial_smoothness=np.random.randint(5,20))\n",
    "        for key in boundaries.keys():\n",
    "            boundaries[key] = tf.expand_dims(boundaries[key], axis = 1)\n",
    "        yield (rhs, boundaries['left'], boundaries['top'], boundaries['right'], boundaries['bottom'], dx * tf.ones((batch_size,1), dtype = tf.keras.backend.floatx())), soln\n",
    "        #yield (rhs, boundaries, dx * tf.ones((batch_size,1), dtype = tf.keras.backend.floatx())), soln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.2140 - mse: 0.0065 - mae: 0.0511 \n",
      "Epoch 00001: mse improved from inf to 0.00489, saving model to asd.h5\n",
      "200/200 [==============================] - 1203s 6s/step - loss: 0.2139 - mse: 0.0065 - mae: 0.0510 - val_loss: 0.1274 - val_mse: 0.0037 - val_mae: 0.0417\n",
      "Epoch 2/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.1875 - mse: 0.0038 - mae: 0.0404 \n",
      "Epoch 00002: mse improved from 0.00489 to 0.00355, saving model to asd.h5\n",
      "200/200 [==============================] - 1195s 6s/step - loss: 0.1870 - mse: 0.0038 - mae: 0.0404 - val_loss: 0.1810 - val_mse: 0.0027 - val_mae: 0.0328\n",
      "Epoch 3/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.1725 - mse: 0.0026 - mae: 0.0334 \n",
      "Epoch 00003: mse improved from 0.00355 to 0.00292, saving model to asd.h5\n",
      "200/200 [==============================] - 1189s 6s/step - loss: 0.1718 - mse: 0.0026 - mae: 0.0334 - val_loss: 0.1058 - val_mse: 0.0022 - val_mae: 0.0333\n",
      "Epoch 4/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.1371 - mse: 0.0023 - mae: 0.0316 \n",
      "Epoch 00004: mse improved from 0.00292 to 0.00206, saving model to asd.h5\n",
      "200/200 [==============================] - 1188s 6s/step - loss: 0.1366 - mse: 0.0023 - mae: 0.0316 - val_loss: 0.1183 - val_mse: 0.0018 - val_mae: 0.0282\n",
      "Epoch 5/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.1184 - mse: 0.0019 - mae: 0.0282 \n",
      "Epoch 00005: mse improved from 0.00206 to 0.00174, saving model to asd.h5\n",
      "200/200 [==============================] - 1194s 6s/step - loss: 0.1182 - mse: 0.0019 - mae: 0.0282 - val_loss: 0.0924 - val_mse: 0.0012 - val_mae: 0.0212\n",
      "Epoch 6/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.1258 - mse: 0.0017 - mae: 0.0266 \n",
      "Epoch 00006: mse did not improve from 0.00174\n",
      "200/200 [==============================] - 1191s 6s/step - loss: 0.1264 - mse: 0.0017 - mae: 0.0266 - val_loss: 0.0984 - val_mse: 0.0015 - val_mae: 0.0233\n",
      "Epoch 7/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.1259 - mse: 0.0016 - mae: 0.0266 \n",
      "Epoch 00007: mse did not improve from 0.00174\n",
      "200/200 [==============================] - 1193s 6s/step - loss: 0.1254 - mse: 0.0016 - mae: 0.0266 - val_loss: 0.2111 - val_mse: 0.0037 - val_mae: 0.0397\n",
      "Epoch 8/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.1388 - mse: 0.0026 - mae: 0.0340 \n",
      "Epoch 00008: mse did not improve from 0.00174\n",
      "\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "200/200 [==============================] - 1196s 6s/step - loss: 0.1383 - mse: 0.0026 - mae: 0.0339 - val_loss: 0.1158 - val_mse: 0.0014 - val_mae: 0.0244\n",
      "Epoch 9/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.1012 - mse: 0.0013 - mae: 0.0224 \n",
      "Epoch 00009: mse improved from 0.00174 to 0.00110, saving model to asd.h5\n",
      "200/200 [==============================] - 1193s 6s/step - loss: 0.1008 - mse: 0.0013 - mae: 0.0224 - val_loss: 0.0432 - val_mse: 8.1306e-04 - val_mae: 0.0165\n",
      "Epoch 10/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0920 - mse: 9.6695e-04 - mae: 0.0192 \n",
      "Epoch 00010: mse improved from 0.00110 to 0.00100, saving model to asd.h5\n",
      "200/200 [==============================] - 1197s 6s/step - loss: 0.0920 - mse: 9.6728e-04 - mae: 0.0192 - val_loss: 0.0523 - val_mse: 7.8969e-04 - val_mae: 0.0172\n",
      "Epoch 11/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0830 - mse: 9.6442e-04 - mae: 0.0192 \n",
      "Epoch 00011: mse improved from 0.00100 to 0.00094, saving model to asd.h5\n",
      "200/200 [==============================] - 1202s 6s/step - loss: 0.0828 - mse: 9.6422e-04 - mae: 0.0192 - val_loss: 0.0834 - val_mse: 0.0011 - val_mae: 0.0191\n",
      "Epoch 12/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0856 - mse: 9.7185e-04 - mae: 0.0196 \n",
      "Epoch 00012: mse did not improve from 0.00094\n",
      "200/200 [==============================] - 1203s 6s/step - loss: 0.0854 - mse: 9.7161e-04 - mae: 0.0195 - val_loss: 0.1517 - val_mse: 0.0012 - val_mae: 0.0231\n",
      "Epoch 13/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0958 - mse: 0.0011 - mae: 0.0204 \n",
      "Epoch 00013: mse did not improve from 0.00094\n",
      "200/200 [==============================] - 1198s 6s/step - loss: 0.0958 - mse: 0.0011 - mae: 0.0204 - val_loss: 0.0804 - val_mse: 8.7551e-04 - val_mae: 0.0189\n",
      "Epoch 14/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0800 - mse: 9.0346e-04 - mae: 0.0183 \n",
      "Epoch 00014: mse improved from 0.00094 to 0.00091, saving model to asd.h5\n",
      "200/200 [==============================] - 1197s 6s/step - loss: 0.0800 - mse: 9.0349e-04 - mae: 0.0183 - val_loss: 0.0847 - val_mse: 9.0430e-04 - val_mae: 0.0177\n",
      "Epoch 15/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0933 - mse: 9.9421e-04 - mae: 0.0196 \n",
      "Epoch 00015: mse did not improve from 0.00091\n",
      "200/200 [==============================] - 1200s 6s/step - loss: 0.0934 - mse: 9.9421e-04 - mae: 0.0196 - val_loss: 0.0981 - val_mse: 0.0010 - val_mae: 0.0198\n",
      "Epoch 16/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0842 - mse: 9.3294e-04 - mae: 0.0190 \n",
      "Epoch 00016: mse improved from 0.00091 to 0.00090, saving model to asd.h5\n",
      "200/200 [==============================] - 1181s 6s/step - loss: 0.0839 - mse: 9.3265e-04 - mae: 0.0190 - val_loss: 0.0696 - val_mse: 8.6645e-04 - val_mae: 0.0184\n",
      "Epoch 17/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0815 - mse: 8.5331e-04 - mae: 0.0177 \n",
      "Epoch 00017: mse improved from 0.00090 to 0.00087, saving model to asd.h5\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "200/200 [==============================] - 1177s 6s/step - loss: 0.0820 - mse: 8.5347e-04 - mae: 0.0177 - val_loss: 0.1341 - val_mse: 0.0011 - val_mae: 0.0223\n",
      "Epoch 18/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0840 - mse: 8.6591e-04 - mae: 0.0182 \n",
      "Epoch 00018: mse did not improve from 0.00087\n",
      "200/200 [==============================] - 1175s 6s/step - loss: 0.0845 - mse: 8.6598e-04 - mae: 0.0182 - val_loss: 0.0600 - val_mse: 7.3098e-04 - val_mae: 0.0161\n",
      "Epoch 19/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0842 - mse: 8.5247e-04 - mae: 0.0179 \n",
      "Epoch 00019: mse improved from 0.00087 to 0.00087, saving model to asd.h5\n",
      "200/200 [==============================] - 1180s 6s/step - loss: 0.0845 - mse: 8.5261e-04 - mae: 0.0179 - val_loss: 0.1178 - val_mse: 9.8590e-04 - val_mae: 0.0199\n",
      "Epoch 20/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0920 - mse: 9.0708e-04 - mae: 0.0186 \n",
      "Epoch 00020: mse did not improve from 0.00087\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "200/200 [==============================] - 1174s 6s/step - loss: 0.0925 - mse: 9.0719e-04 - mae: 0.0186 - val_loss: 0.0961 - val_mse: 9.0430e-04 - val_mae: 0.0186\n",
      "Epoch 21/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0877 - mse: 8.8290e-04 - mae: 0.0184 \n",
      "Epoch 00021: mse did not improve from 0.00087\n",
      "200/200 [==============================] - 1174s 6s/step - loss: 0.0875 - mse: 8.8287e-04 - mae: 0.0184 - val_loss: 0.0688 - val_mse: 7.1765e-04 - val_mae: 0.0163\n",
      "Epoch 22/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0845 - mse: 9.0466e-04 - mae: 0.0184 \n",
      "Epoch 00022: mse did not improve from 0.00087\n",
      "200/200 [==============================] - 1183s 6s/step - loss: 0.0846 - mse: 9.0431e-04 - mae: 0.0184 - val_loss: 0.0912 - val_mse: 9.4396e-04 - val_mae: 0.0186\n",
      "Epoch 23/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0805 - mse: 8.6088e-04 - mae: 0.0178 \n",
      "Epoch 00023: mse improved from 0.00087 to 0.00086, saving model to asd.h5\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "200/200 [==============================] - 1188s 6s/step - loss: 0.0811 - mse: 8.6083e-04 - mae: 0.0178 - val_loss: 0.0655 - val_mse: 6.6507e-04 - val_mae: 0.0158\n",
      "Epoch 24/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0774 - mse: 8.0791e-04 - mae: 0.0171 \n",
      "Epoch 00024: mse improved from 0.00086 to 0.00083, saving model to asd.h5\n",
      "200/200 [==============================] - 1181s 6s/step - loss: 0.0774 - mse: 8.0817e-04 - mae: 0.0171 - val_loss: 0.0834 - val_mse: 8.5653e-04 - val_mae: 0.0177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0813 - mse: 8.5101e-04 - mae: 0.0179 \n",
      "Epoch 00025: mse did not improve from 0.00083\n",
      "200/200 [==============================] - 1182s 6s/step - loss: 0.0812 - mse: 8.5088e-04 - mae: 0.0179 - val_loss: 0.0829 - val_mse: 8.1236e-04 - val_mae: 0.0175\n",
      "Epoch 26/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0804 - mse: 8.5652e-04 - mae: 0.0178 \n",
      "Epoch 00026: mse did not improve from 0.00083\n",
      "200/200 [==============================] - 1174s 6s/step - loss: 0.0809 - mse: 8.5646e-04 - mae: 0.0178 - val_loss: 0.1100 - val_mse: 8.9750e-04 - val_mae: 0.0189\n",
      "Epoch 27/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0821 - mse: 8.6935e-04 - mae: 0.0180 \n",
      "Epoch 00027: mse did not improve from 0.00083\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "200/200 [==============================] - 1185s 6s/step - loss: 0.0824 - mse: 8.6920e-04 - mae: 0.0180 - val_loss: 0.1161 - val_mse: 0.0010 - val_mae: 0.0206\n",
      "Epoch 28/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0761 - mse: 8.5976e-04 - mae: 0.0179 \n",
      "Epoch 00028: mse improved from 0.00083 to 0.00083, saving model to asd.h5\n",
      "200/200 [==============================] - 1185s 6s/step - loss: 0.0758 - mse: 8.5950e-04 - mae: 0.0179 - val_loss: 0.1032 - val_mse: 9.1644e-04 - val_mae: 0.0185\n",
      "Epoch 29/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0801 - mse: 8.2360e-04 - mae: 0.0173 \n",
      "Epoch 00029: mse did not improve from 0.00083\n",
      "200/200 [==============================] - 1182s 6s/step - loss: 0.0799 - mse: 8.2378e-04 - mae: 0.0173 - val_loss: 0.1035 - val_mse: 9.3720e-04 - val_mae: 0.0195\n",
      "Epoch 30/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0797 - mse: 8.3955e-04 - mae: 0.0178 \n",
      "Epoch 00030: mse did not improve from 0.00083\n",
      "200/200 [==============================] - 1191s 6s/step - loss: 0.0803 - mse: 8.3962e-04 - mae: 0.0178 - val_loss: 0.0932 - val_mse: 8.9199e-04 - val_mae: 0.0189\n",
      "Epoch 31/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0782 - mse: 8.4609e-04 - mae: 0.0176 \n",
      "Epoch 00031: mse did not improve from 0.00083\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "200/200 [==============================] - 1190s 6s/step - loss: 0.0779 - mse: 8.4607e-04 - mae: 0.0176 - val_loss: 0.0565 - val_mse: 7.6305e-04 - val_mae: 0.0168\n",
      "Epoch 32/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0848 - mse: 8.7086e-04 - mae: 0.0179 \n",
      "Epoch 00032: mse did not improve from 0.00083\n",
      "200/200 [==============================] - 1194s 6s/step - loss: 0.0844 - mse: 8.7087e-04 - mae: 0.0179 - val_loss: 0.0573 - val_mse: 8.0984e-04 - val_mae: 0.0166\n",
      "Epoch 33/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0901 - mse: 8.8504e-04 - mae: 0.0184 \n",
      "Epoch 00033: mse did not improve from 0.00083\n",
      "200/200 [==============================] - 1189s 6s/step - loss: 0.0899 - mse: 8.8505e-04 - mae: 0.0184 - val_loss: 0.1181 - val_mse: 0.0011 - val_mae: 0.0209\n",
      "Epoch 34/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0804 - mse: 8.6330e-04 - mae: 0.0179 \n",
      "Epoch 00034: mse did not improve from 0.00083\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "200/200 [==============================] - 1183s 6s/step - loss: 0.0805 - mse: 8.6318e-04 - mae: 0.0179 - val_loss: 0.0365 - val_mse: 7.3597e-04 - val_mae: 0.0161\n",
      "Epoch 35/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0828 - mse: 8.7115e-04 - mae: 0.0179 \n",
      "Epoch 00035: mse did not improve from 0.00083\n",
      "200/200 [==============================] - 1189s 6s/step - loss: 0.0826 - mse: 8.7099e-04 - mae: 0.0179 - val_loss: 0.0941 - val_mse: 9.3947e-04 - val_mae: 0.0194\n",
      "Epoch 36/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0872 - mse: 8.9528e-04 - mae: 0.0185 \n",
      "Epoch 00036: mse did not improve from 0.00083\n",
      "200/200 [==============================] - 1185s 6s/step - loss: 0.0868 - mse: 8.9504e-04 - mae: 0.0185 - val_loss: 0.0698 - val_mse: 7.5919e-04 - val_mae: 0.0165\n",
      "Epoch 37/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0881 - mse: 8.9242e-04 - mae: 0.0183 \n",
      "Epoch 00037: mse did not improve from 0.00083\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "200/200 [==============================] - 1183s 6s/step - loss: 0.0882 - mse: 8.9230e-04 - mae: 0.0183 - val_loss: 0.0395 - val_mse: 7.4445e-04 - val_mae: 0.0151\n",
      "Epoch 38/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0811 - mse: 8.7710e-04 - mae: 0.0178 \n",
      "Epoch 00038: mse did not improve from 0.00083\n",
      "200/200 [==============================] - 1183s 6s/step - loss: 0.0809 - mse: 8.7692e-04 - mae: 0.0178 - val_loss: 0.0423 - val_mse: 7.9840e-04 - val_mae: 0.0167\n",
      "Epoch 39/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0786 - mse: 8.0912e-04 - mae: 0.0171 \n",
      "Epoch 00039: mse did not improve from 0.00083\n",
      "200/200 [==============================] - 1193s 6s/step - loss: 0.0783 - mse: 8.0944e-04 - mae: 0.0171 - val_loss: 0.0945 - val_mse: 8.0679e-04 - val_mae: 0.0180\n",
      "Epoch 40/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0865 - mse: 8.4240e-04 - mae: 0.0178 \n",
      "Epoch 00040: mse did not improve from 0.00083\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "200/200 [==============================] - 1187s 6s/step - loss: 0.0865 - mse: 8.4261e-04 - mae: 0.0178 - val_loss: 0.0780 - val_mse: 8.8743e-04 - val_mae: 0.0184\n",
      "Epoch 41/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0783 - mse: 8.4333e-04 - mae: 0.0177 \n",
      "Epoch 00041: mse did not improve from 0.00083\n",
      "200/200 [==============================] - 1195s 6s/step - loss: 0.0780 - mse: 8.4328e-04 - mae: 0.0177 - val_loss: 0.0757 - val_mse: 8.0627e-04 - val_mae: 0.0166\n",
      "Epoch 42/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0828 - mse: 8.5039e-04 - mae: 0.0178 \n",
      "Epoch 00042: mse did not improve from 0.00083\n",
      "200/200 [==============================] - 1192s 6s/step - loss: 0.0826 - mse: 8.5033e-04 - mae: 0.0178 - val_loss: 0.0498 - val_mse: 7.2554e-04 - val_mae: 0.0155\n",
      "Epoch 43/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0844 - mse: 8.7080e-04 - mae: 0.0181 \n",
      "Epoch 00043: mse did not improve from 0.00083\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
      "200/200 [==============================] - 1190s 6s/step - loss: 0.0844 - mse: 8.7079e-04 - mae: 0.0181 - val_loss: 0.0403 - val_mse: 7.2560e-04 - val_mae: 0.0148\n",
      "Epoch 44/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0858 - mse: 9.0833e-04 - mae: 0.0186 \n",
      "Epoch 00044: mse did not improve from 0.00083\n",
      "200/200 [==============================] - 1189s 6s/step - loss: 0.0860 - mse: 9.0799e-04 - mae: 0.0186 - val_loss: 0.0765 - val_mse: 7.9903e-04 - val_mae: 0.0167\n",
      "Epoch 45/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0865 - mse: 8.6902e-04 - mae: 0.0180 \n",
      "Epoch 00045: mse did not improve from 0.00083\n",
      "200/200 [==============================] - 1197s 6s/step - loss: 0.0868 - mse: 8.6904e-04 - mae: 0.0180 - val_loss: 0.0979 - val_mse: 8.5838e-04 - val_mae: 0.0187\n",
      "Epoch 46/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0894 - mse: 9.0697e-04 - mae: 0.0184 \n",
      "Epoch 00046: mse did not improve from 0.00083\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.0000001179769417e-14.\n",
      "200/200 [==============================] - 1186s 6s/step - loss: 0.0895 - mse: 9.0680e-04 - mae: 0.0184 - val_loss: 0.0385 - val_mse: 6.5463e-04 - val_mae: 0.0150\n",
      "Epoch 47/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0803 - mse: 8.3612e-04 - mae: 0.0175 \n",
      "Epoch 00047: mse improved from 0.00083 to 0.00083, saving model to asd.h5\n",
      "200/200 [==============================] - 1186s 6s/step - loss: 0.0800 - mse: 8.3608e-04 - mae: 0.0175 - val_loss: 0.0337 - val_mse: 6.5210e-04 - val_mae: 0.0152\n",
      "Epoch 48/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0791 - mse: 8.7650e-04 - mae: 0.0180 \n",
      "Epoch 00048: mse did not improve from 0.00083\n",
      "200/200 [==============================] - 1184s 6s/step - loss: 0.0789 - mse: 8.7628e-04 - mae: 0.0180 - val_loss: 0.0810 - val_mse: 7.8563e-04 - val_mae: 0.0170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0844 - mse: 8.4803e-04 - mae: 0.0179 \n",
      "Epoch 00049: mse did not improve from 0.00083\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.0000001518582595e-15.\n",
      "200/200 [==============================] - 1197s 6s/step - loss: 0.0841 - mse: 8.4810e-04 - mae: 0.0179 - val_loss: 0.1034 - val_mse: 0.0010 - val_mae: 0.0193\n",
      "Epoch 50/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0781 - mse: 8.4439e-04 - mae: 0.0174 \n",
      "Epoch 00050: mse did not improve from 0.00083\n",
      "200/200 [==============================] - 1191s 6s/step - loss: 0.0781 - mse: 8.4436e-04 - mae: 0.0174 - val_loss: 0.0582 - val_mse: 7.4126e-04 - val_mae: 0.0161\n",
      "Epoch 51/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0881 - mse: 8.9609e-04 - mae: 0.0185 \n",
      "Epoch 00051: mse did not improve from 0.00083\n",
      "200/200 [==============================] - 1193s 6s/step - loss: 0.0879 - mse: 8.9593e-04 - mae: 0.0185 - val_loss: 0.1140 - val_mse: 8.5199e-04 - val_mae: 0.0188\n",
      "Epoch 52/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0902 - mse: 8.6189e-04 - mae: 0.0181 \n",
      "Epoch 00052: mse did not improve from 0.00083\n",
      "\n",
      "Epoch 00052: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1187s 6s/step - loss: 0.0902 - mse: 8.6211e-04 - mae: 0.0181 - val_loss: 0.1267 - val_mse: 9.8097e-04 - val_mae: 0.0200\n",
      "Epoch 53/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0791 - mse: 8.6742e-04 - mae: 0.0178 \n",
      "Epoch 00053: mse did not improve from 0.00083\n",
      "200/200 [==============================] - 1185s 6s/step - loss: 0.0795 - mse: 8.6720e-04 - mae: 0.0178 - val_loss: 0.1090 - val_mse: 9.4259e-04 - val_mae: 0.0198\n",
      "Epoch 54/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0787 - mse: 8.1880e-04 - mae: 0.0175 \n",
      "Epoch 00054: mse improved from 0.00083 to 0.00082, saving model to asd.h5\n",
      "200/200 [==============================] - 1191s 6s/step - loss: 0.0785 - mse: 8.1879e-04 - mae: 0.0175 - val_loss: 0.1205 - val_mse: 0.0010 - val_mae: 0.0207\n",
      "Epoch 55/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0809 - mse: 8.5122e-04 - mae: 0.0177 \n",
      "Epoch 00055: mse did not improve from 0.00082\n",
      "\n",
      "Epoch 00055: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1197s 6s/step - loss: 0.0809 - mse: 8.5117e-04 - mae: 0.0177 - val_loss: 0.0334 - val_mse: 6.0257e-04 - val_mae: 0.0143\n",
      "Epoch 56/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0777 - mse: 8.5512e-04 - mae: 0.0179 \n",
      "Epoch 00056: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1190s 6s/step - loss: 0.0775 - mse: 8.5496e-04 - mae: 0.0179 - val_loss: 0.0424 - val_mse: 6.8601e-04 - val_mae: 0.0152\n",
      "Epoch 57/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0813 - mse: 8.3066e-04 - mae: 0.0173 \n",
      "Epoch 00057: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1190s 6s/step - loss: 0.0811 - mse: 8.3084e-04 - mae: 0.0174 - val_loss: 0.0456 - val_mse: 7.1403e-04 - val_mae: 0.0152\n",
      "Epoch 58/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0772 - mse: 7.9981e-04 - mae: 0.0171 \n",
      "Epoch 00058: mse did not improve from 0.00082\n",
      "\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1195s 6s/step - loss: 0.0771 - mse: 8.0003e-04 - mae: 0.0171 - val_loss: 0.0796 - val_mse: 7.6849e-04 - val_mae: 0.0172\n",
      "Epoch 59/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0799 - mse: 8.7526e-04 - mae: 0.0182 \n",
      "Epoch 00059: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1189s 6s/step - loss: 0.0800 - mse: 8.7495e-04 - mae: 0.0182 - val_loss: 0.0542 - val_mse: 7.6389e-04 - val_mae: 0.0154\n",
      "Epoch 60/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0796 - mse: 8.2842e-04 - mae: 0.0172 \n",
      "Epoch 00060: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1192s 6s/step - loss: 0.0792 - mse: 8.2856e-04 - mae: 0.0172 - val_loss: 0.0427 - val_mse: 7.0257e-04 - val_mae: 0.0154\n",
      "Epoch 61/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0784 - mse: 8.4229e-04 - mae: 0.0175 \n",
      "Epoch 00061: mse did not improve from 0.00082\n",
      "\n",
      "Epoch 00061: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1195s 6s/step - loss: 0.0785 - mse: 8.4233e-04 - mae: 0.0175 - val_loss: 0.0838 - val_mse: 7.8665e-04 - val_mae: 0.0174\n",
      "Epoch 62/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0873 - mse: 8.8157e-04 - mae: 0.0183 \n",
      "Epoch 00062: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1190s 6s/step - loss: 0.0877 - mse: 8.8155e-04 - mae: 0.0183 - val_loss: 0.0469 - val_mse: 6.9883e-04 - val_mae: 0.0156\n",
      "Epoch 63/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0899 - mse: 8.8106e-04 - mae: 0.0183 \n",
      "Epoch 00063: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1188s 6s/step - loss: 0.0896 - mse: 8.8100e-04 - mae: 0.0183 - val_loss: 0.1592 - val_mse: 0.0013 - val_mae: 0.0240\n",
      "Epoch 64/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0832 - mse: 8.6436e-04 - mae: 0.0179 \n",
      "Epoch 00064: mse did not improve from 0.00082\n",
      "\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1191s 6s/step - loss: 0.0828 - mse: 8.6424e-04 - mae: 0.0179 - val_loss: 0.0398 - val_mse: 6.8496e-04 - val_mae: 0.0159\n",
      "Epoch 65/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0779 - mse: 8.5165e-04 - mae: 0.0176 \n",
      "Epoch 00065: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1182s 6s/step - loss: 0.0778 - mse: 8.5145e-04 - mae: 0.0176 - val_loss: 0.0981 - val_mse: 9.0914e-04 - val_mae: 0.0180\n",
      "Epoch 66/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0818 - mse: 8.5671e-04 - mae: 0.0177 \n",
      "Epoch 00066: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1192s 6s/step - loss: 0.0815 - mse: 8.5671e-04 - mae: 0.0177 - val_loss: 0.0726 - val_mse: 6.6907e-04 - val_mae: 0.0159\n",
      "Epoch 67/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0849 - mse: 8.5372e-04 - mae: 0.0178 \n",
      "Epoch 00067: mse did not improve from 0.00082\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1188s 6s/step - loss: 0.0853 - mse: 8.5386e-04 - mae: 0.0178 - val_loss: 0.1462 - val_mse: 0.0012 - val_mae: 0.0229\n",
      "Epoch 68/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0856 - mse: 8.6068e-04 - mae: 0.0178 \n",
      "Epoch 00068: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1192s 6s/step - loss: 0.0857 - mse: 8.6081e-04 - mae: 0.0178 - val_loss: 0.1278 - val_mse: 0.0010 - val_mae: 0.0204\n",
      "Epoch 69/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0808 - mse: 8.5343e-04 - mae: 0.0177 \n",
      "Epoch 00069: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1184s 6s/step - loss: 0.0810 - mse: 8.5335e-04 - mae: 0.0177 - val_loss: 0.0884 - val_mse: 8.0824e-04 - val_mae: 0.0174\n",
      "Epoch 70/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0884 - mse: 8.6405e-04 - mae: 0.0181 \n",
      "Epoch 00070: mse did not improve from 0.00082\n",
      "\n",
      "Epoch 00070: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1192s 6s/step - loss: 0.0882 - mse: 8.6418e-04 - mae: 0.0181 - val_loss: 0.1079 - val_mse: 9.2383e-04 - val_mae: 0.0192\n",
      "Epoch 71/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0804 - mse: 8.3585e-04 - mae: 0.0175 \n",
      "Epoch 00071: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1192s 6s/step - loss: 0.0802 - mse: 8.3592e-04 - mae: 0.0175 - val_loss: 0.0701 - val_mse: 8.3138e-04 - val_mae: 0.0179\n",
      "Epoch 72/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0745 - mse: 7.9973e-04 - mae: 0.0170 \n",
      "Epoch 00072: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1191s 6s/step - loss: 0.0750 - mse: 7.9993e-04 - mae: 0.0170 - val_loss: 0.0668 - val_mse: 6.6113e-04 - val_mae: 0.0158\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0883 - mse: 8.8770e-04 - mae: 0.0184 \n",
      "Epoch 00073: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1190s 6s/step - loss: 0.0885 - mse: 8.8757e-04 - mae: 0.0184 - val_loss: 0.0674 - val_mse: 7.6320e-04 - val_mae: 0.0170\n",
      "Epoch 74/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0861 - mse: 9.2208e-04 - mae: 0.0190 \n",
      "Epoch 00074: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1196s 6s/step - loss: 0.0860 - mse: 9.2161e-04 - mae: 0.0190 - val_loss: 0.0944 - val_mse: 9.5749e-04 - val_mae: 0.0184\n",
      "Epoch 75/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0804 - mse: 8.2935e-04 - mae: 0.0174 \n",
      "Epoch 00075: mse did not improve from 0.00082\n",
      "\n",
      "Epoch 00075: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1197s 6s/step - loss: 0.0802 - mse: 8.2967e-04 - mae: 0.0174 - val_loss: 0.1247 - val_mse: 0.0010 - val_mae: 0.0202\n",
      "Epoch 76/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0879 - mse: 8.7403e-04 - mae: 0.0182 \n",
      "Epoch 00076: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1189s 6s/step - loss: 0.0879 - mse: 8.7413e-04 - mae: 0.0182 - val_loss: 0.1092 - val_mse: 9.6727e-04 - val_mae: 0.0199\n",
      "Epoch 77/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0916 - mse: 8.8216e-04 - mae: 0.0183 \n",
      "Epoch 00077: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1191s 6s/step - loss: 0.0913 - mse: 8.8217e-04 - mae: 0.0183 - val_loss: 0.0915 - val_mse: 7.7154e-04 - val_mae: 0.0176\n",
      "Epoch 78/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0835 - mse: 8.8234e-04 - mae: 0.0185 \n",
      "Epoch 00078: mse did not improve from 0.00082\n",
      "\n",
      "Epoch 00078: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1186s 6s/step - loss: 0.0833 - mse: 8.8206e-04 - mae: 0.0185 - val_loss: 0.1176 - val_mse: 9.6747e-04 - val_mae: 0.0201\n",
      "Epoch 79/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0761 - mse: 8.2960e-04 - mae: 0.0174 \n",
      "Epoch 00079: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1190s 6s/step - loss: 0.0766 - mse: 8.2961e-04 - mae: 0.0174 - val_loss: 0.1179 - val_mse: 9.6484e-04 - val_mae: 0.0201\n",
      "Epoch 80/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0799 - mse: 8.8852e-04 - mae: 0.0182 \n",
      "Epoch 00080: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1195s 6s/step - loss: 0.0795 - mse: 8.8812e-04 - mae: 0.0182 - val_loss: 0.0525 - val_mse: 7.6930e-04 - val_mae: 0.0154\n",
      "Epoch 81/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0838 - mse: 8.4438e-04 - mae: 0.0177 \n",
      "Epoch 00081: mse did not improve from 0.00082\n",
      "\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1188s 6s/step - loss: 0.0842 - mse: 8.4452e-04 - mae: 0.0177 - val_loss: 0.0949 - val_mse: 9.0517e-04 - val_mae: 0.0184\n",
      "Epoch 82/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0914 - mse: 9.0797e-04 - mae: 0.0188 \n",
      "Epoch 00082: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1196s 6s/step - loss: 0.0915 - mse: 9.0773e-04 - mae: 0.0188 - val_loss: 0.0829 - val_mse: 8.4666e-04 - val_mae: 0.0173\n",
      "Epoch 83/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0848 - mse: 8.9970e-04 - mae: 0.0185 \n",
      "Epoch 00083: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1204s 6s/step - loss: 0.0854 - mse: 8.9937e-04 - mae: 0.0185 - val_loss: 0.0636 - val_mse: 7.2675e-04 - val_mae: 0.0167\n",
      "Epoch 84/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0803 - mse: 8.4894e-04 - mae: 0.0176 \n",
      "Epoch 00084: mse did not improve from 0.00082\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1195s 6s/step - loss: 0.0805 - mse: 8.4905e-04 - mae: 0.0176 - val_loss: 0.0764 - val_mse: 8.0212e-04 - val_mae: 0.0175\n",
      "Epoch 85/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0821 - mse: 8.6423e-04 - mae: 0.0181 \n",
      "Epoch 00085: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1194s 6s/step - loss: 0.0820 - mse: 8.6398e-04 - mae: 0.0181 - val_loss: 0.1098 - val_mse: 9.8541e-04 - val_mae: 0.0201\n",
      "Epoch 86/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0868 - mse: 8.4488e-04 - mae: 0.0178 \n",
      "Epoch 00086: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1193s 6s/step - loss: 0.0868 - mse: 8.4515e-04 - mae: 0.0178 - val_loss: 0.1735 - val_mse: 0.0014 - val_mae: 0.0249\n",
      "Epoch 87/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0819 - mse: 8.3558e-04 - mae: 0.0177 \n",
      "Epoch 00087: mse did not improve from 0.00082\n",
      "\n",
      "Epoch 00087: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1197s 6s/step - loss: 0.0816 - mse: 8.3568e-04 - mae: 0.0177 - val_loss: 0.0813 - val_mse: 0.0010 - val_mae: 0.0194\n",
      "Epoch 88/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0782 - mse: 8.5772e-04 - mae: 0.0177 \n",
      "Epoch 00088: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1196s 6s/step - loss: 0.0779 - mse: 8.5765e-04 - mae: 0.0177 - val_loss: 0.0948 - val_mse: 8.8719e-04 - val_mae: 0.0180\n",
      "Epoch 89/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0809 - mse: 8.6920e-04 - mae: 0.0179 \n",
      "Epoch 00089: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1195s 6s/step - loss: 0.0810 - mse: 8.6902e-04 - mae: 0.0179 - val_loss: 0.0755 - val_mse: 8.2065e-04 - val_mae: 0.0173\n",
      "Epoch 90/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0885 - mse: 9.1285e-04 - mae: 0.0187 \n",
      "Epoch 00090: mse did not improve from 0.00082\n",
      "\n",
      "Epoch 00090: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1191s 6s/step - loss: 0.0883 - mse: 9.1254e-04 - mae: 0.0187 - val_loss: 0.0692 - val_mse: 7.6060e-04 - val_mae: 0.0170\n",
      "Epoch 91/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0856 - mse: 8.9010e-04 - mae: 0.0184 \n",
      "Epoch 00091: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1196s 6s/step - loss: 0.0860 - mse: 8.8985e-04 - mae: 0.0183 - val_loss: 0.0363 - val_mse: 7.5748e-04 - val_mae: 0.0156\n",
      "Epoch 92/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0832 - mse: 8.5449e-04 - mae: 0.0179 \n",
      "Epoch 00092: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1194s 6s/step - loss: 0.0829 - mse: 8.5452e-04 - mae: 0.0179 - val_loss: 0.1384 - val_mse: 0.0011 - val_mae: 0.0217\n",
      "Epoch 93/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0844 - mse: 8.6639e-04 - mae: 0.0179 \n",
      "Epoch 00093: mse did not improve from 0.00082\n",
      "\n",
      "Epoch 00093: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1196s 6s/step - loss: 0.0841 - mse: 8.6639e-04 - mae: 0.0179 - val_loss: 0.0762 - val_mse: 8.5200e-04 - val_mae: 0.0174\n",
      "Epoch 94/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0884 - mse: 8.8425e-04 - mae: 0.0183 \n",
      "Epoch 00094: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1194s 6s/step - loss: 0.0883 - mse: 8.8429e-04 - mae: 0.0183 - val_loss: 0.1241 - val_mse: 9.4482e-04 - val_mae: 0.0202\n",
      "Epoch 95/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0873 - mse: 9.0349e-04 - mae: 0.0186 \n",
      "Epoch 00095: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1191s 6s/step - loss: 0.0875 - mse: 9.0328e-04 - mae: 0.0186 - val_loss: 0.0439 - val_mse: 7.9044e-04 - val_mae: 0.0155\n",
      "Epoch 96/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0750 - mse: 8.4299e-04 - mae: 0.0176 \n",
      "Epoch 00096: mse did not improve from 0.00082\n",
      "\n",
      "Epoch 00096: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1195s 6s/step - loss: 0.0750 - mse: 8.4285e-04 - mae: 0.0176 - val_loss: 0.0618 - val_mse: 7.3217e-04 - val_mae: 0.0165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0775 - mse: 8.7093e-04 - mae: 0.0179 \n",
      "Epoch 00097: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1188s 6s/step - loss: 0.0776 - mse: 8.7069e-04 - mae: 0.0179 - val_loss: 0.1281 - val_mse: 9.9236e-04 - val_mae: 0.0203\n",
      "Epoch 98/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0827 - mse: 8.9555e-04 - mae: 0.0183 \n",
      "Epoch 00098: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1195s 6s/step - loss: 0.0828 - mse: 8.9536e-04 - mae: 0.0183 - val_loss: 0.1328 - val_mse: 0.0011 - val_mae: 0.0212\n",
      "Epoch 99/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0834 - mse: 8.5693e-04 - mae: 0.0178 \n",
      "Epoch 00099: mse did not improve from 0.00082\n",
      "\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1190s 6s/step - loss: 0.0834 - mse: 8.5697e-04 - mae: 0.0178 - val_loss: 0.0465 - val_mse: 7.8335e-04 - val_mae: 0.0163\n",
      "Epoch 100/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0844 - mse: 8.5386e-04 - mae: 0.0177 \n",
      "Epoch 00100: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1201s 6s/step - loss: 0.0847 - mse: 8.5392e-04 - mae: 0.0177 - val_loss: 0.0769 - val_mse: 7.7667e-04 - val_mae: 0.0173\n",
      "Epoch 101/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0840 - mse: 9.2699e-04 - mae: 0.0186 \n",
      "Epoch 00101: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1198s 6s/step - loss: 0.0839 - mse: 9.2654e-04 - mae: 0.0186 - val_loss: 0.1006 - val_mse: 8.4995e-04 - val_mae: 0.0186\n",
      "Epoch 102/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0838 - mse: 8.6548e-04 - mae: 0.0179 \n",
      "Epoch 00102: mse did not improve from 0.00082\n",
      "\n",
      "Epoch 00102: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1200s 6s/step - loss: 0.0838 - mse: 8.6555e-04 - mae: 0.0179 - val_loss: 0.0331 - val_mse: 7.2132e-04 - val_mae: 0.0159\n",
      "Epoch 103/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0853 - mse: 8.7346e-04 - mae: 0.0180 \n",
      "Epoch 00103: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1197s 6s/step - loss: 0.0850 - mse: 8.7339e-04 - mae: 0.0180 - val_loss: 0.0495 - val_mse: 6.8889e-04 - val_mae: 0.0154\n",
      "Epoch 104/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0891 - mse: 8.9042e-04 - mae: 0.0185 \n",
      "Epoch 00104: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1204s 6s/step - loss: 0.0890 - mse: 8.9038e-04 - mae: 0.0185 - val_loss: 0.0746 - val_mse: 8.7067e-04 - val_mae: 0.0181\n",
      "Epoch 105/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0830 - mse: 8.4230e-04 - mae: 0.0178 \n",
      "Epoch 00105: mse did not improve from 0.00082\n",
      "\n",
      "Epoch 00105: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1199s 6s/step - loss: 0.0832 - mse: 8.4243e-04 - mae: 0.0178 - val_loss: 0.0735 - val_mse: 8.1302e-04 - val_mae: 0.0169\n",
      "Epoch 106/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0817 - mse: 8.4937e-04 - mae: 0.0176 \n",
      "Epoch 00106: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1196s 6s/step - loss: 0.0816 - mse: 8.4949e-04 - mae: 0.0176 - val_loss: 0.0934 - val_mse: 8.8811e-04 - val_mae: 0.0182\n",
      "Epoch 107/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0864 - mse: 8.8116e-04 - mae: 0.0183 \n",
      "Epoch 00107: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1205s 6s/step - loss: 0.0867 - mse: 8.8121e-04 - mae: 0.0183 - val_loss: 0.1205 - val_mse: 8.9617e-04 - val_mae: 0.0196\n",
      "Epoch 108/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0804 - mse: 8.3987e-04 - mae: 0.0177 \n",
      "Epoch 00108: mse did not improve from 0.00082\n",
      "\n",
      "Epoch 00108: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1206s 6s/step - loss: 0.0802 - mse: 8.3980e-04 - mae: 0.0177 - val_loss: 0.0399 - val_mse: 7.3585e-04 - val_mae: 0.0160\n",
      "Epoch 109/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0838 - mse: 8.5784e-04 - mae: 0.0180 \n",
      "Epoch 00109: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1216s 6s/step - loss: 0.0835 - mse: 8.5787e-04 - mae: 0.0180 - val_loss: 0.0459 - val_mse: 7.2569e-04 - val_mae: 0.0151\n",
      "Epoch 110/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0895 - mse: 9.0065e-04 - mae: 0.0186 \n",
      "Epoch 00110: mse did not improve from 0.00082\n",
      "200/200 [==============================] - 1215s 6s/step - loss: 0.0895 - mse: 9.0040e-04 - mae: 0.0186 - val_loss: 0.0403 - val_mse: 7.7348e-04 - val_mae: 0.0151\n",
      "Epoch 111/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0913 - mse: 9.4867e-04 - mae: 0.0189 \n",
      "Epoch 00111: mse did not improve from 0.00082\n",
      "\n",
      "Epoch 00111: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1214s 6s/step - loss: 0.0911 - mse: 9.4823e-04 - mae: 0.0189 - val_loss: 0.0645 - val_mse: 7.7617e-04 - val_mae: 0.0165\n",
      "Epoch 112/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0701 - mse: 7.9202e-04 - mae: 0.0170 \n",
      "Epoch 00112: mse improved from 0.00082 to 0.00079, saving model to asd.h5\n",
      "200/200 [==============================] - 1208s 6s/step - loss: 0.0699 - mse: 7.9200e-04 - mae: 0.0170 - val_loss: 0.1125 - val_mse: 9.0093e-04 - val_mae: 0.0191\n",
      "Epoch 113/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0855 - mse: 8.7460e-04 - mae: 0.0182 \n",
      "Epoch 00113: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1206s 6s/step - loss: 0.0864 - mse: 8.7455e-04 - mae: 0.0182 - val_loss: 0.0407 - val_mse: 6.5041e-04 - val_mae: 0.0152\n",
      "Epoch 114/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0821 - mse: 8.7319e-04 - mae: 0.0182 \n",
      "Epoch 00114: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1209s 6s/step - loss: 0.0818 - mse: 8.7307e-04 - mae: 0.0182 - val_loss: 0.0539 - val_mse: 7.9544e-04 - val_mae: 0.0164\n",
      "Epoch 115/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0814 - mse: 8.5007e-04 - mae: 0.0177 \n",
      "Epoch 00115: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00115: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1200s 6s/step - loss: 0.0814 - mse: 8.5013e-04 - mae: 0.0177 - val_loss: 0.0933 - val_mse: 7.2005e-04 - val_mae: 0.0171\n",
      "Epoch 116/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0924 - mse: 8.6132e-04 - mae: 0.0180 \n",
      "Epoch 00116: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1194s 6s/step - loss: 0.0928 - mse: 8.6166e-04 - mae: 0.0180 - val_loss: 0.0709 - val_mse: 8.3054e-04 - val_mae: 0.0172\n",
      "Epoch 117/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0864 - mse: 8.4635e-04 - mae: 0.0175 \n",
      "Epoch 00117: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1194s 6s/step - loss: 0.0868 - mse: 8.4674e-04 - mae: 0.0175 - val_loss: 0.0330 - val_mse: 7.5809e-04 - val_mae: 0.0152\n",
      "Epoch 118/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0754 - mse: 8.4768e-04 - mae: 0.0177 \n",
      "Epoch 00118: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00118: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1188s 6s/step - loss: 0.0754 - mse: 8.4748e-04 - mae: 0.0177 - val_loss: 0.0231 - val_mse: 6.5531e-04 - val_mae: 0.0140\n",
      "Epoch 119/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0784 - mse: 8.3549e-04 - mae: 0.0175 \n",
      "Epoch 00119: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1193s 6s/step - loss: 0.0788 - mse: 8.3570e-04 - mae: 0.0175 - val_loss: 0.0913 - val_mse: 9.2525e-04 - val_mae: 0.0179\n",
      "Epoch 120/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0726 - mse: 8.5125e-04 - mae: 0.0176 \n",
      "Epoch 00120: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1192s 6s/step - loss: 0.0723 - mse: 8.5104e-04 - mae: 0.0176 - val_loss: 0.0757 - val_mse: 8.8388e-04 - val_mae: 0.0174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0859 - mse: 8.8892e-04 - mae: 0.0182 \n",
      "Epoch 00121: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00121: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1193s 6s/step - loss: 0.0856 - mse: 8.8880e-04 - mae: 0.0182 - val_loss: 0.0979 - val_mse: 9.4068e-04 - val_mae: 0.0192\n",
      "Epoch 122/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0859 - mse: 8.9671e-04 - mae: 0.0184 \n",
      "Epoch 00122: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1197s 6s/step - loss: 0.0857 - mse: 8.9653e-04 - mae: 0.0184 - val_loss: 0.0482 - val_mse: 6.9064e-04 - val_mae: 0.0153\n",
      "Epoch 123/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0831 - mse: 8.5526e-04 - mae: 0.0178 \n",
      "Epoch 00123: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1200s 6s/step - loss: 0.0829 - mse: 8.5529e-04 - mae: 0.0178 - val_loss: 0.1022 - val_mse: 9.2431e-04 - val_mae: 0.0188\n",
      "Epoch 124/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0803 - mse: 8.7577e-04 - mae: 0.0181 \n",
      "Epoch 00124: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00124: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1191s 6s/step - loss: 0.0806 - mse: 8.7552e-04 - mae: 0.0181 - val_loss: 0.1077 - val_mse: 9.7920e-04 - val_mae: 0.0193\n",
      "Epoch 125/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0828 - mse: 8.4434e-04 - mae: 0.0177 \n",
      "Epoch 00125: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1187s 6s/step - loss: 0.0828 - mse: 8.4448e-04 - mae: 0.0177 - val_loss: 0.1572 - val_mse: 0.0012 - val_mae: 0.0232\n",
      "Epoch 126/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0834 - mse: 8.4912e-04 - mae: 0.0179 \n",
      "Epoch 00126: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1192s 6s/step - loss: 0.0834 - mse: 8.4912e-04 - mae: 0.0179 - val_loss: 0.0471 - val_mse: 6.8709e-04 - val_mae: 0.0149\n",
      "Epoch 127/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0898 - mse: 8.4200e-04 - mae: 0.0176 \n",
      "Epoch 00127: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00127: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1195s 6s/step - loss: 0.0894 - mse: 8.4240e-04 - mae: 0.0176 - val_loss: 0.0286 - val_mse: 6.5980e-04 - val_mae: 0.0151\n",
      "Epoch 128/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0915 - mse: 9.1819e-04 - mae: 0.0188 \n",
      "Epoch 00128: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1190s 6s/step - loss: 0.0915 - mse: 9.1800e-04 - mae: 0.0188 - val_loss: 0.0820 - val_mse: 7.6066e-04 - val_mae: 0.0167\n",
      "Epoch 129/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0845 - mse: 8.8082e-04 - mae: 0.0180 \n",
      "Epoch 00129: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1193s 6s/step - loss: 0.0841 - mse: 8.8077e-04 - mae: 0.0180 - val_loss: 0.0342 - val_mse: 6.6053e-04 - val_mae: 0.0146\n",
      "Epoch 130/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0898 - mse: 8.6314e-04 - mae: 0.0180 \n",
      "Epoch 00130: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00130: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1207s 6s/step - loss: 0.0897 - mse: 8.6329e-04 - mae: 0.0180 - val_loss: 0.1165 - val_mse: 9.8303e-04 - val_mae: 0.0195\n",
      "Epoch 131/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0857 - mse: 9.0508e-04 - mae: 0.0185 \n",
      "Epoch 00131: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1211s 6s/step - loss: 0.0861 - mse: 9.0485e-04 - mae: 0.0185 - val_loss: 0.1131 - val_mse: 0.0010 - val_mae: 0.0201\n",
      "Epoch 132/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0836 - mse: 8.7028e-04 - mae: 0.0182 \n",
      "Epoch 00132: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1219s 6s/step - loss: 0.0834 - mse: 8.7010e-04 - mae: 0.0182 - val_loss: 0.1429 - val_mse: 9.9961e-04 - val_mae: 0.0211\n",
      "Epoch 133/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0787 - mse: 8.7612e-04 - mae: 0.0181 \n",
      "Epoch 00133: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00133: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1223s 6s/step - loss: 0.0784 - mse: 8.7578e-04 - mae: 0.0181 - val_loss: 0.1831 - val_mse: 0.0012 - val_mae: 0.0243\n",
      "Epoch 134/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0777 - mse: 7.9699e-04 - mae: 0.0169 \n",
      "Epoch 00134: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1216s 6s/step - loss: 0.0780 - mse: 7.9738e-04 - mae: 0.0169 - val_loss: 0.1080 - val_mse: 9.4571e-04 - val_mae: 0.0193\n",
      "Epoch 135/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0859 - mse: 8.6065e-04 - mae: 0.0179 \n",
      "Epoch 00135: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1218s 6s/step - loss: 0.0858 - mse: 8.6078e-04 - mae: 0.0179 - val_loss: 0.1068 - val_mse: 8.8407e-04 - val_mae: 0.0192\n",
      "Epoch 136/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0757 - mse: 8.1315e-04 - mae: 0.0171 \n",
      "Epoch 00136: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00136: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1218s 6s/step - loss: 0.0756 - mse: 8.1333e-04 - mae: 0.0171 - val_loss: 0.0969 - val_mse: 9.0437e-04 - val_mae: 0.0194\n",
      "Epoch 137/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0785 - mse: 8.4030e-04 - mae: 0.0176 \n",
      "Epoch 00137: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1219s 6s/step - loss: 0.0789 - mse: 8.4024e-04 - mae: 0.0176 - val_loss: 0.0401 - val_mse: 6.7790e-04 - val_mae: 0.0152\n",
      "Epoch 138/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0798 - mse: 8.2601e-04 - mae: 0.0173 \n",
      "Epoch 00138: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1230s 6s/step - loss: 0.0800 - mse: 8.2625e-04 - mae: 0.0173 - val_loss: 0.1257 - val_mse: 0.0010 - val_mae: 0.0206\n",
      "Epoch 139/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0810 - mse: 8.4214e-04 - mae: 0.0178 \n",
      "Epoch 00139: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00139: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1222s 6s/step - loss: 0.0807 - mse: 8.4217e-04 - mae: 0.0178 - val_loss: 0.0369 - val_mse: 6.8547e-04 - val_mae: 0.0157\n",
      "Epoch 140/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0854 - mse: 9.0196e-04 - mae: 0.0185 \n",
      "Epoch 00140: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1220s 6s/step - loss: 0.0852 - mse: 9.0165e-04 - mae: 0.0185 - val_loss: 0.0720 - val_mse: 7.8011e-04 - val_mae: 0.0165\n",
      "Epoch 141/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0750 - mse: 8.2164e-04 - mae: 0.0172 \n",
      "Epoch 00141: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1225s 6s/step - loss: 0.0748 - mse: 8.2164e-04 - mae: 0.0172 - val_loss: 0.0290 - val_mse: 7.4266e-04 - val_mae: 0.0160\n",
      "Epoch 142/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0825 - mse: 8.6664e-04 - mae: 0.0180 \n",
      "Epoch 00142: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00142: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1218s 6s/step - loss: 0.0829 - mse: 8.6659e-04 - mae: 0.0180 - val_loss: 0.0501 - val_mse: 7.5323e-04 - val_mae: 0.0159\n",
      "Epoch 143/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0823 - mse: 8.6688e-04 - mae: 0.0181 \n",
      "Epoch 00143: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1224s 6s/step - loss: 0.0829 - mse: 8.6678e-04 - mae: 0.0181 - val_loss: 0.1019 - val_mse: 9.0792e-04 - val_mae: 0.0191\n",
      "Epoch 144/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0786 - mse: 8.5367e-04 - mae: 0.0176 \n",
      "Epoch 00144: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1220s 6s/step - loss: 0.0787 - mse: 8.5358e-04 - mae: 0.0176 - val_loss: 0.1289 - val_mse: 9.8076e-04 - val_mae: 0.0207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0857 - mse: 9.3367e-04 - mae: 0.0189 \n",
      "Epoch 00145: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00145: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1220s 6s/step - loss: 0.0855 - mse: 9.3308e-04 - mae: 0.0189 - val_loss: 0.0635 - val_mse: 8.0495e-04 - val_mae: 0.0162\n",
      "Epoch 146/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0859 - mse: 8.9483e-04 - mae: 0.0184 \n",
      "Epoch 00146: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1223s 6s/step - loss: 0.0856 - mse: 8.9469e-04 - mae: 0.0184 - val_loss: 0.1065 - val_mse: 8.3767e-04 - val_mae: 0.0189\n",
      "Epoch 147/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0888 - mse: 8.7087e-04 - mae: 0.0181 \n",
      "Epoch 00147: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1223s 6s/step - loss: 0.0891 - mse: 8.7101e-04 - mae: 0.0181 - val_loss: 0.0714 - val_mse: 7.0870e-04 - val_mae: 0.0161\n",
      "Epoch 148/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0906 - mse: 8.9273e-04 - mae: 0.0184 \n",
      "Epoch 00148: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00148: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1221s 6s/step - loss: 0.0903 - mse: 8.9259e-04 - mae: 0.0184 - val_loss: 0.0688 - val_mse: 7.6212e-04 - val_mae: 0.0168\n",
      "Epoch 149/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0844 - mse: 8.4893e-04 - mae: 0.0177 \n",
      "Epoch 00149: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1229s 6s/step - loss: 0.0842 - mse: 8.4918e-04 - mae: 0.0177 - val_loss: 0.0994 - val_mse: 8.7579e-04 - val_mae: 0.0187\n",
      "Epoch 150/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0845 - mse: 8.6385e-04 - mae: 0.0179 \n",
      "Epoch 00150: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1229s 6s/step - loss: 0.0842 - mse: 8.6381e-04 - mae: 0.0179 - val_loss: 0.0363 - val_mse: 6.6650e-04 - val_mae: 0.0148\n",
      "Epoch 151/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0805 - mse: 8.5008e-04 - mae: 0.0178 \n",
      "Epoch 00151: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00151: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1222s 6s/step - loss: 0.0806 - mse: 8.5019e-04 - mae: 0.0178 - val_loss: 0.0983 - val_mse: 9.9060e-04 - val_mae: 0.0198\n",
      "Epoch 152/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0760 - mse: 8.3214e-04 - mae: 0.0175 \n",
      "Epoch 00152: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1223s 6s/step - loss: 0.0760 - mse: 8.3204e-04 - mae: 0.0175 - val_loss: 0.0630 - val_mse: 8.4857e-04 - val_mae: 0.0171\n",
      "Epoch 153/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0852 - mse: 8.3382e-04 - mae: 0.0174 \n",
      "Epoch 00153: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1223s 6s/step - loss: 0.0849 - mse: 8.3413e-04 - mae: 0.0174 - val_loss: 0.0482 - val_mse: 7.6856e-04 - val_mae: 0.0155\n",
      "Epoch 154/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0790 - mse: 8.3255e-04 - mae: 0.0175 \n",
      "Epoch 00154: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00154: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1220s 6s/step - loss: 0.0789 - mse: 8.3268e-04 - mae: 0.0175 - val_loss: 0.0949 - val_mse: 8.7023e-04 - val_mae: 0.0182\n",
      "Epoch 155/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0758 - mse: 8.7369e-04 - mae: 0.0180 \n",
      "Epoch 00155: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1227s 6s/step - loss: 0.0756 - mse: 8.7329e-04 - mae: 0.0180 - val_loss: 0.0958 - val_mse: 9.4043e-04 - val_mae: 0.0194\n",
      "Epoch 156/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0802 - mse: 8.2834e-04 - mae: 0.0174 \n",
      "Epoch 00156: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1224s 6s/step - loss: 0.0809 - mse: 8.2855e-04 - mae: 0.0174 - val_loss: 0.0742 - val_mse: 8.8407e-04 - val_mae: 0.0180\n",
      "Epoch 157/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0769 - mse: 8.5319e-04 - mae: 0.0179 \n",
      "Epoch 00157: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00157: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1220s 6s/step - loss: 0.0769 - mse: 8.5280e-04 - mae: 0.0178 - val_loss: 0.1817 - val_mse: 0.0015 - val_mae: 0.0256\n",
      "Epoch 158/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0880 - mse: 9.0667e-04 - mae: 0.0186 \n",
      "Epoch 00158: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1192s 6s/step - loss: 0.0878 - mse: 9.0641e-04 - mae: 0.0186 - val_loss: 0.0318 - val_mse: 6.2077e-04 - val_mae: 0.0142\n",
      "Epoch 159/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0814 - mse: 8.7950e-04 - mae: 0.0182 \n",
      "Epoch 00159: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1191s 6s/step - loss: 0.0812 - mse: 8.7931e-04 - mae: 0.0182 - val_loss: 0.1237 - val_mse: 9.7261e-04 - val_mae: 0.0200\n",
      "Epoch 160/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0804 - mse: 8.5655e-04 - mae: 0.0179 \n",
      "Epoch 00160: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00160: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1206s 6s/step - loss: 0.0801 - mse: 8.5645e-04 - mae: 0.0179 - val_loss: 0.1325 - val_mse: 0.0011 - val_mae: 0.0215\n",
      "Epoch 161/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0887 - mse: 8.7215e-04 - mae: 0.0181 \n",
      "Epoch 00161: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1189s 6s/step - loss: 0.0886 - mse: 8.7218e-04 - mae: 0.0181 - val_loss: 0.1266 - val_mse: 0.0010 - val_mae: 0.0207\n",
      "Epoch 162/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0827 - mse: 8.4783e-04 - mae: 0.0176 \n",
      "Epoch 00162: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1188s 6s/step - loss: 0.0830 - mse: 8.4794e-04 - mae: 0.0176 - val_loss: 0.0642 - val_mse: 6.8963e-04 - val_mae: 0.0156\n",
      "Epoch 163/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0829 - mse: 8.4097e-04 - mae: 0.0177 \n",
      "Epoch 00163: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00163: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1196s 6s/step - loss: 0.0826 - mse: 8.4107e-04 - mae: 0.0177 - val_loss: 0.1068 - val_mse: 8.9780e-04 - val_mae: 0.0190\n",
      "Epoch 164/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0840 - mse: 8.3261e-04 - mae: 0.0175 \n",
      "Epoch 00164: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1198s 6s/step - loss: 0.0840 - mse: 8.3278e-04 - mae: 0.0175 - val_loss: 0.0930 - val_mse: 9.0378e-04 - val_mae: 0.0182\n",
      "Epoch 165/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0809 - mse: 8.9419e-04 - mae: 0.0184 \n",
      "Epoch 00165: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1191s 6s/step - loss: 0.0813 - mse: 8.9379e-04 - mae: 0.0184 - val_loss: 0.0909 - val_mse: 8.5326e-04 - val_mae: 0.0182\n",
      "Epoch 166/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0845 - mse: 8.9038e-04 - mae: 0.0184 \n",
      "Epoch 00166: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00166: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1196s 6s/step - loss: 0.0842 - mse: 8.9010e-04 - mae: 0.0184 - val_loss: 0.0686 - val_mse: 6.8574e-04 - val_mae: 0.0159\n",
      "Epoch 167/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0842 - mse: 8.4053e-04 - mae: 0.0175 \n",
      "Epoch 00167: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1212s 6s/step - loss: 0.0839 - mse: 8.4083e-04 - mae: 0.0175 - val_loss: 0.0615 - val_mse: 8.0226e-04 - val_mae: 0.0170\n",
      "Epoch 168/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0848 - mse: 8.6069e-04 - mae: 0.0178 \n",
      "Epoch 00168: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1209s 6s/step - loss: 0.0854 - mse: 8.6075e-04 - mae: 0.0179 - val_loss: 0.0381 - val_mse: 7.3776e-04 - val_mae: 0.0153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0817 - mse: 8.3051e-04 - mae: 0.0176 \n",
      "Epoch 00169: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00169: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1222s 6s/step - loss: 0.0819 - mse: 8.3064e-04 - mae: 0.0176 - val_loss: 0.0451 - val_mse: 7.1911e-04 - val_mae: 0.0160\n",
      "Epoch 170/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0809 - mse: 8.9156e-04 - mae: 0.0182 \n",
      "Epoch 00170: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1225s 6s/step - loss: 0.0807 - mse: 8.9120e-04 - mae: 0.0182 - val_loss: 0.0763 - val_mse: 6.8836e-04 - val_mae: 0.0162\n",
      "Epoch 171/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0820 - mse: 8.7870e-04 - mae: 0.0181 \n",
      "Epoch 00171: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1225s 6s/step - loss: 0.0821 - mse: 8.7848e-04 - mae: 0.0181 - val_loss: 0.0806 - val_mse: 8.9982e-04 - val_mae: 0.0185\n",
      "Epoch 172/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0849 - mse: 8.6522e-04 - mae: 0.0179 \n",
      "Epoch 00172: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00172: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1228s 6s/step - loss: 0.0849 - mse: 8.6534e-04 - mae: 0.0179 - val_loss: 0.0465 - val_mse: 6.6756e-04 - val_mae: 0.0150\n",
      "Epoch 173/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0882 - mse: 8.5930e-04 - mae: 0.0178 \n",
      "Epoch 00173: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1231s 6s/step - loss: 0.0887 - mse: 8.5956e-04 - mae: 0.0178 - val_loss: 0.0453 - val_mse: 7.1069e-04 - val_mae: 0.0154\n",
      "Epoch 174/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0861 - mse: 8.4938e-04 - mae: 0.0178 \n",
      "Epoch 00174: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1230s 6s/step - loss: 0.0859 - mse: 8.4955e-04 - mae: 0.0178 - val_loss: 0.0944 - val_mse: 9.4665e-04 - val_mae: 0.0193\n",
      "Epoch 175/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0817 - mse: 8.1996e-04 - mae: 0.0174 \n",
      "Epoch 00175: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00175: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1232s 6s/step - loss: 0.0814 - mse: 8.2025e-04 - mae: 0.0174 - val_loss: 0.0689 - val_mse: 6.9936e-04 - val_mae: 0.0160\n",
      "Epoch 176/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0867 - mse: 9.2774e-04 - mae: 0.0188 \n",
      "Epoch 00176: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1227s 6s/step - loss: 0.0865 - mse: 9.2734e-04 - mae: 0.0188 - val_loss: 0.1261 - val_mse: 9.7885e-04 - val_mae: 0.0200\n",
      "Epoch 177/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0786 - mse: 8.6837e-04 - mae: 0.0179 \n",
      "Epoch 00177: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1193s 6s/step - loss: 0.0788 - mse: 8.6830e-04 - mae: 0.0178 - val_loss: 0.0607 - val_mse: 7.6524e-04 - val_mae: 0.0170\n",
      "Epoch 178/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0832 - mse: 8.5931e-04 - mae: 0.0178 \n",
      "Epoch 00178: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00178: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1191s 6s/step - loss: 0.0830 - mse: 8.5930e-04 - mae: 0.0178 - val_loss: 0.0850 - val_mse: 8.1180e-04 - val_mae: 0.0173\n",
      "Epoch 179/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0849 - mse: 8.6056e-04 - mae: 0.0176 \n",
      "Epoch 00179: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1197s 6s/step - loss: 0.0850 - mse: 8.6067e-04 - mae: 0.0176 - val_loss: 0.0517 - val_mse: 7.0278e-04 - val_mae: 0.0155\n",
      "Epoch 180/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0767 - mse: 8.4404e-04 - mae: 0.0177 \n",
      "Epoch 00180: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1193s 6s/step - loss: 0.0768 - mse: 8.4387e-04 - mae: 0.0177 - val_loss: 0.0725 - val_mse: 8.0184e-04 - val_mae: 0.0169\n",
      "Epoch 181/5000\n",
      "199/200 [============================>.] - ETA: 5s - loss: 0.0830 - mse: 8.8413e-04 - mae: 0.0182 \n",
      "Epoch 00181: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00181: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1199s 6s/step - loss: 0.0826 - mse: 8.8389e-04 - mae: 0.0182 - val_loss: 0.0943 - val_mse: 8.1988e-04 - val_mae: 0.0180\n",
      "Epoch 182/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0843 - mse: 8.9251e-04 - mae: 0.0181 \n",
      "Epoch 00182: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1210s 6s/step - loss: 0.0843 - mse: 8.9244e-04 - mae: 0.0181 - val_loss: 0.1042 - val_mse: 9.4498e-04 - val_mae: 0.0189\n",
      "Epoch 183/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0814 - mse: 8.1591e-04 - mae: 0.0173 \n",
      "Epoch 00183: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1215s 6s/step - loss: 0.0817 - mse: 8.1627e-04 - mae: 0.0173 - val_loss: 0.1462 - val_mse: 0.0012 - val_mae: 0.0227\n",
      "Epoch 184/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0828 - mse: 9.1910e-04 - mae: 0.0187 \n",
      "Epoch 00184: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00184: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1222s 6s/step - loss: 0.0827 - mse: 9.1854e-04 - mae: 0.0187 - val_loss: 0.0663 - val_mse: 8.2436e-04 - val_mae: 0.0174\n",
      "Epoch 185/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0790 - mse: 8.4604e-04 - mae: 0.0176 \n",
      "Epoch 00185: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1228s 6s/step - loss: 0.0786 - mse: 8.4602e-04 - mae: 0.0176 - val_loss: 0.0730 - val_mse: 8.0601e-04 - val_mae: 0.0177\n",
      "Epoch 186/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0879 - mse: 8.6617e-04 - mae: 0.0180 \n",
      "Epoch 00186: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1229s 6s/step - loss: 0.0879 - mse: 8.6631e-04 - mae: 0.0180 - val_loss: 0.0588 - val_mse: 7.7349e-04 - val_mae: 0.0167\n",
      "Epoch 187/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0847 - mse: 8.8838e-04 - mae: 0.0184 \n",
      "Epoch 00187: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00187: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1228s 6s/step - loss: 0.0846 - mse: 8.8816e-04 - mae: 0.0184 - val_loss: 0.0868 - val_mse: 9.5658e-04 - val_mae: 0.0184\n",
      "Epoch 188/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0831 - mse: 8.6339e-04 - mae: 0.0179 \n",
      "Epoch 00188: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1224s 6s/step - loss: 0.0828 - mse: 8.6325e-04 - mae: 0.0179 - val_loss: 0.1379 - val_mse: 0.0010 - val_mae: 0.0213\n",
      "Epoch 189/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0817 - mse: 8.4524e-04 - mae: 0.0177 \n",
      "Epoch 00189: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1223s 6s/step - loss: 0.0821 - mse: 8.4530e-04 - mae: 0.0177 - val_loss: 0.0759 - val_mse: 7.1927e-04 - val_mae: 0.0169\n",
      "Epoch 190/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0805 - mse: 8.8861e-04 - mae: 0.0183 \n",
      "Epoch 00190: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00190: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1222s 6s/step - loss: 0.0801 - mse: 8.8821e-04 - mae: 0.0183 - val_loss: 0.0886 - val_mse: 9.7570e-04 - val_mae: 0.0191\n",
      "Epoch 191/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0888 - mse: 8.4736e-04 - mae: 0.0179 \n",
      "Epoch 00191: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1227s 6s/step - loss: 0.0892 - mse: 8.4760e-04 - mae: 0.0179 - val_loss: 0.1138 - val_mse: 0.0011 - val_mae: 0.0203\n",
      "Epoch 192/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0855 - mse: 8.5115e-04 - mae: 0.0180 \n",
      "Epoch 00192: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1221s 6s/step - loss: 0.0855 - mse: 8.5133e-04 - mae: 0.0180 - val_loss: 0.0435 - val_mse: 7.1806e-04 - val_mae: 0.0153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 193/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0833 - mse: 8.7152e-04 - mae: 0.0181 \n",
      "Epoch 00193: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00193: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1224s 6s/step - loss: 0.0831 - mse: 8.7133e-04 - mae: 0.0181 - val_loss: 0.0609 - val_mse: 8.6454e-04 - val_mae: 0.0168\n",
      "Epoch 194/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0773 - mse: 8.3904e-04 - mae: 0.0174 \n",
      "Epoch 00194: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1232s 6s/step - loss: 0.0770 - mse: 8.3900e-04 - mae: 0.0174 - val_loss: 0.0950 - val_mse: 9.0418e-04 - val_mae: 0.0191\n",
      "Epoch 195/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0840 - mse: 8.4074e-04 - mae: 0.0176 \n",
      "Epoch 00195: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1225s 6s/step - loss: 0.0842 - mse: 8.4093e-04 - mae: 0.0176 - val_loss: 0.0718 - val_mse: 8.8634e-04 - val_mae: 0.0168\n",
      "Epoch 196/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0839 - mse: 8.8154e-04 - mae: 0.0183 \n",
      "Epoch 00196: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00196: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1220s 6s/step - loss: 0.0838 - mse: 8.8122e-04 - mae: 0.0183 - val_loss: 0.1453 - val_mse: 0.0012 - val_mae: 0.0229\n",
      "Epoch 197/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0852 - mse: 9.0258e-04 - mae: 0.0186 \n",
      "Epoch 00197: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1222s 6s/step - loss: 0.0854 - mse: 9.0236e-04 - mae: 0.0186 - val_loss: 0.0675 - val_mse: 7.4722e-04 - val_mae: 0.0169\n",
      "Epoch 198/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0799 - mse: 8.3870e-04 - mae: 0.0176 \n",
      "Epoch 00198: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1222s 6s/step - loss: 0.0805 - mse: 8.3876e-04 - mae: 0.0176 - val_loss: 0.0776 - val_mse: 8.8306e-04 - val_mae: 0.0177\n",
      "Epoch 199/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0827 - mse: 8.5690e-04 - mae: 0.0178 \n",
      "Epoch 00199: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00199: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1219s 6s/step - loss: 0.0829 - mse: 8.5691e-04 - mae: 0.0178 - val_loss: 0.0700 - val_mse: 7.1485e-04 - val_mae: 0.0165\n",
      "Epoch 200/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0785 - mse: 8.6286e-04 - mae: 0.0179 \n",
      "Epoch 00200: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1220s 6s/step - loss: 0.0784 - mse: 8.6264e-04 - mae: 0.0179 - val_loss: 0.1034 - val_mse: 8.3304e-04 - val_mae: 0.0186\n",
      "Epoch 201/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0786 - mse: 8.2997e-04 - mae: 0.0175 \n",
      "Epoch 00201: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1214s 6s/step - loss: 0.0783 - mse: 8.3003e-04 - mae: 0.0175 - val_loss: 0.1187 - val_mse: 9.0709e-04 - val_mae: 0.0198\n",
      "Epoch 202/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0839 - mse: 8.4580e-04 - mae: 0.0177 \n",
      "Epoch 00202: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00202: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1215s 6s/step - loss: 0.0838 - mse: 8.4593e-04 - mae: 0.0177 - val_loss: 0.1065 - val_mse: 8.9596e-04 - val_mae: 0.0190\n",
      "Epoch 203/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0872 - mse: 8.7261e-04 - mae: 0.0179 \n",
      "Epoch 00203: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1212s 6s/step - loss: 0.0869 - mse: 8.7259e-04 - mae: 0.0179 - val_loss: 0.0946 - val_mse: 8.9546e-04 - val_mae: 0.0194\n",
      "Epoch 204/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0854 - mse: 8.7172e-04 - mae: 0.0181 \n",
      "Epoch 00204: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1227s 6s/step - loss: 0.0857 - mse: 8.7175e-04 - mae: 0.0181 - val_loss: 0.0837 - val_mse: 8.5544e-04 - val_mae: 0.0175\n",
      "Epoch 205/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0839 - mse: 8.2099e-04 - mae: 0.0172 \n",
      "Epoch 00205: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00205: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1214s 6s/step - loss: 0.0841 - mse: 8.2143e-04 - mae: 0.0173 - val_loss: 0.0631 - val_mse: 7.6000e-04 - val_mae: 0.0163\n",
      "Epoch 206/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0915 - mse: 8.7069e-04 - mae: 0.0182 \n",
      "Epoch 00206: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1233s 6s/step - loss: 0.0914 - mse: 8.7089e-04 - mae: 0.0182 - val_loss: 0.0725 - val_mse: 7.5250e-04 - val_mae: 0.0162\n",
      "Epoch 207/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0808 - mse: 8.3464e-04 - mae: 0.0175 \n",
      "Epoch 00207: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1227s 6s/step - loss: 0.0805 - mse: 8.3476e-04 - mae: 0.0175 - val_loss: 0.0518 - val_mse: 6.8404e-04 - val_mae: 0.0147\n",
      "Epoch 208/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0834 - mse: 8.6313e-04 - mae: 0.0177 \n",
      "Epoch 00208: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00208: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1233s 6s/step - loss: 0.0830 - mse: 8.6311e-04 - mae: 0.0177 - val_loss: 0.0991 - val_mse: 9.0309e-04 - val_mae: 0.0185\n",
      "Epoch 209/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0847 - mse: 8.6522e-04 - mae: 0.0179 \n",
      "Epoch 00209: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1223s 6s/step - loss: 0.0847 - mse: 8.6538e-04 - mae: 0.0179 - val_loss: 0.0437 - val_mse: 8.4708e-04 - val_mae: 0.0161\n",
      "Epoch 210/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0884 - mse: 8.6532e-04 - mae: 0.0179 \n",
      "Epoch 00210: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1227s 6s/step - loss: 0.0892 - mse: 8.6558e-04 - mae: 0.0179 - val_loss: 0.0782 - val_mse: 8.7397e-04 - val_mae: 0.0178\n",
      "Epoch 211/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0861 - mse: 8.8171e-04 - mae: 0.0184 \n",
      "Epoch 00211: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00211: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1218s 6s/step - loss: 0.0867 - mse: 8.8165e-04 - mae: 0.0184 - val_loss: 0.0774 - val_mse: 8.5356e-04 - val_mae: 0.0170\n",
      "Epoch 212/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0913 - mse: 8.5720e-04 - mae: 0.0178 \n",
      "Epoch 00212: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1223s 6s/step - loss: 0.0912 - mse: 8.5754e-04 - mae: 0.0178 - val_loss: 0.0781 - val_mse: 7.9337e-04 - val_mae: 0.0171\n",
      "Epoch 213/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0855 - mse: 8.4311e-04 - mae: 0.0176 \n",
      "Epoch 00213: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1227s 6s/step - loss: 0.0861 - mse: 8.4338e-04 - mae: 0.0176 - val_loss: 0.0755 - val_mse: 7.8778e-04 - val_mae: 0.0176\n",
      "Epoch 214/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0818 - mse: 8.6074e-04 - mae: 0.0179 \n",
      "Epoch 00214: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00214: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1223s 6s/step - loss: 0.0817 - mse: 8.6059e-04 - mae: 0.0179 - val_loss: 0.0730 - val_mse: 9.1143e-04 - val_mae: 0.0180\n",
      "Epoch 215/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0886 - mse: 8.8898e-04 - mae: 0.0181 \n",
      "Epoch 00215: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1221s 6s/step - loss: 0.0883 - mse: 8.8901e-04 - mae: 0.0181 - val_loss: 0.0615 - val_mse: 7.8298e-04 - val_mae: 0.0169\n",
      "Epoch 216/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0783 - mse: 8.5849e-04 - mae: 0.0176 \n",
      "Epoch 00216: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1227s 6s/step - loss: 0.0787 - mse: 8.5852e-04 - mae: 0.0176 - val_loss: 0.0774 - val_mse: 8.9285e-04 - val_mae: 0.0181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 217/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0891 - mse: 8.8550e-04 - mae: 0.0183 \n",
      "Epoch 00217: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00217: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1221s 6s/step - loss: 0.0888 - mse: 8.8544e-04 - mae: 0.0183 - val_loss: 0.0745 - val_mse: 7.9814e-04 - val_mae: 0.0171\n",
      "Epoch 218/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0848 - mse: 8.6776e-04 - mae: 0.0181 \n",
      "Epoch 00218: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1220s 6s/step - loss: 0.0845 - mse: 8.6777e-04 - mae: 0.0181 - val_loss: 0.1244 - val_mse: 0.0011 - val_mae: 0.0210\n",
      "Epoch 219/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0805 - mse: 8.6161e-04 - mae: 0.0179 \n",
      "Epoch 00219: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1220s 6s/step - loss: 0.0801 - mse: 8.6140e-04 - mae: 0.0179 - val_loss: 0.0500 - val_mse: 7.4430e-04 - val_mae: 0.0164\n",
      "Epoch 220/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0834 - mse: 8.3709e-04 - mae: 0.0174 \n",
      "Epoch 00220: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00220: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1221s 6s/step - loss: 0.0832 - mse: 8.3728e-04 - mae: 0.0174 - val_loss: 0.1220 - val_mse: 9.2166e-04 - val_mae: 0.0203\n",
      "Epoch 221/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0729 - mse: 7.9775e-04 - mae: 0.0170 \n",
      "Epoch 00221: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1225s 6s/step - loss: 0.0726 - mse: 7.9795e-04 - mae: 0.0170 - val_loss: 0.0709 - val_mse: 8.1857e-04 - val_mae: 0.0167\n",
      "Epoch 222/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0858 - mse: 8.9350e-04 - mae: 0.0184 \n",
      "Epoch 00222: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1218s 6s/step - loss: 0.0858 - mse: 8.9319e-04 - mae: 0.0184 - val_loss: 0.0643 - val_mse: 9.2720e-04 - val_mae: 0.0167\n",
      "Epoch 223/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0840 - mse: 8.4975e-04 - mae: 0.0178 \n",
      "Epoch 00223: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00223: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1221s 6s/step - loss: 0.0838 - mse: 8.4983e-04 - mae: 0.0178 - val_loss: 0.0987 - val_mse: 9.4936e-04 - val_mae: 0.0189\n",
      "Epoch 224/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0781 - mse: 8.5109e-04 - mae: 0.0177 \n",
      "Epoch 00224: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1222s 6s/step - loss: 0.0784 - mse: 8.5099e-04 - mae: 0.0177 - val_loss: 0.0698 - val_mse: 7.6389e-04 - val_mae: 0.0171\n",
      "Epoch 225/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0822 - mse: 8.5912e-04 - mae: 0.0179 \n",
      "Epoch 00225: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1221s 6s/step - loss: 0.0820 - mse: 8.5902e-04 - mae: 0.0179 - val_loss: 0.0678 - val_mse: 8.4404e-04 - val_mae: 0.0175\n",
      "Epoch 226/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0823 - mse: 8.6108e-04 - mae: 0.0178 \n",
      "Epoch 00226: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00226: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1218s 6s/step - loss: 0.0825 - mse: 8.6108e-04 - mae: 0.0178 - val_loss: 0.1025 - val_mse: 8.7631e-04 - val_mae: 0.0187\n",
      "Epoch 227/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0816 - mse: 8.5622e-04 - mae: 0.0178 \n",
      "Epoch 00227: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1221s 6s/step - loss: 0.0813 - mse: 8.5624e-04 - mae: 0.0178 - val_loss: 0.0919 - val_mse: 9.1607e-04 - val_mae: 0.0182\n",
      "Epoch 228/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0848 - mse: 8.6501e-04 - mae: 0.0180 \n",
      "Epoch 00228: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1226s 6s/step - loss: 0.0847 - mse: 8.6500e-04 - mae: 0.0180 - val_loss: 0.0861 - val_mse: 7.6398e-04 - val_mae: 0.0173\n",
      "Epoch 229/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0851 - mse: 8.1224e-04 - mae: 0.0172 \n",
      "Epoch 00229: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00229: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1221s 6s/step - loss: 0.0855 - mse: 8.1271e-04 - mae: 0.0172 - val_loss: 0.0914 - val_mse: 8.4177e-04 - val_mae: 0.0177\n",
      "Epoch 230/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0878 - mse: 9.0472e-04 - mae: 0.0183 \n",
      "Epoch 00230: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1222s 6s/step - loss: 0.0876 - mse: 9.0449e-04 - mae: 0.0183 - val_loss: 0.0748 - val_mse: 7.4703e-04 - val_mae: 0.0167\n",
      "Epoch 231/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0875 - mse: 8.8420e-04 - mae: 0.0184 \n",
      "Epoch 00231: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1212s 6s/step - loss: 0.0874 - mse: 8.8405e-04 - mae: 0.0184 - val_loss: 0.0823 - val_mse: 0.0010 - val_mae: 0.0194\n",
      "Epoch 232/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0812 - mse: 8.6181e-04 - mae: 0.0178 \n",
      "Epoch 00232: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00232: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1222s 6s/step - loss: 0.0812 - mse: 8.6168e-04 - mae: 0.0178 - val_loss: 0.1014 - val_mse: 9.9165e-04 - val_mae: 0.0190\n",
      "Epoch 233/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0751 - mse: 8.4004e-04 - mae: 0.0174 \n",
      "Epoch 00233: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1223s 6s/step - loss: 0.0756 - mse: 8.4001e-04 - mae: 0.0174 - val_loss: 0.0531 - val_mse: 6.5112e-04 - val_mae: 0.0152\n",
      "Epoch 234/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0843 - mse: 8.4806e-04 - mae: 0.0179 \n",
      "Epoch 00234: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1223s 6s/step - loss: 0.0840 - mse: 8.4824e-04 - mae: 0.0179 - val_loss: 0.1535 - val_mse: 0.0012 - val_mae: 0.0229\n",
      "Epoch 235/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0839 - mse: 8.6823e-04 - mae: 0.0180 \n",
      "Epoch 00235: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00235: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1221s 6s/step - loss: 0.0838 - mse: 8.6809e-04 - mae: 0.0180 - val_loss: 0.0540 - val_mse: 7.5584e-04 - val_mae: 0.0167\n",
      "Epoch 236/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0903 - mse: 8.5317e-04 - mae: 0.0176 \n",
      "Epoch 00236: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1224s 6s/step - loss: 0.0902 - mse: 8.5352e-04 - mae: 0.0176 - val_loss: 0.0419 - val_mse: 7.0168e-04 - val_mae: 0.0149\n",
      "Epoch 237/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0820 - mse: 8.6111e-04 - mae: 0.0178 \n",
      "Epoch 00237: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1218s 6s/step - loss: 0.0819 - mse: 8.6099e-04 - mae: 0.0178 - val_loss: 0.1545 - val_mse: 0.0011 - val_mae: 0.0222\n",
      "Epoch 238/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0779 - mse: 8.4400e-04 - mae: 0.0173 \n",
      "Epoch 00238: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00238: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1225s 6s/step - loss: 0.0776 - mse: 8.4410e-04 - mae: 0.0173 - val_loss: 0.1002 - val_mse: 9.7933e-04 - val_mae: 0.0192\n",
      "Epoch 239/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0933 - mse: 9.3692e-04 - mae: 0.0190 \n",
      "Epoch 00239: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1225s 6s/step - loss: 0.0929 - mse: 9.3649e-04 - mae: 0.0190 - val_loss: 0.1407 - val_mse: 0.0012 - val_mae: 0.0221\n",
      "Epoch 240/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0828 - mse: 8.7042e-04 - mae: 0.0180 \n",
      "Epoch 00240: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1230s 6s/step - loss: 0.0835 - mse: 8.7042e-04 - mae: 0.0180 - val_loss: 0.0518 - val_mse: 6.5998e-04 - val_mae: 0.0151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 241/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0762 - mse: 8.2434e-04 - mae: 0.0174 \n",
      "Epoch 00241: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00241: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1228s 6s/step - loss: 0.0762 - mse: 8.2429e-04 - mae: 0.0174 - val_loss: 0.1295 - val_mse: 0.0011 - val_mae: 0.0208\n",
      "Epoch 242/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0818 - mse: 8.7677e-04 - mae: 0.0178 \n",
      "Epoch 00242: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1221s 6s/step - loss: 0.0816 - mse: 8.7669e-04 - mae: 0.0178 - val_loss: 0.0798 - val_mse: 8.4996e-04 - val_mae: 0.0170\n",
      "Epoch 243/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0840 - mse: 8.4980e-04 - mae: 0.0177 \n",
      "Epoch 00243: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1221s 6s/step - loss: 0.0838 - mse: 8.4994e-04 - mae: 0.0177 - val_loss: 0.1065 - val_mse: 9.0079e-04 - val_mae: 0.0191\n",
      "Epoch 244/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0805 - mse: 8.6655e-04 - mae: 0.0182 \n",
      "Epoch 00244: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00244: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1227s 6s/step - loss: 0.0802 - mse: 8.6623e-04 - mae: 0.0182 - val_loss: 0.1687 - val_mse: 0.0014 - val_mae: 0.0248\n",
      "Epoch 245/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0886 - mse: 9.0964e-04 - mae: 0.0186 \n",
      "Epoch 00245: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1225s 6s/step - loss: 0.0890 - mse: 9.0940e-04 - mae: 0.0186 - val_loss: 0.0756 - val_mse: 8.4206e-04 - val_mae: 0.0172\n",
      "Epoch 246/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0764 - mse: 8.3183e-04 - mae: 0.0174 \n",
      "Epoch 00246: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1226s 6s/step - loss: 0.0762 - mse: 8.3191e-04 - mae: 0.0174 - val_loss: 0.0703 - val_mse: 7.7428e-04 - val_mae: 0.0167\n",
      "Epoch 247/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0912 - mse: 8.8862e-04 - mae: 0.0182 \n",
      "Epoch 00247: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00247: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1219s 6s/step - loss: 0.0910 - mse: 8.8866e-04 - mae: 0.0182 - val_loss: 0.0876 - val_mse: 9.3877e-04 - val_mae: 0.0189\n",
      "Epoch 248/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0800 - mse: 8.4111e-04 - mae: 0.0177 \n",
      "Epoch 00248: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1222s 6s/step - loss: 0.0799 - mse: 8.4121e-04 - mae: 0.0177 - val_loss: 0.0872 - val_mse: 8.4655e-04 - val_mae: 0.0179\n",
      "Epoch 249/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0795 - mse: 8.1708e-04 - mae: 0.0172 \n",
      "Epoch 00249: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1224s 6s/step - loss: 0.0796 - mse: 8.1731e-04 - mae: 0.0172 - val_loss: 0.0747 - val_mse: 8.4301e-04 - val_mae: 0.0169\n",
      "Epoch 250/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0897 - mse: 9.1760e-04 - mae: 0.0187 \n",
      "Epoch 00250: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00250: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1226s 6s/step - loss: 0.0903 - mse: 9.1737e-04 - mae: 0.0187 - val_loss: 0.0891 - val_mse: 8.5244e-04 - val_mae: 0.0181\n",
      "Epoch 251/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0849 - mse: 8.2768e-04 - mae: 0.0175 \n",
      "Epoch 00251: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1222s 6s/step - loss: 0.0849 - mse: 8.2794e-04 - mae: 0.0175 - val_loss: 0.0274 - val_mse: 7.6940e-04 - val_mae: 0.0158\n",
      "Epoch 252/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0799 - mse: 8.1933e-04 - mae: 0.0172 \n",
      "Epoch 00252: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1221s 6s/step - loss: 0.0805 - mse: 8.1957e-04 - mae: 0.0172 - val_loss: 0.1208 - val_mse: 9.7630e-04 - val_mae: 0.0199\n",
      "Epoch 253/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0771 - mse: 8.6284e-04 - mae: 0.0179 \n",
      "Epoch 00253: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00253: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1221s 6s/step - loss: 0.0771 - mse: 8.6268e-04 - mae: 0.0179 - val_loss: 0.0401 - val_mse: 6.9156e-04 - val_mae: 0.0150\n",
      "Epoch 254/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0941 - mse: 9.0136e-04 - mae: 0.0185 \n",
      "Epoch 00254: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1228s 6s/step - loss: 0.0937 - mse: 9.0144e-04 - mae: 0.0185 - val_loss: 0.0719 - val_mse: 8.8703e-04 - val_mae: 0.0174\n",
      "Epoch 255/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0769 - mse: 8.3892e-04 - mae: 0.0177 \n",
      "Epoch 00255: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1223s 6s/step - loss: 0.0769 - mse: 8.3889e-04 - mae: 0.0177 - val_loss: 0.0843 - val_mse: 8.5585e-04 - val_mae: 0.0180\n",
      "Epoch 256/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0808 - mse: 8.5748e-04 - mae: 0.0177 \n",
      "Epoch 00256: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00256: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1220s 6s/step - loss: 0.0805 - mse: 8.5743e-04 - mae: 0.0177 - val_loss: 0.1467 - val_mse: 0.0012 - val_mae: 0.0224\n",
      "Epoch 257/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0894 - mse: 8.7924e-04 - mae: 0.0179 \n",
      "Epoch 00257: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1221s 6s/step - loss: 0.0899 - mse: 8.7935e-04 - mae: 0.0179 - val_loss: 0.0623 - val_mse: 7.5616e-04 - val_mae: 0.0170\n",
      "Epoch 258/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0795 - mse: 8.6486e-04 - mae: 0.0180 \n",
      "Epoch 00258: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1222s 6s/step - loss: 0.0804 - mse: 8.6478e-04 - mae: 0.0180 - val_loss: 0.1038 - val_mse: 9.0368e-04 - val_mae: 0.0189\n",
      "Epoch 259/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0779 - mse: 8.2832e-04 - mae: 0.0174 \n",
      "Epoch 00259: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00259: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1217s 6s/step - loss: 0.0784 - mse: 8.2843e-04 - mae: 0.0174 - val_loss: 0.1243 - val_mse: 9.5705e-04 - val_mae: 0.0206\n",
      "Epoch 260/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0815 - mse: 8.6457e-04 - mae: 0.0180 \n",
      "Epoch 00260: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1217s 6s/step - loss: 0.0815 - mse: 8.6446e-04 - mae: 0.0180 - val_loss: 0.1030 - val_mse: 8.8394e-04 - val_mae: 0.0190\n",
      "Epoch 261/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0821 - mse: 8.7128e-04 - mae: 0.0180 \n",
      "Epoch 00261: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1224s 6s/step - loss: 0.0826 - mse: 8.7107e-04 - mae: 0.0180 - val_loss: 0.1350 - val_mse: 0.0011 - val_mae: 0.0218\n",
      "Epoch 262/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0751 - mse: 8.1363e-04 - mae: 0.0172 \n",
      "Epoch 00262: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00262: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1221s 6s/step - loss: 0.0751 - mse: 8.1382e-04 - mae: 0.0172 - val_loss: 0.0996 - val_mse: 8.9826e-04 - val_mae: 0.0189\n",
      "Epoch 263/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0854 - mse: 8.3686e-04 - mae: 0.0176 \n",
      "Epoch 00263: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1224s 6s/step - loss: 0.0852 - mse: 8.3714e-04 - mae: 0.0176 - val_loss: 0.1871 - val_mse: 0.0013 - val_mae: 0.0250\n",
      "Epoch 264/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0849 - mse: 8.4543e-04 - mae: 0.0177 \n",
      "Epoch 00264: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1212s 6s/step - loss: 0.0847 - mse: 8.4562e-04 - mae: 0.0177 - val_loss: 0.1379 - val_mse: 0.0011 - val_mae: 0.0217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 265/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0912 - mse: 8.7195e-04 - mae: 0.0180 \n",
      "Epoch 00265: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00265: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1215s 6s/step - loss: 0.0911 - mse: 8.7213e-04 - mae: 0.0180 - val_loss: 0.0690 - val_mse: 8.1492e-04 - val_mae: 0.0169\n",
      "Epoch 266/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0831 - mse: 8.7249e-04 - mae: 0.0182 \n",
      "Epoch 00266: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1220s 6s/step - loss: 0.0827 - mse: 8.7249e-04 - mae: 0.0182 - val_loss: 0.0761 - val_mse: 8.4093e-04 - val_mae: 0.0175\n",
      "Epoch 267/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0829 - mse: 8.4559e-04 - mae: 0.0176 \n",
      "Epoch 00267: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1221s 6s/step - loss: 0.0834 - mse: 8.4575e-04 - mae: 0.0176 - val_loss: 0.0402 - val_mse: 6.4054e-04 - val_mae: 0.0142\n",
      "Epoch 268/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0839 - mse: 8.8732e-04 - mae: 0.0182 \n",
      "Epoch 00268: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00268: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1226s 6s/step - loss: 0.0842 - mse: 8.8721e-04 - mae: 0.0182 - val_loss: 0.0214 - val_mse: 6.1261e-04 - val_mae: 0.0138\n",
      "Epoch 269/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0773 - mse: 8.4763e-04 - mae: 0.0177 \n",
      "Epoch 00269: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1222s 6s/step - loss: 0.0770 - mse: 8.4753e-04 - mae: 0.0177 - val_loss: 0.1402 - val_mse: 0.0011 - val_mae: 0.0215\n",
      "Epoch 270/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0852 - mse: 8.6792e-04 - mae: 0.0179 \n",
      "Epoch 00270: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1225s 6s/step - loss: 0.0855 - mse: 8.6798e-04 - mae: 0.0179 - val_loss: 0.0556 - val_mse: 7.8536e-04 - val_mae: 0.0163\n",
      "Epoch 271/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0829 - mse: 8.0919e-04 - mae: 0.0172 \n",
      "Epoch 00271: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00271: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1216s 6s/step - loss: 0.0831 - mse: 8.0963e-04 - mae: 0.0172 - val_loss: 0.0645 - val_mse: 7.3319e-04 - val_mae: 0.0168\n",
      "Epoch 272/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0879 - mse: 9.0022e-04 - mae: 0.0185 \n",
      "Epoch 00272: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1215s 6s/step - loss: 0.0879 - mse: 9.0009e-04 - mae: 0.0185 - val_loss: 0.0820 - val_mse: 8.7888e-04 - val_mae: 0.0177\n",
      "Epoch 273/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0800 - mse: 8.7653e-04 - mae: 0.0182 \n",
      "Epoch 00273: mse did not improve from 0.00079\n",
      "200/200 [==============================] - 1220s 6s/step - loss: 0.0799 - mse: 8.7618e-04 - mae: 0.0182 - val_loss: 0.0995 - val_mse: 9.0349e-04 - val_mae: 0.0189\n",
      "Epoch 274/5000\n",
      "199/200 [============================>.] - ETA: 6s - loss: 0.0835 - mse: 8.4678e-04 - mae: 0.0177 \n",
      "Epoch 00274: mse did not improve from 0.00079\n",
      "\n",
      "Epoch 00274: ReduceLROnPlateau reducing learning rate to 1e-15.\n",
      "200/200 [==============================] - 1219s 6s/step - loss: 0.0833 - mse: 8.4698e-04 - mae: 0.0177 - val_loss: 0.1002 - val_mse: 0.0010 - val_mae: 0.0198\n",
      "Epoch 275/5000\n",
      "152/200 [=====================>........] - ETA: 4:50 - loss: 0.0852 - mse: 8.6627e-04 - mae: 0.0179"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7305dda89235>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintegral_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mae'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/.conda/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1245\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m           \u001b[0mreset_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1247\u001b[0;31m           output_loss_metrics=self._output_loss_metrics)\n\u001b[0m\u001b[1;32m   1248\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msymbolic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, reset_metrics, output_loss_metrics)\u001b[0m\n\u001b[1;32m    293\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    296\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    243\u001b[0m                         'compiling the model.')\n\u001b[1;32m    244\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         model.optimizer.apply_gradients(zip(grads,\n\u001b[1;32m    247\u001b[0m                                             model.trainable_weights))\n",
      "\u001b[0;32m~/.conda/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_RealDivGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m   \u001b[0msx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m   \u001b[0msy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m   \u001b[0mrx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_gradient_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(input, name, out_type)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m   \"\"\"\n\u001b[0;32m--> 313\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mshape_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape_internal\u001b[0;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moptimize\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/tf-nightly/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(input, out_type, name)\u001b[0m\n\u001b[1;32m   8402\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m   8403\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Shape\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8404\u001b[0;31m         name, _ctx._post_execution_callbacks, input, \"out_type\", out_type)\n\u001b[0m\u001b[1;32m   8405\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8406\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cb = [tf.keras.callbacks.ModelCheckpoint('asd.h5', monitor='mse', verbose=1, save_best_only=True, save_weights_only=True), tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', min_lr = 1e-15, verbose = True, patience = 3)]\n",
    "mod.compile(loss = mod.integral_loss, optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3), metrics = ['mse', 'mae'])\n",
    "mod.run_eagerly = True\n",
    "mod.fit_generator(generator=dataset_generator(), steps_per_epoch=200, epochs=5000, validation_data=dataset_generator(), validation_steps=3, callbacks=cb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = dataset_generator()\n",
    "inp, soln = next(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = np.meshgrid(np.linspace(0, soln.shape[-2]*inp[-1][0,0], soln.shape[-2]), np.linspace(0, soln.shape[-1]*inp[-1][0,0], soln.shape[-1]), indexing = 'ij')\n",
    "p_r = np.random.randint(0,soln.shape[0])\n",
    "#z = soln[p_r,0,...]\n",
    "#z = mod(inp)[p_r,0,...]\n",
    "#z = tf.cast(mod(inp)[p_r,0,...], tf.float64)-soln[p_r,0,...]\n",
    "z = tf.divide(tf.cast(mod(inp)[p_r,0,...], tf.float64)-soln[p_r,0,...], soln[p_r,0,...])\n",
    "z_min, z_max = -np.abs(z).max(), np.abs(z).max()\n",
    "fig, ax = plt.subplots()\n",
    "c = ax.pcolormesh(x, y, z, cmap='RdBu', vmin=z_min, vmax=z_max)\n",
    "ax.set_title('percentage error')\n",
    "ax.axis([x.min(), x.max(), y.min(), y.max()])\n",
    "fig.colorbar(c, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = np.meshgrid(np.linspace(0, soln.shape[-2]*inp[-1][0,0], soln.shape[-2]), np.linspace(0, soln.shape[-1]*inp[-1][0,0], soln.shape[-1]), indexing = 'ij')\n",
    "#p_r = np.random.randint(0,soln.shape[0])\n",
    "z = soln[p_r,0,...]\n",
    "#z = mod(inp)[p_r,0,...]\n",
    "#z = mod([tf.expand_dims(b['left'], axis = 1), dx * tf.ones((soln.shape[0],1), dtype = 'float64')])[p_r,0,...]\n",
    "#z = mod(tf.expand_dims(b['left'], axis = 1))[p_r,0,...]\n",
    "#z = tf.divide(mod([tf.expand_dims(b['left'], axis = 1), dx * tf.ones((soln.shape[0],1), dtype = 'float64')])[p_r,0,...]-soln[p_r,0,...], soln[p_r,0,...])\n",
    "z_min, z_max = -np.abs(z).max(), np.abs(z).max()\n",
    "fig, ax = plt.subplots()\n",
    "c = ax.pcolormesh(x, y, z, cmap='RdBu', vmin=z_min, vmax=z_max)\n",
    "ax.set_title('cholesky')\n",
    "ax.axis([x.min(), x.max(), y.min(), y.max()])\n",
    "fig.colorbar(c, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbnn = Dirichlet_BC_NN_2B(data_format = 'channels_first', mse_component_weight = 1e+1, x_output_resolution = 64)\n",
    "from generate_laplace_soln import generate_laplace_soln\n",
    "def dbnn_dataset_generator():\n",
    "    while True:\n",
    "        nx = 64#np.random.randint(64,128)\n",
    "        ny = 64#np.random.randint(64,128)\n",
    "        boundaries, soln, dx = generate_laplace_soln(batch_size=75, nonzero_boundaries=['left'], smoothness=np.random.randint(5,20), nx = nx, ny = ny)\n",
    "        #print(boundaries)\n",
    "        yield [boundaries['left'], dx * tf.ones((soln.shape[0],1), dtype = 'float64')], soln\n",
    "s = dbnn_dataset_generator()\n",
    "inp,soln = next(s)\n",
    "dbnn(inp)\n",
    "dbnn.load_weights('Dirichlet_BC_NN.h5')\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEICAYAAABCnX+uAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJztnXuQHWd55p/nzEiyfJc9tnGMfCGILOZSvgiTXWq5mkTZbGzvxiE2RdausqMiwctu2KSAhSIpJ9QaUhVCKt5dBCEhVwNOBZTEhIAvm42DibVgbKTEtiw7WJZvkiVjWbeZOe/+cXrE6be/mf66p/uc7tPPr6pLp7u//vrtPkfffP30e6GZQQghRHvpjdsAIYQQy0MDuRBCtBwN5EII0XI0kAshRMvRQC6EEC1HA7kQQrQcDeSidZD8NZJ/PG47hGgKGshFqyF5LkkjOT1uW4QYFxrIRW1ocBViNGggF5VC8jGS7yd5P4AXSZ5N8s9JPkvyUZLvHWp7CcktJL9P8mmSv5VsfzPJnYF+Lw2c8u+Sf/eR3E/yX9d2cUI0FA3kog6uBvCTAE4B8BcAvgPgLABvA/BfSf540u6TAD5pZicC+GEAXyhxrjcm/55sZseb2TeWZbkQLUQDuaiD3zGzxwG8GsBpZnajmR0xsx0APg3gqqTdLICXk5wxs/1mds+4DBaizWggF3XwePLvOQB+iOS+hQXAfwdwRrL/OgCvAPDPJO8l+e/HYKsQrUcvo0QdLKTUfBzAo2a2LtjI7GEAV5PsAfiPAG4leSqAFwEcu9CO5BSA03LOJURn0Yxc1Mk/Avh+8vJzNckpkq8m+ToAIPkukqeZWR/AvuSYeQAPATiG5E+SXAHgwwBWLXKOZwH0Abys3ksRorloIBe1YWbzAH4KwAUAHgWwG8BnAJyUNNkAYCvJ/Ri8+LzKzA6Z2fMAfjFp+wQGM/SdCGBmBwB8FMDdiXzzozVekhCNhCosIYQQ7UYzciGEaDkayIUQouVoIBdCiJajgVwIIVrO2PzIZ2Zm7Jyzz150v5EjtGZxqJfBombK/Nar+F3mnTd0jnH9v/z2t76128wWiyWIonfiSw1zh6La2sE9XzWzDcs53ygZ20B+ztln4+677150vwZy0RU0kOdz7OrV/7LsTuYOYfpHLotqOnvf788s+3wjRJGdQohuQIK9qXFbUQsayIUQHYHoTa8ctxG10NiBPObR0T/m+WPqeAwM9dkU+aXMo/I4GNXj+Sjkh3ExCruaeu2l0YxcCCHaDQFwSgO5EEK0FxI9zciFEKLdSFppIHkaaB2acFN05hBNtm2YttgJtMvWojTF/XBk91gauRBCtBuC6E2vGLcZtaCBXAjRDTQjFxOB9dPrjEi144/xxPQhUowrkrMo43I/rPO8GsiFEKLNkHI/FEKINkNoRi6EEO2GPUwpRF+MlTr0bVE5ExfWPklwcmfkelMlhOgExMBrJWbJ7YvcQPJBkttJfiCw/xMk70uWh0juG9o3P7RvcxXXphm5EKIzVDEjJzkF4GYAbwewE8C9JDeb2baFNmb2S0Pt/zOAC4e6OGhmFyzbkCE0kOdRhaRRh4teFbJJXX102CWxrmjIvEyfZfpoCiOzqzo/8ksAbDezHYNueQuAywFsW6T91QB+tYoTL0Z3/8cJITpGIWllhuSWoWXjUEdnAXh8aH1nsi17RvIcAOcBuGNo8zFJn/eQvKKKK9OMXAjRCUiityLaa2W3ma1frKvAtsUeka4CcKuZzQ9tO9vMdpF8GYA7SD5gZo/EGhZCM3IhRDdgZS87dwJYO7T+UgC7Fml7FYA/G95gZruSf3cAuAtp/bwUGsjHgfWLL22izbYXxMjUUhc0Sy2iHBUN5PcCWEfyPJIrMRisM94nJH8EwBoA3xjatobkquTzDIA3YHFtPZqogTzP1SZp8w6S20huJfmnyzVMCCGqptdj1LIUZjYH4AYAXwXwTwC+YGZbSd5I8rKhplcDuMUs9Zf3lQC2kPwOgDsB3DTs7VKWXI08xtWG5DoAHwTwBjPbS/L05RomhBBVQhLMGaRjMbPbANzmtn3Erf9a4Lh/APCaSowYIuZlZ4yrzc8DuNnM9gKAmT1TtaEjow4pYMLlhVxG4Y7ZEOqSV4q6HzbV1XDcTE1N5m8v5qpiXG1eAeAVJO9OXGo2hDoiuXHBnefZ3bvLWSyEEGUgwB6jlrYRMyOPcbWZBrAOwJsxeIP7f0m+2sz2pQ4y2wRgEwBcfNFFemMjhBgZg+yH7RukY4gZyGNcbXYCuMfMZgE8SvJBDAb2eyuxcozQyQI2JlmgCjvK9FHL9U+Q1OIlDkkaTYboTej3E/M/KMbV5ksA3gIcdal5BYAdVRoqhBDLosvSipnNkVxwtZkC8NkFVxsAW8xsc7Lvx0huAzAP4FfMbE+dhgshRFHaOEjHEBWin+dqk/hJvi9ZhBCicZDA1HSHB/LWUKZQcFHXwIj2TdXVR9XnuK63CdSV/VBUAyf0u5isgVwIIRaBzI/abCsayIUQnaHTGnkjGFEhhTrkiBiKnrcKO8d1rRlaXJyiioIPbTpv29FALoQQbYaYWD9yDeRCiE5AEL3pdjzpFUUDuRCiGxB62dlZJiicfFRU4n7ZkvsubbpdyP1QCCFazCBp1ritqAcN5EKIbiBpRRQiRxYo5fZXhdRQpo+mSBx1XL+nRdO1SZJ0RnctRG9CC0toIBdCdAJqRi6EEO1HAUFCCNFiSGBqQgfyyRSMGgatn1qisH56KUMVfeT1GVocpa6/qB1l2rCXXqowi8wsohh13sOpHqOWPEhuIPkgye0kPxDYfy3JZ0nelyzXD+27huTDyXJNFdelGbkQohMQcYN0bj/kFICbAbwdgzKX95LcbGbbXNPPm9kN7thTAPwqgPUY1D7+f8mxe5djk2bkQohOQAIrp3tRSw6XANhuZjvM7AiAWwBcHmnGjwP4mpk9lwzeXwOwofRFJWhGXgGVyAVVFLioozByWxiR3U2RSqqww3v9NeTSaoMEpuNn5DMktwytbzKzTcnnswA8PrRvJ4DXB/r4aZJvBPAQgF8ys8cXOfasWKMWQwO5EKITEIVedu42s/VLdOXxzvB/CeDPzOwwyXcD+ByAt0YeWxhJK0KIbsC4F50Rg/1OAGuH1l8KYNdwAzPbY2aHk9VPA7g49tgyaCAXQnSCwYy8F7XkcC+AdSTPI7kSwFUANqfORZ45tHoZgH9KPn8VwI+RXENyDYAfS7Yti4mWVkpl4fNaqxMSx1ZVJ0bQrKGQtGgHPsy9KVp+06jCa8XM5kjegMEAPAXgs2a2leSNALaY2WYA7yV5GYA5AM8BuDY59jmSv47BHwMAuNHMnluuTRM9kAshxAI9MsYjJQozuw3AbW7bR4Y+fxDABxc59rMAPluJIQlRV7Uc53chhGgKU2TU0jZyZ+TLcX5fFiUKJZcpYFCLVJKRQfz+EUkaeVnlYn6wZXzUKsj+WKoYRVGaktmxAtokpYwrc+Mkh+jHSCtHnd8BgOSC87sfyIUQotFM6kAeMwWJdWD/aZL3k7yV5NrAfpDcSHILyS3P7t5dwlwhhCjHQkBQzNI2YgbyWOf3c83stQC+joHze/Ygs01mtt7M1p82M1PMUiE6wgTVjGgUBKsK0W8cMdJKlPP70OqnAXxs+aalidFV89qMzXUwhqL/e0Pti+qkZUaMrsV1j4DQ1zCO29yUr7YuDX2SNfKYPz3LcX4XQohGsBCiX0Ua26aROyNfjvO7EEI0hgmekUcFBC3H+b0qQu5ojZFKRmFHjKucxNVW0OavqSnySxmqykfeRBTZKYToDBrIhRCixfSSwhKTyEQN5F5+KSW9tPm5N486Ihmr8J4RYhR0XSMXQoi2Q7Qzj0oMGsiFEJ2hp4FcCCHaCwFMTeY4roF8JJTS6sdQ0DnmmBZnCKwiW2ZTUWGJCAj0pJELIUR7IYAV+WXcWokGciFEJ5C00hIy7oYTVDggIwsEGxW8viqklzJ1UCOOyXMdnSRZJIY2R1Tm4WWg2gpPkJJWhBCizRDyWhFCiNYzqdJKt55PhRCdhQRWTPWilvy+cgvSv4/ktqRq2u0kzxnaNz9UqH6zP7YM3ZqRh3RXr7Xm6cZWpvjwVPFjylCwsEaMzpx7TPCcro1/nK3g3UXMtRQtNFLJe4cxIXfDfKqSViIL0n8bwHozO0DyFwB8HMDPJvsOmtkFyzZkiHb8SoUQogKmGLfkcLQgvZkdAbBQkP4oZnanmR1IVu/BoLJabWggF0J0AoLoMW4BMLNQKD5ZNg51FVuQfoHrAHxlaP2YpM97SF5RxbV1S1ppChHuVVW4UpbJ/tiYYh0dph/4feRJAnVEdsZ4AbbKLbJY9sPdZrZ+8Z4yhJU58l0A1gN409Dms81sF8mXAbiD5ANm9kisYSE0kAshOsFAI6+kq9yC9ABA8lIAHwLwJjM7vLDdzHYl/+4geReACwEsayCXtCKE6AQLIfoxSw4xBekvBPApAJeZ2TND29eQXJV8ngHwBgDDL0lLoRm5EKIbEIjwLMwlsiD9bwI4HsAXOdCbvmdmlwF4JYBPkexjMJG+yXm7lKKxA/kkZ6oLUVSbDrUveo+q0NCb8r1I2xd5VBnZGVGQ/tJFjvsHAK+pxIghGjuQCyFEtahCkBBCtJpJzrUS9VycF4461O5KkkZyMbed8cJedmkq1k8vEdD6qWUU+HPS2y3JozCL+DW3ErPsMi4GIfqMWtpG7kg2FI76EwDOB3A1yfMD7U4A8F4A36zaSCGEqAIybmkbMVPS3HDUhF/HIJ/AoQrtE0KIyuiBUUvbiBnIc8NRE5/JtWb2V0t1RHLjQsjrs7t3FzZWCCHKQkzujDzmZeeS4agkewA+AeDavI7MbBOATQBw8UUXFVLLgppv0bD1mOyHZXDCX8ZFzyIyBuZmXYzQmotmcixDmfuVF8dd1/ciKsenD2ibfj+hBYKiBvK8cNQTALwawF2J4/tLAGwmeZmZbanKUCGEWBYtnW3HEDOQHw1HBfAEBuGo71zYaWbPA5hZWE9yB/yyBnEhRJNgl/3II8NRm0EFUkJu0V9EFImogjLZD/OO8ZSSSSanoHUZ6sgyGMp26GmbhNFUuiyt5Iajuu1vXr5ZQghRPRM6jiuyUwjRDSY5slMDeR2eHTkSR5QHTtFzAMVljjLXHuEZ4/+rBMokuAPK1EHtlqST8Rbxd9ndwzIRlFV4pGSkp4bNgSd0HNdALoToDpP6518DuRCiE7BYqbdWoYFcCNEZJK2I0VJGvx5FpsGmaNVjsqMKd8O2EOMW6cncn0AX47qHhKQVIYRoPZzQP8QayIUQ3YAdDwgaCWWSRuUlY6qCjCth6FmxAkkjr4++2x96RiyaNKuShGElknll+gg9wtcvE2XqjZa4HzHqw4ROAgFk5ZcqQuDrkl4IoIU1I6JozkAuhBA1M6nSyqRq/0IIkWIQ2Rm35PaVU/6S5CqSn0/2f5PkuUP7Pphsf5Dkj1dxbRrIhRCdgZHLkn3Elb+8DsBeM3s5BvUaPpYcez4GGWRfBWADgP+Z9LcsmiutVKI7l9DQi2YQjOgjE5If02d/6T4s9DfY9+t1dU8Vf8YjdOWs7cX7KFWcIuc8uXbF2raEWaJJVFbI+mj5SwAguVD+cttQm8sB/Fry+VYAv8uBrnM5gFvM7DCAR0luT/r7xnIM0oxcCNENIsu8JWP9zEJZymTZONRTbvnL4TZmNgfgeQCnRh5bmObOyIUQokJoBvbnY5vvNrP1i3UV2OYf6BZrE3NsYbo1kId8xYo+aoWy/eVJJxVIKXntS1Gmj16Ei2MlWRgLuiwqg2IjaHz2w2qin/PKXw632UlyGsBJAJ6LPLYw+iULITqCDf6YxyxLc7T8JcmVGLy89JXSNgO4Jvl8JYA7zMyS7VclXi3nAVgH4B+Xe2XdmpELIbpNmUTtmS6iyl/+HoA/Sl5mPofBYI+k3RcweDE6B+A9Zhat9yyGBnIhRDcwqyyxXF75SzM7BOBnFjn2owA+WokhCe0ZyCv4SxrVbxXuh3nh9FW4NIaa5LjX5bWvjA5r0TE/07ysgjX90gvbESLvjV5M9sNxUpFG3jjaM5ALIcSyMKA/N24jakEDuRCiGxhGk7N/DGgg99RR0CGiz3wXRu9uF9FHHcS4LE4t7aKYfRzvjvQSooz6kFuMOQIf5VhGaqmCUSQxTc5UjdtuA4n6HxSRIObdJB8geR/Jvw/kHRBCiLFD60ctbSN3II9MEPOnZvYaM7sAwMcB/FbllgohxHKpxo+8ccRIK7kJYszs+0Ptj0Pj3lULITqPGRAfot8qYgbyUJKX1/tGJN8D4H0AVgJ4a6ijJPHMRgBYu3ZtqEm1VPGXtWjofKCNxdgRU4loqfYlKCNFRunZBd0PQ/c0dyYwJl29jIxcVHvuB5oXLVEWOuc4NPHQOaqoIlSWNsomMcT8b4hK8mJmN5vZDwN4P4APhzoys01mtt7M1p82M1PMUiGEWBaVheg3jpgZedEkL7cA+F/LMUoIIWqhhYN0DDED+dEEMQCewCBnwDuHG5BcZ2YPJ6s/CeBhjILMl1IsshEISAW5BR0i7KghcyFdIIP1Al9dJqou79G5hB0RbTJFL+ooAp3z3YfPM55H+qa4+eURk4c175hGU2GIftPIHcgjE8TcQPJSALMA9uIHWb+EEKIREJOrkUcFBEUkiPkvFdslhBAVY8B8d71WhBCi/ShEvwFEVObJutLku8H5PnIfvUL781wHYzTxvCyMMXbkadP++ktptRGuhf56fch+UyiRpbHwawhUo4l7l0TvjujPETpjUTvCfSxtR9PptLQihBDtp8MvO4UQYmLQQF4DQzc1m/2vIY5NFRSaiHqcy5N4YmSACrIw5sKp/D7HEXUZY0dOmr1ghGnBa4mRLxryy87YUUY28cfE/Lcd23/tjofoCyHEBGCwudlxG1ELGsiFEN3AoBl5I8n1yoiQEqqQAbyXRua8+XJEfmGJ4sm78gjJBrl1Pi3CsyMvadiopJeW1A7NyhMR2kMFUaq+h7Z5oBTFYLAJ9SNv5i9bCCGqxjCYdMUsy4DkKSS/RvLh5N81gTYXkPwGya0k7yf5s0P7/oDko0mhnvtIXpB3Tg3kQoiOkLzsjFmWxwcA3G5m6wDcnqx7DgD4T2b2KgAbAPw2yZOH9v+KmV2QLPflnbDd0ooQQsRiI3vZeTmANyefPwfgLgzSew+ZYg8Nfd5F8hkApwHYV+aErRnIyxR0iKJo0eMIfTtXIw5poAU18jh3u6XvR1ASrcKF0QqKrRHXkqvdj0n/LuNuGCockX+e9DpzIz2zfeRp4GXsyqMprpYDDBY/254huWVofZOZbYo89gwzexIAzOxJkqcv1ZjkJRgU5HlkaPNHSX4EyYzezA4v1UdrBnIhhFgWxbxWdpvZ+sV2kvw6gJcEdn2oiEkkzwTwRwCusR/M+D4I4CkMBvdNGMzmb1yqHw3kQoiOYMt+kXm0J7NLF9tH8mmSZyaz8TMBPLNIuxMB/DWAD5vZPUN9P5l8PEzy9wH8cp497R7Ii0opZaL//DGhH0Ke7BEzCyjobliqDmjR/QGiinU4F0XOu6IYMcmpCkbUBusRFpRbYtrnllItdMaFPtNHxUgcGRfFMdbBbA2GUbkfbsagJsNNyb9f9g1IrgTwFwD+0My+6PYt/BEggCsAfDfvhPJaEUJ0hJF5rdwE4O0kHwbw9mQdJNeT/EzS5h0A3gjg2oCb4Z+QfADAAwBmAPxG3gnbPSMXQohYRuS1YmZ7ALwtsH0LgOuTz38M4I8XOf6tRc+pgVwI0RGUNGv01JFusgpNONSH/3GUCafPO8Zr84zQ6uvAFYGOy8KYk6KgTCbHCDLvFRoaol8FMWH+/ZxyylGpAQraEW6TbuSLU9eGcq0IIUS7MRisIq+VpqGBXAjRDTQjbwAxtTKL7o9ts9w+y0Sl5swcaqs96M/bS8sRpR6+m1KVJWNHfvSol2O8CtBzckUo0rOO6MZMEQjkuzD6NkXPAeRHkFZxrayr8oQZbPZIPX2PmfYM5EIIsSyqCwhqGlFvf0huIPkgye0kM5m8SL6P5LYkHePtJM+p3lQhhFgmo/EjHzm5AznJKQA3A/gJAOcDuJrk+a7ZtwGsN7PXArgVwMerNlQIIZaFDZJmxSxtI0ZauQTAdjPbAQAkb8EgTeO2hQZmdudQ+3sAvKuwJUWLDwPZx6Sp3tL7S9hRRt8uFU6fZ6vXDUf1iJh3jwNkrz/newl1WdRVcERFoDNfg9sQUnfzXAOr0Jlj+qgiu2EdGRJHSZe9Vs4C8PjQ+k4Ar1+i/XUAvhLaQXIjgI0AsHbt2kgThRCiAsxg890dyIP5iIINyXcBWA/gTaH9ST7fTQBw8UUXtfxvuxCiTZgZ+rNz+Q1bSMxAvhPA8PT5pQB2+UYkL8UgF++b8pKgR1FBkYioTH3evcxHLpYpCuEzrMU84ufochm5oleTw1FO1GUmk+FUwI6imRxDX1MocnUpYqJDRxDZGZIeikopMfKFdwOM6cMf42doMTOrvGNibJ/KuDD6SM8IQ8pg6PSM/F4A60ieB+AJAFcBeOdwA5IXAvgUgA1mFsy9K4QQ46azA7mZzZG8AcBXMUia8Vkz20ryRgBbzGwzgN8EcDyALw5S6OJ7ZnZZjXYLIUQhzAz90eQjHzlRz+dmdhuA29y2jwx9XrRahhBCNIUue63UxnLDzHO11gh9m16L9Xqu08yDroQ5urrXlUMUrojjzxlBFZV5vDYfujZ/lzO6qtfdQ/J2FWH9OQWcrZfOyhi6P+Zj8msISY8ho7O7u5rRzCOyH5bRoqu43rzfR2103GtFCCFaT9e9VoQQYiLoa0ZeM7lVbfOzH2Ye0aKKHqdXM5KFeznCqexf9Exkp5djYtzeaiiUnLk/3o4yLns5GQSBiMjWzHrg4TpPy+yVuKcNKSyR57IXlkXS9FybeR9xGjivPyav0ESIPDnG284mFYXuuPuhEEK0H2nkQgjRbgzyWhGTiPWbITeYZSs2iGVRxS01s2ZJI8vFDP0j9b/sJHkKgM8DOBfAYwDeYWZ7A+3mATyQrB6NvUmCL28BcAqAbwH4OTNbsiJGA/4XL4L100sJaP3CC+bn04u3I7T03ZLXPuZ6M+dw+ZJj7KriPlfwPXgy9xwYjDzDSyUn6qWXCuibuQWppQw9ppcoO9wSwt9Sf4yZpZZwH+k2/nqruX6mltowoN/vRy3L5AMAbjezdQBuT9ZDHDSzC5JlOIDyYwA+kRy/F4NEhEvS3IFcCCEqxDDQyGOWZXI5gM8lnz8H4IrYAzl4BHorBnUdoo+XtCKE6AYGWHyI/gzJLUPrm5LsrTGcYWZPAoCZPUny9EXaHZOcYw7ATWb2JQCnAthnZgsa0E4MUokvyRgHciv2qB563PGPg+a+pExB44A+VjT7YagP5+Zoc7PpPmPc/nJcJf1LmlIPoDHPX94OF/2YcS0M9uHT7OW5H2a/28x5/D2LKXgxBvfDoOtgjuQwH6FJ+G69AuHvYLBLv83fjhhJy2cqzJzXRY+GfiHO+JFlP4QVedm528zWL7aT5NcBvCSw60MFDDrbzHaRfBmAO0g+AOD7gXa5X4xm5EKIblChH/lS+aVIPk3yzGQ2fiaAYEZYM9uV/LuD5F0ALgTw5wBOJjmdzMqDacM90siFEJ3AzDB/ZC5qWSabAVyTfL4GwJd9A5JrSK5KPs8AeAOAbTZ4pLsTwJVLHe8Z74x8+JGqYDECICJxlNtPJ3kMNvpnVBeV6Y+ZWplrh82lPYUySYICUYl25FDWtiWIchBwsgh6EbORjLTivoeej3RdkWubv14f6Rn6bn0BXJ/cLFNYI0amy0miFkzi5uuNVoCXPTJRmYEvdz5H0si2z27L5P/y9U8iPEYyskdOgYtmUUhaWQ43AfgCyesAfA/AzwAAyfUA3m1m1wN4JYBPcfDD7mGgkS/UQX4/gFtI/gYGhe1/L++EklaEEN1gRCH6ZrYHwNsC27cAuD75/A8AXrPI8TswKHofjQZyIUQ3MMBCjyoTgAZyIUQnMJiyH9bCsCaX94cypqCD794XPQhqoG7Va+JO7+Z8IFJ2Pn2M91UlnGbutevAMRnKaHsZPdtr5lk7Mhq5d62cTr8jYEB3zxb88G6AEUU0Mq6DOW6QwXR/OZkbY7Iw5uA149mAHX4S6PXurGae/c/gXRT9r8VHRIb6mHKCtvcSXZEpipzpot0YYBN3UQM0IxdCdAIzYP5Ih2t2CiFE6zGTRl49dJF2edJCdn9uAQfvwhYR2ZlxHfQySchVzssx/aWlBWY99gDfh5c9YopkeLws4naHhITM9Xo5pgw5bn6l6nNWELWZtSO/jxgJIw9/iO8jFOmZd5ZsH9k2fabbrOj5CMuckwTaeI/FMr+WbKKs+gbbvgZyIYRoMaoQJIQQ7cYA9Cf0ZWfU8yjJDSQfJLmdZCa3Lsk3kvwWyTmSV4b6EEKIsWKG+SPzUUvbyJ2Rk5wCcDOAt2OQUvFekpuHwkmBQRjqtQB+uawhGf3a68ohfTsny6B3Fcxo2QiEj88Gwvhz+shmP3Qui07vtkAX2X6dS2MZjdzBHM0cCNiak3UxZBdXHrP0MV4T9e6IiNCvXR/hMH+3IZPtLyILo0/z4FISMGKCl6eBz3nXwpC+naMbe909NPHMyyoYU9MhmzBxaZ19qkQmw8zvoyKs4wFBlwDYnoSNguQtGCROPzqQm9ljyb7JFKCEEO2n4wP5WQAeH1rfCeD1ZU5GciOAjQCwdu3aMl0IIURJuh3ZGfRSK3OypMLGJgC4+KKL0n34jHjz3qUvK61w9mB6g5cFDr2YXg/IJualFSeLZCIqA5kLvcSTkUn8XQ7IM3nZD8u4BWaiRUOykD/GXR9dJGdGNnIyCoBsMYoVPsKyhllRRHEKn8kwyh3TuzUGsj0u1WcIb6kfW44EZo1eWslKKflnzquH6aWXYOuel7SWPmdI4vHHePmljp/HoONuR3buBDA8fY5KdC6EEE3C0G0/8nsBrCN5HoAnAFwF4J21WiWEEFVjhn4LPVJiyHU/TMoN3QDgqwD+CcAXzGwryRtJXgYAJF9HcicGCdS4fhwdAAANc0lEQVQ/RXJrnUYLIURRzAYz8pilbUQFBJnZbQBuc9s+MvT5XgwklwK44steI4+o7tN/8YV0mxVp/bJ/MK2RhzIIcjp9jNeqva7eC1X3yWRMdJq5PyDgsmeHl9b7s+cMZGHMwfoR2Q+9ru7v2fTSGnGITEUkV2Up6DpY/CTZbf4+ew3YZccMVY/xOnHP3zOvmQcGAa+Bew141um2swE7fB++TYxGviLz2116Hjcd8FfsxbxYGKLMkBhzLWUZUYWgkaPITiFEN7B2zrZj0EAuhOgGHfcjHw0+gnAuLXHwiJNJAMztfSa13jvh5NR6nkwCAFx1TG6b1P6QC5+XUryrYET0aG7x5RypJdimYCFlIPDo6R/HM9JLILLTu4H6CNxpJ2nEZFjMRIO6exqQVjLxad6l1bcPbfQRpq7oM916KJLRR3Z6KeWgq0axP1DFPSO/lPCHXjWdvkez/fT6CavctQWuZSozDi6trTTJ28+gpFlCCNFuzDB/ZDIH8uJJnIUQooWYDV6kxizLgeQpJL9G8uHk3zWBNm8hed/QcojkFcm+PyD56NC+C/LOqYFcCNEZ5s2ilmXyAQC3m9k6ALcn6ynM7E4zu8DMLgDwVgAHAPztUJNfWdhvZvflnbAx0krG3fBI2h1v/vk9mWP6+/el1jOaeEabzbrOZULdK8gymMmGeNitH8rq/UHtPdWne2cQCI3PVirKKaQcUXw585N2Ifu9Y47NduHWe+6+c1X+463Xov16RruPcCvLZth060EXRve79Nr89Kol9wNZd0Mfgr/3UPp7e/5Q9rdwwOno3v3QuxauCIj1xzr3U69fr3IC+MpAHz7M33soRoX5O0ZVIMgQ9A6tg8sBvDn5/DkAdwF4/xLtrwTwFTM7UPaEmpELITpDgRn5DMktQ8vGAqc5w8yeBIDk39Nz2l8F4M/cto+SvJ/kJ0iuCh00TGNm5EIIUSd9A47Eu9HsNrP1i+0k+XUALwns+lARm0ieCeA1GETOL/BBAE8BWIlBksH3A7hxqX4aU3zZuxfawXTU5vyzT2R6mN2zO7U+ffzxS59y9XGZTZnozxx8cQYAgcf8dJv+gReW3A8EXBJ9Gy8LhdwVMwUsXPSnd5OMkJG8O2LPSTohQaPn5BcvX3mZpL8iK8/YCjcJ8dJKJho0YIkv+jzr5Km5wzl9ZuWqqWNOSO9fmba9N706a4fjRZfvw0spew9mpZUXZ9PH7D+UlnxWOtdCvw5k3QtPcuteFlntvwMAU+5r8PLLKrc+HdBW5nLG0tAxVVGVtGJmly62j+TTJM80syeTgfqZxdoCeAeAvzD7QbmZhdk8gMMkfx8RBXskrQghOoEhTlap4GXnZgDXJJ+vAfDlJdpeDSerJIM/SBLAFQC+m3dCDeRCiE6w8LIzZlkmNwF4O8mHMSiReRMAkFxP8jMLjUiei0GK8P/jjv8Tkg8AeADADIDfyDvhGKWVdNIszqYfc2d3PZZan9v7bKaHF773tNuSXudU+u/UcWeemuljxfFpucVHh/ZOSLuA9o49MdNHRkrx+w+lX0b3X/x+posj+/an1r3tft3bDQB00hFX+KIQ6Uf2uQNZeWb2xbS30JRLRDZ1TPqRfjokE3n5xSU36x3n3GoDxRpsZfpazCfaynigBCJMD6a9mrzU0t+X/k2FPIe8/DR1QlqumlqVtnPq5GzlK++l8pyTTp55Md3nE3tdAjUAe1ybg06emXK6yKnHpe8XAJx2Yloq8Z4uJzlHqJiJaZ7DSaj+puV0XFfNTmA0XitmtgfA2wLbtwC4fmj9MQwqsPl2by16Tr3sFEJ0ArNsuoRJQQO5EKITGAp5rbQKDeRCiE4wwoCgkdOYgdwXF5h96vHU+r6H0usAsP8Jp3G6b8lXzJ4/lC3GcPxZp6XWV5+U1tF7J56StnNVwL3M67OByM1hvB4OAAee2bvkMcee7nTlEtkQ+14jP+jc75C9R4f3pm3tzzq3txOzroPHu/vho1CnfEZB79MG4BDTGu9hl+xopXNpXBmK2vVukPPpa5vd81RqfX7Pk/D47+qYtWkNfNWxJ6XWeydn66t4jfyp/en7fv/O51PrD+3KvkM5dMB9d04jX+FcCdesyUb+zvXTrpNrVi9dJGQq4ArhMyge49Z75r77uWwmx2kXDdvzWSkDbqBVIWlFCCFajFXjkdJINJALITqDZuRVM6iEenT18LZvpnbvvPNb6fV7spGdB/elH1F9Qp/jzkg/9q84Lhuptvp0527opBWeOJNa769OP0oDWVe4nouonHOulPseyV7Lvu1p18np1S7Jv3vO9Y/4ADB9WtqTyUet9l9IyzeHAxKPd+k88sLSBS9Wn5qNpvWFNE489czUujl3w7nVafkKAJ56IX0P9x9OP26vXuFc51ZlpaYZ78J4OO0GeuixR1LrT34jG3fxwpPpe7TmvO2p9bXuWnnayzN97D6Q/n1sddLJtn9Jfy/7ns3mTvr+c+lt8y6pnI+mfWEmLaMAWZfFU49PH7P2pLRsGEqadfxKJ6VkomVzCqQA6PnBtEYpJXUahCORJwHNyIUQncBg8loRQog2M/Ba0UAuhBDtpesvO0luAPBJAFMAPmNmN7n9qwD8IYCLAewB8LNJ+Omi2L5ncORLv3N0/ds3/21q/ze3pjMbPn0468a0fy6teB3vXKHW7s+6G3pWrUlriSe+Lr1/7tRz03bMZUOfV7jw6Jkz0rf1xTu+klrf/tfbMn187+m0nn2acydbeULanWzmbYHka+e8Jm3X/rR75vzT30utH3gqW6xjz4Pp++414r7TWVfPZN0Pz3gxrZMe61IjTL/q36TWdx/MfrfffTp93h3PLZ1z/5w1WTte90Pp73ate3ex/Uvp9zJ3//3OTB97XdbBc+9Nuyj+25Xp7+nsizdk+nhsX7rN3VvT7yEe/+f0O5MXdqV1eAA47N5v9OfS1+I18v0zP5Tt4+ArU+s7ZtLvEF5/dtrFNfTeYfqFdCK/TFF071q6MptKwmey7Ls2nM2mKKiCSZ6R5ybNIjkF4GYAPwHgfABXkzzfNbsOwF4zezmATwD4WNWGCiHEchlR0qyRE5P98BIA281sh5kdAXALBqWMhrkcg5JGAHArgLclKRiFEKIR9DEI0Y9Z2gbzMpGRvBLABjO7Pln/OQCvN7Mbhtp8N2mzM1l/JGmz2/W1EcBCyaRXIyLP7piYAbA7t9X4aLJ9sq08TbZv3LadY2an5TdbHJJ/g8F1xLDbzLI6WUOJ0chDM2s/+se0gZltwqB0EUhuWaqU0jhpsm1As+2TbeVpsn1Nti2WNg3MRYmRVnZikPx8gZcC2LVYG5LTAE4C8FwVBgohhFiamIH8XgDrSJ5HciUGFZ83uzbDpY2uBHCH5Wk2QgghKiFXWjGzOZI3YFDleQrAZ81sK8kbAWwxs80Afg/AH5HcjsFM/KqIc29aht1102TbgGbbJ9vK02T7mmxb58l92SmEEKLZqPiyEEK0HA3kQgjRcmofyEluIPkgye0kPxDYv4rk55P93yR5bt02FbDtjSS/RXIu8acfGRG2vY/kNpL3k7yd5DkNs+/dJB8geR/Jvw9EA4/NtqF2V5I0kiN1q4u4d9eSfDa5d/eRvD7UzzhsS9q8I/ntbSX5p6OyTSyBmdW2YPBy9BEALwOwEsB3AJzv2vwigP+dfL4KwOfrtKmgbecCeC0GeWSuHIVdBWx7C4Bjk8+/MKr7VsC+E4c+Xwbgb5piW9LuBAB/B+AeAOsbdu+uBfC7o7KpoG3rAHwbwJpk/fRR26klu9Q9I29yeH+ubWb2mJndj9Hno4+x7U4zW8gkdQ8G/v1Nsm+4esJxCASIjcu2hF8H8HEA+ZUQqiXWvnEQY9vPA7jZzPYCgJk9AzF26h7IzwIwXDV5Z7It2MbM5gA8D+BU1E+MbeOiqG3XAfjKEvurJso+ku9J0jV8HMB7m2IbyQsBrDWzvxqRTcPEfrc/nchmt5LMloOqhxjbXgHgFSTvJnlPkhlVjJm6B/LKwvtrYFznjSHaNpLvArAewG/WapE7bWBbKCXDzWb2wwDeD+DDtVs1YEnbSPYwyND530Zkjyfm3v0lgHPN7LUAvo4fPLHWTYxt0xjIK28GcDWAz5A82R8kRkvdA3mTw/tjbBsXUbaRvBTAhwBcZmaH/f4aKXrvbgFwRa0W/YA8207AIGHbXSQfA/CjADaP8IVn7r0zsz1D3+enMcjz3wjbkjZfNrNZM3sUwIMYDOxinNQpwGPw13sHgPPwg5cnr3Jt3oP0y84vjOLlQIxtQ23/AKN92Rlz3y7E4MXUulHZVdC+dUOffwqDKOBG2Oba34XRvuyMuXdnDn3+DwDuaZBtGwB8Lvk8g4EUc+qof4Na3Hc3gh/HvwPwUDLofCjZdiMGs0gAOAbAFwFsB/CPAF42sovPt+11GMxAXsSg8tHWBtn2dQBPA7gvWTaP9IeTb98nAWxNbLtzqcF01La5tiMdyCPv3f9I7t13knv3rxpkGwH8FoBtAB4AcNUo752W8KIQfSGEaDmK7BRCiJajgVwIIVqOBnIhhGg5GsiFEKLlaCAXQoiWo4FcCCFajgZyIYRoOf8ffmjjBq3OiEYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = np.meshgrid(np.linspace(0, soln.shape[-2]*inp[-1][0,0], soln.shape[-2]), np.linspace(0, soln.shape[-1]*inp[-1][0,0], soln.shape[-1]), indexing = 'ij')\n",
    "#p_r = np.random.randint(0,soln.shape[0])\n",
    "#z = soln[p_r,0,...]\n",
    "from Poisson_CNN import channels_first_rot90, channels_first_flip_left_right, channels_first_flip_up_down\n",
    "z = channels_first_flip_up_down(channels_first_rot90(dbnn(inp), k = 3))[p_r,0,...]\n",
    "#z = mod([tf.expand_dims(b['left'], axis = 1), dx * tf.ones((soln.shape[0],1), dtype = 'float64')])[p_r,0,...]\n",
    "#z = mod(tf.expand_dims(b['left'], axis = 1))[p_r,0,...]\n",
    "#z = tf.divide(mod([tf.expand_dims(b['left'], axis = 1), dx * tf.ones((soln.shape[0],1), dtype = 'float64')])[p_r,0,...]-soln[p_r,0,...], soln[p_r,0,...])\n",
    "z_min, z_max = -np.abs(z).max(), np.abs(z).max()\n",
    "fig, ax = plt.subplots()\n",
    "c = ax.pcolormesh(x, y, z, cmap='RdBu', vmin=z_min, vmax=z_max)\n",
    "ax.set_title('result')\n",
    "ax.axis([x.min(), x.max(), y.min(), y.max()])\n",
    "fig.colorbar(c, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.save_weights('asd.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
