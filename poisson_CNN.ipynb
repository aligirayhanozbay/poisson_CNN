{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Flatten, Conv2DTranspose, AveragePooling2D, Add\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from Boundary import Boundary1D\n",
    "from collections.abc import Iterable\n",
    "import itertools, h5py\n",
    "from multiprocessing import Pool as ThreadPool\n",
    "opts = tf.GPUOptions(per_process_gpu_memory_fraction=0.95)\n",
    "conf = tf.ConfigProto(gpu_options=opts)\n",
    "tfe.enable_eager_execution(config=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(tf.keras.layers.Layer):\n",
    "    def __init__(self, upsample_ratio = (2,2), output_dtype = tf.float32, data_format = 'channels_first', resize_method = tf.image.ResizeMethod.BICUBIC, **kwargs):\n",
    "        super(Upsample, self).__init__(**kwargs)\n",
    "        self.output_dtype = output_dtype\n",
    "        self.data_format = data_format\n",
    "        self.resize_method = resize_method\n",
    "        if not isinstance(upsample_ratio, Iterable):\n",
    "            self.upsample_ratio = (upsample_ratio, upsample_ratio)\n",
    "        else:\n",
    "            self.upsample_ratio = upsample_ratio\n",
    "            \n",
    "    def build(self, input_shape):\n",
    "        super(Upsample, self).build(input_shape)\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.data_format == 'channels_first':\n",
    "            return tf.TensorShape(list(input_shape)[0:2] + [self.upsample_ratio[0] * input_shape[-2], self.upsample_ratio[1] * input_shape[-1]])\n",
    "        else:\n",
    "            return tf.TensorShape([input_shape[0], self.upsample_ratio[0] * input_shape[-3], self.upsample_ratio[1] * input_shape[-2], input_shape[-1]])\n",
    "    \n",
    "#     def upsample(self, inp):\n",
    "#         dummy_coords_inp = (np.linspace(0,1,int(inp.shape[-2])), np.linspace(0,1,int(inp.shape[-1])))\n",
    "#         dummy_coords_out = (np.linspace(0,1,int(self.compute_output_shape(inp.shape)[-2])),np.linspace(0,1,int(self.compute_output_shape(inp.shape)[-1])))\n",
    "#         #pdb.set_trace()\n",
    "#         res = []\n",
    "#         for i in range(inp.shape[0]):\n",
    "#             spl = RectBivariateSpline(dummy_coords_inp[0], dummy_coords_inp[1], np.array(inp[i,...]))\n",
    "#             res.append(spl(dummy_coords_out[0], dummy_coords_out[1]))\n",
    "#         return tf.dtypes.cast(np.array(res), dtype=self.output_dtype)\n",
    "    \n",
    "#     @tf.contrib.eager.defun\n",
    "#     def upsample_wrap(self,inp):\n",
    "#         return tf.map_fn(self.upsample, inp)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "#         return self.upsample_wrap(inputs)\n",
    "        if self.data_format == 'channels_first':\n",
    "            return tf.transpose(tf.image.resize_images(tf.transpose(inputs, (0,2,3,1)), tf.multiply(self.upsample_ratio, inputs.shape[2:]), align_corners=True, method=self.resize_method), (0,3,1,2))\n",
    "        else:\n",
    "            return tf.image.resize_images(inputs, tf.multiply(self.upsample_ratio, inputs.shape[1:-1]), align_corners=True, method=self.resize_method)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_matrix(m,n):\n",
    "    '''\n",
    "    Generates the matrix A to express the Poisson equation in the form Ax=b for an m-by-n grid\n",
    "    \n",
    "    Them matrix returned shall be (m-2)*(n-2)-by-(m-2)*(n-2) in size\n",
    "    \n",
    "    CURRENTLY ONLY WORKS FOR SQUARE DOMAINS!!!!\n",
    "    '''\n",
    "    m = m-2\n",
    "    n = n-2\n",
    "    \n",
    "    D = np.zeros((m,m), dtype = np.float64)\n",
    "    i,j = np.indices(D.shape)\n",
    "    D[i==j] = 4.0\n",
    "    D[i==j-1] = -1.0\n",
    "    D[i==j+1] = -1.0\n",
    "    \n",
    "    S = -np.eye(D.shape[0], dtype = np.float64)\n",
    "    \n",
    "    P = np.zeros((m*n,m*n), dtype = np.float64)\n",
    "    ind = np.arange(0,m*(n+1), m)\n",
    "    \n",
    "    for i in range(len(ind)-1):\n",
    "        P[ind[i]:ind[i+1], ind[i]:ind[i+1]] = D\n",
    "        try:\n",
    "            P[ind[i+1]:ind[i+2], ind[i]:ind[i+1]] = S\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            P[ind[i-1]:ind[i], ind[i]:ind[i+1]] = S\n",
    "        except:\n",
    "            pass\n",
    "    return P\n",
    "\n",
    "def generate_random_RHS(n, n_controlpts = None, n_outputpts = None, s = 5, domain = [0,1,0,1]):\n",
    "    \n",
    "    '''\n",
    "    This function generates random smooth RHS 'functions' defined pointwise using bivariate splines. \n",
    "    n: no. of random RHSes to generate\n",
    "    n_controlpts: no. of control pts of the spline. Smaller values lead to 'smoother' results\n",
    "    n_outputpts: no. of gridpoints in each direction of the output \n",
    "    s: see parameter s in scipy.interpolate.RectBivariateSpline\n",
    "    domain: [x_min, x_max, y_min, y_max]\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    if isinstance(n, Iterable):\n",
    "        n_controlpts = n[1]\n",
    "        n_outputpts = n[2]\n",
    "        try:\n",
    "            s = n[3]\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            domain = n[4]\n",
    "        except:\n",
    "            pass\n",
    "        n = n[0]\n",
    "    \n",
    "    x = np.linspace(domain[0], domain[1], n_controlpts)\n",
    "    y = np.linspace(domain[2], domain[3], n_controlpts)\n",
    "    if n_controlpts != n_outputpts:\n",
    "        x_out = np.linspace(domain[0], domain[1], n_outputpts)\n",
    "        y_out = np.linspace(domain[2], domain[3], n_outputpts)\n",
    "    else:\n",
    "        x_out = x\n",
    "        y_out = y\n",
    "            \n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        spl = RectBivariateSpline(x,y,2*np.random.rand(len(x), len(y))-1, s=s)\n",
    "        out.append(spl(x_out,y_out))\n",
    "    return np.array(out)\n",
    "    \n",
    "\n",
    "def poisson_RHS(F, boundaries = None, h = None):\n",
    "    '''\n",
    "    Generates the RHS vector b of a discretized Poisson problem in the form Ax=b.\n",
    "    h = grid spacing\n",
    "    boundaries = dict containing entries 'top', 'bottom', 'right' and 'left' which correspond to the Dirichlet BCs at these boundaries. Each entry must be a vector of length m or n, where m and n are defined as in te function poisson_matrix\n",
    "    F = an m by n matrix containing the RHS values of the Poisson equation\n",
    "    \n",
    "    (i.e. this function merely takes the BC information and the array from generate_random_RHS to provide the RHS for the matrix eq. form)\n",
    "    '''\n",
    "    \n",
    "    if isinstance(F, Iterable):\n",
    "        boundaries = F[1]\n",
    "        h = F[2]\n",
    "        F = F[0]\n",
    "    \n",
    "    F = -h**2 * F\n",
    "    F[...,1:-1,1] = F[...,1:-1,1] + np.array(boundaries['top'])[1:-1]\n",
    "    F[...,1:-1,-2] = F[...,1:-1,-2] + np.array(boundaries['bottom'])[1:-1]\n",
    "    F[...,1,1:-1] = F[...,1,1:-1] + np.array(boundaries['left'])[1:-1]\n",
    "    F[...,-2,1:-1] = F[...,-2,1:-1] + np.array(boundaries['right'])[1:-1]\n",
    "    \n",
    "    return F[...,1:-1,1:-1].reshape(list(F[...,1:-1,1:-1].shape[:-2]) + [np.prod(F[...,1:-1,1:-1].shape[-2:])])\n",
    " \n",
    "def generate_dataset(batch_size, n, h, boundaries, n_batches = 1, rhs_range = [-1,1]):\n",
    "    lhs = tf.constant(poisson_matrix(n,n), dtype=tf.float64)\n",
    "    lhs_chol = tf.linalg.cholesky(lhs)\n",
    "    \n",
    "    def chol(r):\n",
    "        return tf.linalg.cholesky_solve(lhs_chol, tf.transpose(tf.stack([r])))\n",
    "    \n",
    "    @tf.contrib.eager.defun\n",
    "    def chol_solve(rhs_arr):\n",
    "        return tf.map_fn(chol, rhs)\n",
    "    \n",
    "    #pdb.set_trace()\n",
    "    pool = ThreadPool(n_batches)\n",
    "    F = pool.map(generate_random_RHS, zip(itertools.repeat(batch_size, n_batches), itertools.repeat(10), itertools.repeat(n), itertools.repeat(5), itertools.repeat([0,n*h,0,n*h])))\n",
    "    rhs = tf.concat(pool.map(poisson_RHS, zip(F, itertools.repeat(boundaries), itertools.repeat(h))), axis=0)\n",
    "    \n",
    "    soln = np.zeros((n_batches * batch_size, n, n), dtype = np.float64)\n",
    "    soln[...,:,0] = boundaries['top']\n",
    "    soln[...,:,-1] = boundaries['bottom']\n",
    "    soln[...,0,:] = boundaries['left']\n",
    "    soln[...,-1,:] = boundaries['right']\n",
    "    soln[:,1:-1,1:-1] = tf.reshape(chol_solve(rhs), (n_batches * batch_size, n-2, n-2))\n",
    "    #soln = chol_solve(rhs)\n",
    "    \n",
    "    #return tf.reshape(soln, (n_batches, batch_size, lhs_chol.shape[0])), tf.reshape(F, (n_batches, batch_size, F.shape[-2], F.shape[-1]))\n",
    "    return tf.reshape(soln, (n_batches*batch_size, 1, soln.shape[-2], soln.shape[-1])), tf.reshape(tf.concat(F, axis = 0), (n_batches*batch_size, 1, F[0].shape[-2], F[0].shape[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lp_integral_norm(image_size, domain, n_quadpts = 10, quadpts_randomization = 0, p=2, dtype = tf.float32):\n",
    "    '''\n",
    "    This function generates a function that takes 2 function(s) evaluated on a 2D grid, the dimensions of which are stored in image_size, on a rectangular domain and evaluates the Lp norm of their difference using Gauss-Legendre quadrature\n",
    "    \n",
    "    image_size              :     tuple with 2 elements.\n",
    "    domain                  :     tuple with 4 elements containing (xmin,xmax,ymin,ymax)\n",
    "    n_quadpts               :     no of GL quadrature points\n",
    "    quadpts_randomization   :     if set to a nonzero value, the dataset inputted to the generated func. will be split into 2*quadpts_randomization+1 pieces and the integral for the ith piece will be computed with n_quadpts + i * quadpts_randomization GL pts (where i = [n_quadpts-quadpts_randomization, n_quadpts+quadpts_randomization])\n",
    "    p                       :     order of the Lp norm\n",
    "    \n",
    "    The shape of the inputs to the generated function must be (batch_size,channels,image_size[0],image_size[1])\n",
    "    \n",
    "    User inputs to the closure (generated function):\n",
    "    y_true                  :     labels\n",
    "    y_pred                  :     images\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #arrays to store values for different # of GL quad. pts. in accordance with quadpts_randomization. length of each: 2 * quadpts_randomization + 1\n",
    "    interpolation_weights = [] #array that stores weights b_ii such that the interpolated value at x,y is b_ii*f(X_i, Y_j) within a rectangle bounded by [X_1,X_2] x [Y_1, Y_2] (shapes: (n_quadpts, n_quadpts, 4, 2))\n",
    "    quadweights_list = [] #array that stores GL quad weights (shapes: (n_quadpts, n_quadpts))\n",
    "    index_combinations_list = [] #array of tensors, where each tensor stores the indices for the 4 points between which every GL quad pt lies (shapes: (no of quadpts, no of quadpts, 4, 2))\n",
    "    coords = np.array(np.meshgrid(np.linspace(domain[0], domain[1], image_size[0]),np.linspace(domain[2], domain[3], image_size[1]),indexing = 'xy'), dtype = np.float32).transpose((1,2,0)) #coordinates of each grid pt in the domain\n",
    "    image_coords = [coords[0,:,0], coords[:,1,1]] #x and y coordinates separately\n",
    "    c = np.array([np.array(0.5*(domain[1] - domain[0]),dtype=np.float64),np.array(0.5*(domain[3] - domain[2]),dtype=np.float32)]) #scaling coefficients - for handling domains other than [-1,1] x [-1,1]\n",
    "    d = np.array([np.array(0.5*(domain[1] + domain[0]),dtype=np.float64),np.array(0.5*(domain[3] + domain[2]),dtype=np.float32)])\n",
    "    for n in range(n_quadpts - quadpts_randomization, n_quadpts + quadpts_randomization+1): #loop over no of quadpts\n",
    "        quadrature_x, quadrature_w = tuple([np.polynomial.legendre.leggauss(n)[i].astype(np.float64) for i in range(2)]) #quadrature weights and points\n",
    "        quadpts = tf.constant(np.apply_along_axis(lambda x: x + d, 0, np.einsum('ijk,i->ijk',np.array(np.meshgrid(quadrature_x,quadrature_x,indexing = 'xy')),c)).transpose((1,2,0)),dtype = tf.float32)\n",
    "        quadweights = tf.reduce_prod(c)*tf.tensordot(quadrature_w,quadrature_w,axes = 0)\n",
    "        indices = [[],[]] #indices between each quadrature point lies - indices[0] is in x-dir and indices[1] is in the y-dir\n",
    "        quad_coords = [quadpts[0,:,0], quadpts[:,1,1]] #x and y coordinates of each quad pt respectively\n",
    "        #find the indices of coords between which every quad. pt. lies\n",
    "        for i in range(len(indices)):\n",
    "            j=0\n",
    "            #does not work if more than 2 quad pts fall within 1 cell - fix later\n",
    "            while len(indices[i]) < quadpts.shape[0] and j<image_coords[i].shape[0]:\n",
    "                try:\n",
    "                    if abs(float(quad_coords[i][len(indices[i])] - image_coords[i][j])) == float(min(abs(quad_coords[i][len(indices[i])] - image_coords[i][j-1]), abs(quad_coords[i][len(indices[i])] - image_coords[i][j]), abs(quad_coords[i][len(indices[i])] - image_coords[i][j+1]))):\n",
    "                        if quad_coords[i][len(indices[i])] - image_coords[i][j] < 0:\n",
    "                            indices[i].append((j-1,j))\n",
    "                        else:\n",
    "                            indices[i].append((j,j+1))\n",
    "                except:\n",
    "                    if abs(float(quad_coords[i][len(indices[i])] - image_coords[i][j])) == float(min(abs(quad_coords[i][len(indices[i])] - image_coords[i][j-1]), abs(quad_coords[i][len(indices[i])] - image_coords[i][j]))):\n",
    "                        indices[i].append((j-1,j))\n",
    "                j+=1\n",
    "        \n",
    "        index_combinations = np.zeros((quadpts.shape[0], quadpts.shape[1], 4 , 2), dtype = np.int32) #array storing the 4 index combinations on the original grid which surround each quad. pt.\n",
    "        corners = np.zeros((quadpts.shape[0], quadpts.shape[1], 2 , 2), dtype = np.int32) #array storing the lower left corner and the upper right corner of each box stored in index_combinations. effectively this will contain [[xmin,ymin],[xmax,ymax]] for the rectangle around each quad pt.\n",
    "        s=np.array(indices)\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                index_combinations[i,j,:,:] = np.array(list(itertools.product(np.array(s)[0,i,:],np.array(s)[1,j,:])))\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                corners[i,j,:,:] = np.array([s[0,i,:],s[1,j,:]])\n",
    "        corners = corners.transpose((0,1,3,2))\n",
    "        corner_coords = tf.gather_nd(tf.transpose(coords,(1,0,2)),corners)\n",
    "        \n",
    "        #compute the coefficients [b_11,b_12,b_21,b_22]\n",
    "        #steps:\n",
    "        #1. compute transpose(invert(array([[1,xmin,ymin,xmin*ymin],[1,xmin,ymax,xmin*ymax],[1,xmax,ymin,xmax*ymin],[1,xmax,ymax,xmax*ymax]]))) for the rectangle around each quad pt.\n",
    "        #2. compute array([1,x_quadpt, y_quadpt, x_quadpt*y_quadpt]) for each quadpt\n",
    "        #3. multiply the result of 1 and 2 for each quad pt.\n",
    "        interpolation_matrix = np.ones((n,n,4,4))\n",
    "        interpolation_matrix[:,:,0:2,1] = np.einsum('ijk,ij->ijk',interpolation_matrix[:,:,0:2,1],corner_coords[:,:,0,0])\n",
    "        interpolation_matrix[:,:,2:,1] = np.einsum('ijk,ij->ijk',interpolation_matrix[:,:,2:,1],corner_coords[:,:,1,0])\n",
    "        interpolation_matrix[:,:,(0,2),2] = np.einsum('ijk,ij->ijk',interpolation_matrix[:,:,(0,2),2],corner_coords[:,:,0,1])\n",
    "        interpolation_matrix[:,:,(1,3),2] = np.einsum('ijk,ij->ijk',interpolation_matrix[:,:,(1,3),2], corner_coords[:,:,1,1])\n",
    "        interpolation_matrix[:,:,:,3] *= np.multiply(interpolation_matrix[:,:,:,1], interpolation_matrix[:,:,:,2])\n",
    "        interpolation_matrix = tf.transpose(tf.linalg.inv(interpolation_matrix), (0,1,3,2))\n",
    "        q = np.ones((n,n,4))\n",
    "        q[:,:,1] = tf.transpose(quadpts[:,:,0])\n",
    "        q[:,:,2] = tf.transpose(quadpts[:,:,1])\n",
    "        q[:,:,3] = np.multiply(q[:,:,1],q[:,:,2])\n",
    "        \n",
    "        b = tf.einsum('ijkl, ijl->ijk', tf.constant(interpolation_matrix), tf.constant(q))\n",
    "        \n",
    "        #store results for the closure\n",
    "        quadweights_list.append(tf.cast(quadweights,dtype))\n",
    "        index_combinations_list.append(index_combinations)\n",
    "        interpolation_weights.append(tf.cast(b, dtype))\n",
    "    \n",
    "    @tfe.defun\n",
    "    def Lp_integrate_batch(inp):\n",
    "        '''\n",
    "        Helper function to facilitate quad pt randomization\n",
    "        \n",
    "        Given the bilinear interp. weights b, GL quad. weights w, Lp norm order p and the indices bounding each quad pt ind, computes the Lp norm for each channel/batch element\n",
    "        '''\n",
    "        #unpack values\n",
    "        data = tf.transpose(inp[0], (2,3,1,0))\n",
    "        b = inp[1]\n",
    "        w = inp[2]\n",
    "        ind = inp[3]\n",
    "        p = inp[4]\n",
    "        #print(w)\n",
    "        #get the points from the image to perform interpolation on\n",
    "        interp_pts = tf.squeeze(tf.gather_nd(data, ind))\n",
    "        #pdb.set_trace()\n",
    "        #multiply image values with the weights b ti interpolate original image onto the GL quadrature points\n",
    "        values_at_quad_pts = tf.einsum('ijkl, ijk->ijl', interp_pts, b)\n",
    "        #pdb.set_trace()\n",
    "        #compute Lp norm\n",
    "        return tf.pow(tf.reduce_sum(tf.einsum('ij,ijk->ijk',w,tf.pow(values_at_quad_pts, p)), axis = (0,1)), 1/p)\n",
    "    \n",
    "    @tfe.defun\n",
    "    def Lp_integrate(y_true,y_pred, b=interpolation_weights, w=quadweights_list, ind=index_combinations_list, p=p):\n",
    "        '''\n",
    "        Split the batch, pack with the appropriate parameters and obtain the integrals\n",
    "        '''\n",
    "        #pool = ThreadPool(len(b))\n",
    "        return tf.concat(list(map(Lp_integrate_batch, zip(tf.split(y_true-y_pred, len(b)), itertools.cycle(b), itertools.cycle(w), itertools.cycle(ind), itertools.repeat(p)))), 0)\n",
    "        \n",
    "    return Lp_integrate\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntest = 64\n",
    "h = 0.05\n",
    "boundary_top = Boundary1D('Dirichlet', [(0,ntest*h),(ntest*h,ntest*h)], orientation='clockwise', RHS_function=lambda t: t-t, boundary_rhs_is_parametric=True)\n",
    "boundary_right = Boundary1D('Dirichlet', [(ntest*h,ntest*h),(ntest*h,0)], orientation='clockwise', RHS_function=lambda t: t-t, boundary_rhs_is_parametric=True)\n",
    "boundary_bottom = Boundary1D('Dirichlet', [(ntest*h,0),(0,0)], orientation='clockwise', RHS_function=lambda t: t-t, boundary_rhs_is_parametric=True)\n",
    "boundary_left = Boundary1D('Dirichlet', [(0,0),(0,ntest*h)], orientation='clockwise', RHS_function=lambda t: t-t, boundary_rhs_is_parametric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "soln,F = generate_dataset(batch_size=2000, n = ntest, h = h, n_batches=20, boundaries={'top': boundary_top.RHS_evaluate(np.linspace(boundary_top.t.min(),boundary_top.t.max(),ntest)), 'right': boundary_right.RHS_evaluate(np.linspace(boundary_right.t.min(),boundary_right.t.max(),ntest)), 'bottom': boundary_bottom.RHS_evaluate(np.linspace(boundary_bottom.t.min(),boundary_bottom.t.max(),ntest)), 'left': boundary_left.RHS_evaluate(np.linspace(boundary_left.t.min(),boundary_left.t.max(),ntest))})\n",
    "t1 = time.time()\n",
    "print('Generation of training data took ' + str(t1-t0) + ' seconds')\n",
    "with h5py.File('dataset8.h5', 'w') as hf:\n",
    "    hf.create_dataset('soln', data=soln)\n",
    "    hf.create_dataset('F', data=F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,5):\n",
    "    with h5py.File('dataset' + str(i) + '.h5', 'r') as hf:\n",
    "        F = np.array(hf.get('F'), dtype = np.float64)\n",
    "        soln = np.array(hf.get('soln'), dtype = np.float64)\n",
    "        try:\n",
    "            train_data = train_data.concatenate(tf.data.Dataset.from_tensor_slices((F,soln)))\n",
    "        except:\n",
    "            train_data = tf.data.Dataset.from_tensor_slices((F,soln))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(6,7):\n",
    "    with h5py.File('dataset' + str(i) + '.h5', 'r') as hf:\n",
    "        F = np.array(hf.get('F'), dtype = np.float64)\n",
    "        soln = np.array(hf.get('soln'), dtype = np.float64)\n",
    "        try:\n",
    "            valid_data = valid_data.concatenate(tf.data.Dataset.from_tensor_slices((F,soln)))\n",
    "        except:\n",
    "            valid_data = tf.data.Dataset.from_tensor_slices((F,soln))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with h5py.File('dataset' + str(4) + '.h5', 'r') as hf:\n",
    "#         F_valid = np.array(hf.get('F'), dtype = np.float64)\n",
    "#         soln_valid = np.array(hf.get('soln'), dtype = np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_size = 100000\n",
    "batch_size = 3000\n",
    "train_data = train_data.shuffle(shuffle_size).batch(batch_size)\n",
    "valid_data = valid_data.shuffle(shuffle_size).batch(batch_size)\n",
    "del F, soln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_0 = Input(shape=(1,ntest,ntest,))\n",
    "conv_1_0 = Conv2D(filters = 5, kernel_size = 3, activation=tf.nn.relu, data_format='channels_first', padding='same')(input_0)\n",
    "\n",
    "conv_2_0 = Conv2D(filters = 8, kernel_size = 3, activation=tf.nn.relu, data_format='channels_first', padding='same')(conv_1_0)\n",
    "pool_2_0 = AveragePooling2D(data_format = 'channels_first')(conv_2_0)\n",
    "pool_2_1 = AveragePooling2D(data_format = 'channels_first')(pool_2_0)\n",
    "\n",
    "conv_3_0 = Conv2D(filters = 8, kernel_size = 3, activation=tf.nn.relu, data_format = 'channels_first', padding='same')(conv_2_0)\n",
    "conv_3_1 = Conv2D(filters = 8, kernel_size = 3, activation=tf.nn.relu, data_format = 'channels_first', padding='same')(pool_2_0)\n",
    "conv_3_2 = Conv2D(filters = 8, kernel_size = 3, activation=tf.nn.relu, data_format = 'channels_first', padding='same')(pool_2_1)\n",
    "#transposeconv_3_1 = Conv2DTranspose(filters = 8, kernel_size = 3, strides = 2, activation = tf.nn.relu, data_format = 'channels_first', padding='same')(conv_3_1)\n",
    "#transposeconv_3_2 = Conv2DTranspose(filters = 8, kernel_size = 3, strides = 4, activation = tf.nn.relu, data_format = 'channels_first', padding='same')(conv_3_2)\n",
    "#upsample_3_1 = UpSampling2D(size=(2,2), data_format = 'channels_first', interpolation = 'bilinear')(conv_3_1)\n",
    "#upsample_3_2= UpSampling2D(size=(4,4), data_format = 'channels_first', interpolation = 'bilinear')(conv_3_2)\n",
    "upsample_3_1 = Upsample()(conv_3_1)\n",
    "upsample_3_2 = Upsample(4)(conv_3_2)\n",
    "#merge_3_0 = Add()([conv_3_0, transposeconv_3_1, transposeconv_3_2])\n",
    "merge_3_0 = Add()([conv_3_0, upsample_3_1, upsample_3_2])\n",
    "\n",
    "conv_4_0 = Conv2D(filters = 8, kernel_size = 1, activation = tf.nn.relu, data_format = 'channels_first', padding='same')(merge_3_0)\n",
    "\n",
    "conv_5_0 = Conv2D(filters = 1, kernel_size = 1, activation = 'linear', data_format = 'channels_first', padding='same')(conv_4_0)\n",
    "final_activation = tf.keras.layers.PReLU()(conv_5_0)\n",
    "a = Model(input_0, final_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 1, 64, 64)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 5, 64, 64)    50          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 8, 64, 64)    368         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 8, 32, 32)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 8, 16, 16)    0           average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 8, 32, 32)    584         average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 8, 16, 16)    584         average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 8, 64, 64)    584         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "upsample (Upsample)             (None, 8, 64, 64)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "upsample_1 (Upsample)           (None, 8, 64, 64)    0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 8, 64, 64)    0           conv2d_2[0][0]                   \n",
      "                                                                 upsample[0][0]                   \n",
      "                                                                 upsample_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 8, 64, 64)    72          add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 1, 64, 64)    9           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu (PReLU)                 (None, 1, 64, 64)    4096        conv2d_6[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 6,347\n",
      "Trainable params: 6,347\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "a.compile(optimizer = tf.train.AdamOptimizer(learning_rate=1e-2), loss = Lp_integral_norm((ntest,ntest), [0,ntest*h,0,ntest*h], n_quadpts=8, quadpts_randomization=2, p=2), metrics = ['accuracy'])\n",
    "a.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[1,1,100,2000,1,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Cast] name: Cast/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-343-444daa6af141>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/test1/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1612\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1614\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1615\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribution_strategy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m       return training_distributed.fit_loop(\n",
      "\u001b[0;32m~/anaconda3/envs/test1/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, class_weight, val_inputs, val_targets, val_sample_weights, batch_size, epochs, verbose, callbacks, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    703\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m           \u001b[0mdo_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_validation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m           batch_size=batch_size)\n\u001b[0m\u001b[1;32m    706\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test1/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36miterator_fit_loop\u001b[0;34m(model, inputs, class_weight, steps_per_epoch, epoch_logs, val_inputs, val_targets, val_sample_weights, epochs, verbose, callbacks, validation_steps, do_validation, batch_size)\u001b[0m\n\u001b[1;32m    215\u001b[0m     x, y, sample_weights = model._standardize_user_data(\n\u001b[1;32m    216\u001b[0m         x, y, sample_weight=sample_weights, class_weight=class_weight)\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast_if_floating_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast_if_floating_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test1/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcast_if_floating_dtype\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    920\u001b[0m         'Please provide tensors for casting, got: {x}'.format(x=x))\n\u001b[1;32m    921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_single_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test1/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **check_types_dict)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 381\u001b[0;31m       structure[0], [func(*x) for x in entries])\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test1/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 381\u001b[0;31m       structure[0], [func(*x) for x in entries])\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test1/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcast_single_tensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcast_single_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test1/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, dtype, name)\u001b[0m\n\u001b[1;32m    675\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Casting complex to real discards imaginary part.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test1/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mcast\u001b[0;34m(x, DstT, Truncate, name)\u001b[0m\n\u001b[1;32m   1681\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test1/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1,1,100,2000,1,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Cast] name: Cast/"
     ]
    }
   ],
   "source": [
    "a.fit(train_data, steps_per_epoch=5, epochs=10, validation_data=valid_data, validation_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.save_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.random.randint(0,F_valid.shape[0])\n",
    "y, x = np.meshgrid(np.linspace(0, ntest*h, ntest), np.linspace(0, ntest*h, ntest))\n",
    "z = a.predict(tf.expand_dims(F[p,...], axis=0))[0,0,...]\n",
    "#z = soln[p,0,...]\n",
    "#z = generate_random_RHS(10, n_controlpts=10, n_outputpts=64)[4,:,:]\n",
    "#z = a.predict(tf.expand_dims(F_valid[p,...], axis=0))[0,0,...] - soln_valid[p,0,...]\n",
    "z_min, z_max = -np.abs(z).max(), np.abs(z).max()\n",
    "fig, ax = plt.subplots()\n",
    "c = ax.pcolormesh(x, y, z, cmap='RdBu', vmin=z_min, vmax=z_max)\n",
    "ax.set_title('pcolormesh')\n",
    "ax.axis([x.min(), x.max(), y.min(), y.max()])\n",
    "fig.colorbar(c, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soln_valid,F_valid = generate_dataset(batch_size=50, n = ntest, h = h, n_batches=1, boundaries={'top': boundary_top.RHS_evaluate(np.linspace(boundary_top.t.min(),boundary_top.t.max(),ntest)), 'right': boundary_right.RHS_evaluate(np.linspace(boundary_right.t.min(),boundary_right.t.max(),ntest)), 'bottom': boundary_bottom.RHS_evaluate(np.linspace(boundary_bottom.t.min(),boundary_bottom.t.max(),ntest)), 'left': boundary_left.RHS_evaluate(np.linspace(boundary_left.t.min(),boundary_left.t.max(),ntest))})\n",
    "#p = np.random.randint(0,F_valid.shape[0])\n",
    "y, x = np.meshgrid(np.linspace(0, ntest*h, ntest), np.linspace(0, ntest*h, ntest))\n",
    "z = F[p,0,...]\n",
    "#z = soln[p,0,...]\n",
    "z_min, z_max = -np.abs(z).max(), np.abs(z).max()\n",
    "fig, ax = plt.subplots()\n",
    "c = ax.pcolormesh(x, y, z, cmap='RdBu', vmin=z_min, vmax=z_max)\n",
    "ax.set_title('pcolormesh')\n",
    "ax.axis([x.min(), x.max(), y.min(), y.max()])\n",
    "fig.colorbar(c, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.randint(1,ntest-2)\n",
    "j = np.random.randint(1,ntest-2)\n",
    "p = np.random.randint(0,soln_valid.shape[0]-1)\n",
    "print((-4 * soln_valid[p,0,i,j] + soln_valid[p,0,i,j-1] + soln_valid[p,0,i-1,j] + soln_valid[p,0,i+1,j] + soln_valid[p,0,i,j+1])/(h**2))\n",
    "example_soln = a.predict(tf.expand_dims(F_valid[p,...], axis=0))\n",
    "print((-4 * example_soln[0,0,i,j] + example_soln[0,0,i,j-1] + example_soln[0,0,i-1,j] + example_soln[0,0,i+1,j] + example_soln[0,0,i,j+1])/(h**2))\n",
    "print(F_valid[p,0,i,j])\n",
    "print(soln_valid[p,0,i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = np.meshgrid(np.linspace(0,1,20),np.linspace(0,1,20))\n",
    "Z_m = np.array([[np.sin(np.pi*(X+Y)), np.cos(np.pi * (X+Y))] , [np.exp(-X**2 * Y**2), np.tan(np.multiply(X,Y))]])\n",
    "#Z_m = tf.expand_dims(Z, axis = 0)\n",
    "ur = 3\n",
    "b = BilinearUpsample(ur)(input_0)\n",
    "model1 = Model(input_0, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0, Y0 = np.meshgrid(np.linspace(0,1,X.shape[0]*ur),np.linspace(0,1,X.shape[0]*ur))\n",
    "fig, ax = plt.subplots()\n",
    "c = ax.pcolormesh(X0, Y0, model1(Z_m)[0,0,...], cmap='RdBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "c = ax.pcolormesh(X, Y, Z_m[0,0,...], cmap='RdBu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(a.predict(F_valid) - soln_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit a.predict(tf.expand_dims(F_valid[0,...], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeit a.predict(F_valid[0:1000,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx = 110\n",
    "ny = 73\n",
    "X,Y = np.meshgrid(np.linspace(0,1,nx), np.linspace(0,2,ny), indexing = 'ij')\n",
    "f = tf.expand_dims(np.array([X + Y,X*Y, X-Y, X**2 + Y**2, X**2 - Y**2, X-2*Y]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Lp_integral_norm((nx,ny), [0,1,0,2], n_quadpts=20, quadpts_randomization=1, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=2760583, shape=(6,), dtype=float64, numpy=\n",
       "array([2.30940108, 0.94280905, 1.15470054, 2.92895012, 2.24112116,\n",
       "       2.70801284])>"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g(f, np.zeros((6,1,nx,ny), dtype = np.float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.48 ms ± 57.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "timeit g(tf.random.uniform((3000,1,nx,ny), dtype=tf.float64), tf.random.uniform((3000,1,nx,ny), dtype = tf.float64))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.6",
   "language": "python",
   "name": "py3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
