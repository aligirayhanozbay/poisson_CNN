{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from Homogeneous_Poisson_NN import Homogeneous_Poisson_NN_3#,Homogeneous_Poisson_NN\n",
    "from generate_numerical_soln import generate_dataset\n",
    "from generate_analytical_soln import generate_analytical_solution_homogeneous_bc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "tf.keras.backend.set_floatx('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mod = Homogeneous_Poisson_NN_2D(data_format = 'channels_first', mae_component_weight = 1e+0, mse_component_weight = 1e+1)\n",
    "mod = Homogeneous_Poisson_NN_3(data_format='channels_first', mae_component_weight = 1e+0, nmodes=(32,32), filters = [8,16,24,32,40,48,56,64,64], kernel_sizes = [13,11,9,9,9,9,9,9,9], n_quadpts = 27, pyramid_pooling_params = {'pooling_type': 'MAX', 'levels': [[2,2],3,4,5,6,9,12,20]})\n",
    "mod((tf.random.uniform((10,1,74,83), dtype = tf.keras.backend.floatx()), tf.random.uniform((10,1), dtype = tf.keras.backend.floatx())))\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "mod.load_weights('weights/Homogeneous_Poisson_NN/Fourier_24x24modes_big.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nNOTES:\\n-Mixing 'analytical dataset' and Cholesky seems to result in not-so-good behaviour\\n-Learning rate should be super small\\n\\n---these parameters led to decent results on the variable dx 64x64 case: (results saved in Homogeneous_Poisson_NN_2.h5)\\ndataset:\\nbatch size 125, Fourier data excluded\\ntraining:\\ncb = [tf.keras.callbacks.ModelCheckpoint('Homogeneous_Poisson_NN_2.h5', monitor='loss', verbose=1, save_best_only=True, save_weights_only=True), tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', min_lr = 1e-15)]\\nmod.compile(loss = mod.integral_loss, optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-5), metrics = ['mse', 'mae'])\\nmod.run_eagerly = True\\nmod.fit_generator(generator=dataset_generator_2(), steps_per_epoch=600, epochs=5000, validation_data=dataset_generator_2(), validation_steps=3, callbacks=cb)\\n\\nWhen trained on ONLY interpolation dataset, the results for the analytical one are max 10% off pointwise!\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dataset_generator_2(n_batches = 60, dx = 0.1*(np.random.rand() + 0.01), nx = 64, ny = 64):\n",
    "    \n",
    "#     nx = np.random.randint(64,128)\n",
    "#     ny = np.random.randint(64,128)\n",
    "#     if np.random.rand() > 0.5:\n",
    "#         nx = 64\n",
    "#     else:\n",
    "#         nx = 64\n",
    "        \n",
    "#     if np.random.rand() > 0.5:\n",
    "#         ny = 64\n",
    "#     else:\n",
    "#         ny = 64\n",
    "    batch_size = 125\n",
    "    while True:\n",
    "        dx = 0.1*(np.random.rand() + 0.01)\n",
    "        if np.random.rand() < -1:\n",
    "            soln, rhs = generate_analytical_solution_homogeneous_bc(output_shape=(ny,nx), nmodes=(32,32), max_random_magnitude=1.0, domain = [(nx-1)*dx, (ny-1)*dx], n_random=batch_size, expanded_dims = True)\n",
    "            yield ((rhs, dx * tf.ones((batch_size,1), dtype = tf.keras.backend.floatx())), soln)\n",
    "        else:\n",
    "            soln, rhs = generate_dataset(batch_size, [nx,ny], dx, {'top':np.zeros(nx),'bottom':np.zeros(nx),'left':np.zeros(ny),'right':np.zeros(ny)})\n",
    "            yield ((rhs, dx * tf.ones((batch_size,1), dtype = tf.keras.backend.floatx())), soln)\n",
    "def dataset_generator_2_rbg():\n",
    "    batch_size = 100\n",
    "    while True:\n",
    "        nx = np.random.randint(64,85)\n",
    "        ny = np.random.randint(64,85)\n",
    "#         if np.random.rand() > 0.5:\n",
    "#             nx = 64\n",
    "#         else:\n",
    "#             nx = 64\n",
    "\n",
    "#         if np.random.rand() > 0.5:\n",
    "#             ny = 64\n",
    "#         else:\n",
    "#             ny = 64\n",
    "        dx = 0.1*(np.random.rand() + 0.01)\n",
    "        if np.random.rand() > 0.5:\n",
    "            soln, rhs = generate_analytical_solution_homogeneous_bc(output_shape=(ny,nx), nmodes=(32,32), max_random_magnitude=1.0, domain = [(nx-1)*dx, (ny-1)*dx], n_random=batch_size, expanded_dims = True)\n",
    "            yield ((rhs, dx * tf.ones((batch_size,1), dtype = tf.keras.backend.floatx())), soln)\n",
    "        else:\n",
    "            soln, rhs = generate_dataset(batch_size, [nx,ny], dx, {'top':np.zeros(nx),'bottom':np.zeros(nx),'left':np.zeros(ny),'right':np.zeros(ny)})\n",
    "            yield ((rhs, dx * tf.ones((batch_size,1), dtype = tf.keras.backend.floatx())), soln)\n",
    "\n",
    "'''\n",
    "NOTES:\n",
    "-Mixing 'analytical dataset' and Cholesky seems to result in not-so-good behaviour\n",
    "-Learning rate should be super small\n",
    "\n",
    "---these parameters led to decent results on the variable dx 64x64 case: (results saved in Homogeneous_Poisson_NN_2.h5)\n",
    "dataset:\n",
    "batch size 125, Fourier data excluded\n",
    "training:\n",
    "cb = [tf.keras.callbacks.ModelCheckpoint('Homogeneous_Poisson_NN_2.h5', monitor='loss', verbose=1, save_best_only=True, save_weights_only=True), tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', min_lr = 1e-15)]\n",
    "mod.compile(loss = mod.integral_loss, optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-5), metrics = ['mse', 'mae'])\n",
    "mod.run_eagerly = True\n",
    "mod.fit_generator(generator=dataset_generator_2(), steps_per_epoch=600, epochs=5000, validation_data=dataset_generator_2(), validation_steps=3, callbacks=cb)\n",
    "\n",
    "When trained on ONLY interpolation dataset, the results for the analytical one are max 10% off pointwise!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.1456 - mse: 0.0011 - mae: 0.0177\n",
      "Epoch 00001: mse improved from inf to 0.00109, saving model to weights/Homogeneous_Poisson_NN/Fourier_24x24modes_big.h5\n",
      "200/200 [==============================] - 432s 2s/step - loss: 0.1474 - mse: 0.0011 - mae: 0.0179 - val_loss: 0.0806 - val_mse: 3.4071e-04 - val_mae: 0.0112\n",
      "Epoch 2/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.1196 - mse: 7.8295e-04 - mae: 0.0147\n",
      "Epoch 00002: mse improved from 0.00109 to 0.00078, saving model to weights/Homogeneous_Poisson_NN/Fourier_24x24modes_big.h5\n",
      "200/200 [==============================] - 427s 2s/step - loss: 0.1190 - mse: 7.7947e-04 - mae: 0.0147 - val_loss: 0.0502 - val_mse: 2.1737e-04 - val_mae: 0.0065\n",
      "Epoch 3/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.1207 - mse: 7.2725e-04 - mae: 0.0145\n",
      "Epoch 00003: mse improved from 0.00078 to 0.00072, saving model to weights/Homogeneous_Poisson_NN/Fourier_24x24modes_big.h5\n",
      "200/200 [==============================] - 417s 2s/step - loss: 0.1204 - mse: 7.2483e-04 - mae: 0.0145 - val_loss: 0.1478 - val_mse: 7.7295e-04 - val_mae: 0.0161\n",
      "Epoch 4/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.1142 - mse: 6.6766e-04 - mae: 0.0137\n",
      "Epoch 00004: mse improved from 0.00072 to 0.00067, saving model to weights/Homogeneous_Poisson_NN/Fourier_24x24modes_big.h5\n",
      "200/200 [==============================] - 418s 2s/step - loss: 0.1143 - mse: 6.6657e-04 - mae: 0.0137 - val_loss: 0.1693 - val_mse: 9.3978e-04 - val_mae: 0.0225\n",
      "Epoch 5/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.1405 - mse: 0.0010 - mae: 0.0165\n",
      "Epoch 00005: mse did not improve from 0.00067\n",
      "200/200 [==============================] - 420s 2s/step - loss: 0.1406 - mse: 0.0010 - mae: 0.0165 - val_loss: 0.3596 - val_mse: 0.0029 - val_mae: 0.0372\n",
      "Epoch 6/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0978 - mse: 5.5280e-04 - mae: 0.0120\n",
      "Epoch 00006: mse improved from 0.00067 to 0.00055, saving model to weights/Homogeneous_Poisson_NN/Fourier_24x24modes_big.h5\n",
      "200/200 [==============================] - 432s 2s/step - loss: 0.0979 - mse: 5.5238e-04 - mae: 0.0120 - val_loss: 0.0228 - val_mse: 5.8900e-05 - val_mae: 0.0045\n",
      "Epoch 7/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.1029 - mse: 5.2050e-04 - mae: 0.0126\n",
      "Epoch 00007: mse improved from 0.00055 to 0.00052, saving model to weights/Homogeneous_Poisson_NN/Fourier_24x24modes_big.h5\n",
      "200/200 [==============================] - 419s 2s/step - loss: 0.1028 - mse: 5.1987e-04 - mae: 0.0126 - val_loss: 0.0467 - val_mse: 1.3712e-04 - val_mae: 0.0062\n",
      "Epoch 8/5000\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0972 - mse: 4.7969e-04 - mae: 0.0119\n",
      "Epoch 00008: mse improved from 0.00052 to 0.00048, saving model to weights/Homogeneous_Poisson_NN/Fourier_24x24modes_big.h5\n",
      "200/200 [==============================] - 414s 2s/step - loss: 0.0967 - mse: 4.7762e-04 - mae: 0.0118 - val_loss: 0.1447 - val_mse: 7.3924e-04 - val_mae: 0.0170\n",
      "Epoch 9/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.1291 - mse: 7.6694e-04 - mae: 0.0150\n",
      "Epoch 00009: mse did not improve from 0.00048\n",
      "200/200 [==============================] - 435s 2s/step - loss: 0.1285 - mse: 7.6321e-04 - mae: 0.0149 - val_loss: 0.1219 - val_mse: 7.4725e-04 - val_mae: 0.0188\n",
      "Epoch 10/5000\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0944 - mse: 5.8031e-04 - mae: 0.0114\n",
      "Epoch 00010: mse did not improve from 0.00048\n",
      "200/200 [==============================] - 400s 2s/step - loss: 0.0941 - mse: 5.7736e-04 - mae: 0.0114 - val_loss: 0.1156 - val_mse: 5.7475e-04 - val_mae: 0.0116\n",
      "Epoch 11/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0908 - mse: 4.3594e-04 - mae: 0.0110\n",
      "Epoch 00011: mse improved from 0.00048 to 0.00043, saving model to weights/Homogeneous_Poisson_NN/Fourier_24x24modes_big.h5\n",
      "200/200 [==============================] - 427s 2s/step - loss: 0.0905 - mse: 4.3442e-04 - mae: 0.0110 - val_loss: 0.1822 - val_mse: 0.0012 - val_mae: 0.0177\n",
      "Epoch 12/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0992 - mse: 5.5320e-04 - mae: 0.0123\n",
      "Epoch 00012: mse did not improve from 0.00043\n",
      "200/200 [==============================] - 430s 2s/step - loss: 0.1013 - mse: 5.6892e-04 - mae: 0.0125 - val_loss: 0.2196 - val_mse: 0.0015 - val_mae: 0.0234\n",
      "Epoch 13/5000\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0882 - mse: 4.6855e-04 - mae: 0.0107\n",
      "Epoch 00013: mse did not improve from 0.00043\n",
      "200/200 [==============================] - 410s 2s/step - loss: 0.0879 - mse: 4.6644e-04 - mae: 0.0107 - val_loss: 0.0893 - val_mse: 4.0916e-04 - val_mae: 0.0109\n",
      "Epoch 14/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.1009 - mse: 5.1756e-04 - mae: 0.0120\n",
      "Epoch 00014: mse did not improve from 0.00043\n",
      "200/200 [==============================] - 417s 2s/step - loss: 0.1008 - mse: 5.1633e-04 - mae: 0.0120 - val_loss: 0.0999 - val_mse: 4.1916e-04 - val_mae: 0.0140\n",
      "Epoch 15/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0842 - mse: 4.0113e-04 - mae: 0.0103\n",
      "Epoch 00015: mse improved from 0.00043 to 0.00040, saving model to weights/Homogeneous_Poisson_NN/Fourier_24x24modes_big.h5\n",
      "200/200 [==============================] - 414s 2s/step - loss: 0.0840 - mse: 3.9977e-04 - mae: 0.0103 - val_loss: 0.0544 - val_mse: 1.5265e-04 - val_mae: 0.0076\n",
      "Epoch 16/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0796 - mse: 3.3340e-04 - mae: 0.0100\n",
      "Epoch 00016: mse improved from 0.00040 to 0.00033, saving model to weights/Homogeneous_Poisson_NN/Fourier_24x24modes_big.h5\n",
      "200/200 [==============================] - 426s 2s/step - loss: 0.0793 - mse: 3.3157e-04 - mae: 0.0099 - val_loss: 0.0576 - val_mse: 2.2634e-04 - val_mae: 0.0076\n",
      "Epoch 17/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.1114 - mse: 5.9692e-04 - mae: 0.0129\n",
      "Epoch 00017: mse did not improve from 0.00033\n",
      "200/200 [==============================] - 417s 2s/step - loss: 0.1109 - mse: 5.9395e-04 - mae: 0.0129 - val_loss: 0.0304 - val_mse: 6.4048e-05 - val_mae: 0.0055\n",
      "Epoch 18/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0883 - mse: 4.0858e-04 - mae: 0.0107\n",
      "Epoch 00018: mse did not improve from 0.00033\n",
      "200/200 [==============================] - 429s 2s/step - loss: 0.0882 - mse: 4.0760e-04 - mae: 0.0107 - val_loss: 0.3633 - val_mse: 0.0049 - val_mae: 0.0366\n",
      "Epoch 19/5000\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0734 - mse: 3.1494e-04 - mae: 0.0090\n",
      "Epoch 00019: mse improved from 0.00033 to 0.00031, saving model to weights/Homogeneous_Poisson_NN/Fourier_24x24modes_big.h5\n",
      "200/200 [==============================] - 405s 2s/step - loss: 0.0735 - mse: 3.1444e-04 - mae: 0.0090 - val_loss: 0.0526 - val_mse: 1.2013e-04 - val_mae: 0.0063\n",
      "Epoch 20/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.1027 - mse: 6.0921e-04 - mae: 0.0122\n",
      "Epoch 00020: mse did not improve from 0.00031\n",
      "200/200 [==============================] - 420s 2s/step - loss: 0.1022 - mse: 6.0641e-04 - mae: 0.0121 - val_loss: 0.0015 - val_mse: 8.0151e-07 - val_mae: 6.1132e-04\n",
      "Epoch 21/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.1038 - mse: 5.1886e-04 - mae: 0.0123\n",
      "Epoch 00021: mse did not improve from 0.00031\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "200/200 [==============================] - 431s 2s/step - loss: 0.1040 - mse: 5.1837e-04 - mae: 0.0123 - val_loss: 0.0157 - val_mse: 3.3366e-05 - val_mae: 0.0027\n",
      "Epoch 22/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0689 - mse: 2.5909e-04 - mae: 0.0084\n",
      "Epoch 00022: mse improved from 0.00031 to 0.00026, saving model to weights/Homogeneous_Poisson_NN/Fourier_24x24modes_big.h5\n",
      "200/200 [==============================] - 424s 2s/step - loss: 0.0688 - mse: 2.5803e-04 - mae: 0.0084 - val_loss: 0.1248 - val_mse: 5.8126e-04 - val_mae: 0.0137\n",
      "Epoch 23/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0589 - mse: 1.9453e-04 - mae: 0.0073\n",
      "Epoch 00023: mse improved from 0.00026 to 0.00019, saving model to weights/Homogeneous_Poisson_NN/Fourier_24x24modes_big.h5\n",
      "200/200 [==============================] - 420s 2s/step - loss: 0.0586 - mse: 1.9350e-04 - mae: 0.0073 - val_loss: 0.0818 - val_mse: 3.6010e-04 - val_mae: 0.0097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0665 - mse: 2.4785e-04 - mae: 0.0080\n",
      "Epoch 00024: mse did not improve from 0.00019\n",
      "200/200 [==============================] - 413s 2s/step - loss: 0.0663 - mse: 2.4668e-04 - mae: 0.0080 - val_loss: 0.0486 - val_mse: 9.4196e-05 - val_mae: 0.0063\n",
      "Epoch 25/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0608 - mse: 2.0545e-04 - mae: 0.0077\n",
      "Epoch 00025: mse did not improve from 0.00019\n",
      "200/200 [==============================] - 433s 2s/step - loss: 0.0610 - mse: 2.0596e-04 - mae: 0.0077 - val_loss: 0.1872 - val_mse: 0.0011 - val_mae: 0.0209\n",
      "Epoch 26/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0718 - mse: 2.9576e-04 - mae: 0.0087\n",
      "Epoch 00026: mse did not improve from 0.00019\n",
      "200/200 [==============================] - 434s 2s/step - loss: 0.0714 - mse: 2.9409e-04 - mae: 0.0086 - val_loss: 0.0973 - val_mse: 2.4916e-04 - val_mae: 0.0106\n",
      "Epoch 27/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0613 - mse: 2.4025e-04 - mae: 0.0075\n",
      "Epoch 00027: mse did not improve from 0.00019\n",
      "200/200 [==============================] - 425s 2s/step - loss: 0.0612 - mse: 2.3963e-04 - mae: 0.0075 - val_loss: 0.0393 - val_mse: 1.0031e-04 - val_mae: 0.0053\n",
      "Epoch 28/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0700 - mse: 2.6565e-04 - mae: 0.0083\n",
      "Epoch 00028: mse did not improve from 0.00019\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "200/200 [==============================] - 431s 2s/step - loss: 0.0697 - mse: 2.6439e-04 - mae: 0.0083 - val_loss: 0.0055 - val_mse: 4.5732e-06 - val_mae: 0.0016\n",
      "Epoch 29/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0651 - mse: 2.3915e-04 - mae: 0.0079\n",
      "Epoch 00029: mse did not improve from 0.00019\n",
      "200/200 [==============================] - 420s 2s/step - loss: 0.0648 - mse: 2.3799e-04 - mae: 0.0079 - val_loss: 0.0755 - val_mse: 2.4529e-04 - val_mae: 0.0091\n",
      "Epoch 30/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0663 - mse: 2.4148e-04 - mae: 0.0079\n",
      "Epoch 00030: mse did not improve from 0.00019\n",
      "200/200 [==============================] - 420s 2s/step - loss: 0.0662 - mse: 2.4074e-04 - mae: 0.0079 - val_loss: 0.2192 - val_mse: 0.0016 - val_mae: 0.0263\n",
      "Epoch 31/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0743 - mse: 2.9981e-04 - mae: 0.0086\n",
      "Epoch 00031: mse did not improve from 0.00019\n",
      "200/200 [==============================] - 425s 2s/step - loss: 0.0741 - mse: 2.9885e-04 - mae: 0.0086 - val_loss: 0.1118 - val_mse: 4.4711e-04 - val_mae: 0.0133\n",
      "Epoch 32/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0655 - mse: 2.4591e-04 - mae: 0.0080\n",
      "Epoch 00032: mse did not improve from 0.00019\n",
      "200/200 [==============================] - 424s 2s/step - loss: 0.0653 - mse: 2.4494e-04 - mae: 0.0080 - val_loss: 0.0745 - val_mse: 2.5065e-04 - val_mae: 0.0089\n",
      "Epoch 33/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0600 - mse: 2.1938e-04 - mae: 0.0074\n",
      "Epoch 00033: mse did not improve from 0.00019\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0000000116860975e-08.\n",
      "200/200 [==============================] - 419s 2s/step - loss: 0.0597 - mse: 2.1832e-04 - mae: 0.0074 - val_loss: 0.0264 - val_mse: 6.3736e-05 - val_mae: 0.0045\n",
      "Epoch 34/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0592 - mse: 2.1429e-04 - mae: 0.0071\n",
      "Epoch 00034: mse did not improve from 0.00019\n",
      "200/200 [==============================] - 418s 2s/step - loss: 0.0602 - mse: 2.1898e-04 - mae: 0.0072 - val_loss: 0.0036 - val_mse: 3.0734e-06 - val_mae: 0.0010\n",
      "Epoch 35/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0697 - mse: 2.6393e-04 - mae: 0.0082\n",
      "Epoch 00035: mse did not improve from 0.00019\n",
      "200/200 [==============================] - 422s 2s/step - loss: 0.0694 - mse: 2.6274e-04 - mae: 0.0082 - val_loss: 0.0270 - val_mse: 4.1630e-05 - val_mae: 0.0032\n",
      "Epoch 36/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0582 - mse: 2.1738e-04 - mae: 0.0070\n",
      "Epoch 00036: mse did not improve from 0.00019\n",
      "200/200 [==============================] - 418s 2s/step - loss: 0.0583 - mse: 2.1745e-04 - mae: 0.0071 - val_loss: 0.0639 - val_mse: 2.2783e-04 - val_mae: 0.0079\n",
      "Epoch 37/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0723 - mse: 2.8376e-04 - mae: 0.0085\n",
      "Epoch 00037: mse did not improve from 0.00019\n",
      "200/200 [==============================] - 438s 2s/step - loss: 0.0723 - mse: 2.8318e-04 - mae: 0.0085 - val_loss: 0.0344 - val_mse: 7.7987e-05 - val_mae: 0.0041\n",
      "Epoch 38/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0731 - mse: 2.7709e-04 - mae: 0.0087\n",
      "Epoch 00038: mse did not improve from 0.00019\n",
      "200/200 [==============================] - 424s 2s/step - loss: 0.0732 - mse: 2.7682e-04 - mae: 0.0087 - val_loss: 0.0505 - val_mse: 8.5811e-05 - val_mae: 0.0060\n",
      "Epoch 39/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0646 - mse: 2.3635e-04 - mae: 0.0078\n",
      "Epoch 00039: mse did not improve from 0.00019\n",
      "200/200 [==============================] - 410s 2s/step - loss: 0.0643 - mse: 2.3527e-04 - mae: 0.0077 - val_loss: 0.0588 - val_mse: 1.9695e-04 - val_mae: 0.0073\n",
      "Epoch 40/5000\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0628 - mse: 2.1771e-04 - mae: 0.0076\n",
      "Epoch 00040: mse did not improve from 0.00019\n",
      "200/200 [==============================] - 404s 2s/step - loss: 0.0640 - mse: 2.2618e-04 - mae: 0.0078 - val_loss: 0.0434 - val_mse: 8.9223e-05 - val_mae: 0.0065\n",
      "Epoch 41/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0662 - mse: 2.4095e-04 - mae: 0.0081\n",
      "Epoch 00041: mse did not improve from 0.00019\n",
      "200/200 [==============================] - 445s 2s/step - loss: 0.0666 - mse: 2.4321e-04 - mae: 0.0081 - val_loss: 0.0849 - val_mse: 2.4341e-04 - val_mae: 0.0111\n",
      "Epoch 42/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0644 - mse: 2.4012e-04 - mae: 0.0079\n",
      "Epoch 00042: mse did not improve from 0.00019\n",
      "200/200 [==============================] - 432s 2s/step - loss: 0.0640 - mse: 2.3892e-04 - mae: 0.0079 - val_loss: 0.0835 - val_mse: 2.4975e-04 - val_mae: 0.0097\n",
      "Epoch 43/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0736 - mse: 2.5877e-04 - mae: 0.0087\n",
      "Epoch 00043: mse did not improve from 0.00019\n",
      "200/200 [==============================] - 425s 2s/step - loss: 0.0732 - mse: 2.5759e-04 - mae: 0.0086 - val_loss: 0.0028 - val_mse: 2.0056e-06 - val_mae: 6.5962e-04\n",
      "Epoch 44/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0700 - mse: 2.4999e-04 - mae: 0.0082\n",
      "Epoch 00044: mse did not improve from 0.00019\n",
      "200/200 [==============================] - 428s 2s/step - loss: 0.0705 - mse: 2.5180e-04 - mae: 0.0082 - val_loss: 0.0151 - val_mse: 1.9931e-05 - val_mae: 0.0028\n",
      "Epoch 45/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0646 - mse: 2.3535e-04 - mae: 0.0080\n",
      "Epoch 00045: mse did not improve from 0.00019\n",
      "200/200 [==============================] - 433s 2s/step - loss: 0.0643 - mse: 2.3428e-04 - mae: 0.0079 - val_loss: 0.0865 - val_mse: 2.9533e-04 - val_mae: 0.0103\n",
      "Epoch 46/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0580 - mse: 2.1250e-04 - mae: 0.0070\n",
      "Epoch 00046: mse did not improve from 0.00019\n",
      "200/200 [==============================] - 425s 2s/step - loss: 0.0578 - mse: 2.1162e-04 - mae: 0.0070 - val_loss: 0.1727 - val_mse: 0.0011 - val_mae: 0.0170\n",
      "Epoch 47/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0680 - mse: 2.5907e-04 - mae: 0.0082\n",
      "Epoch 00047: mse did not improve from 0.00019\n",
      "200/200 [==============================] - 435s 2s/step - loss: 0.0677 - mse: 2.5764e-04 - mae: 0.0082 - val_loss: 0.0455 - val_mse: 9.1157e-05 - val_mae: 0.0062\n",
      "Epoch 48/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0645 - mse: 2.2653e-04 - mae: 0.0078\n",
      "Epoch 00048: mse did not improve from 0.00019\n",
      "200/200 [==============================] - 425s 2s/step - loss: 0.0654 - mse: 2.3187e-04 - mae: 0.0079 - val_loss: 0.0412 - val_mse: 8.7014e-05 - val_mae: 0.0067\n",
      "Epoch 49/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0534 - mse: 1.8282e-04 - mae: 0.0068\n",
      "Epoch 00049: mse improved from 0.00019 to 0.00018, saving model to weights/Homogeneous_Poisson_NN/Fourier_24x24modes_big.h5\n",
      "200/200 [==============================] - 435s 2s/step - loss: 0.0531 - mse: 1.8187e-04 - mae: 0.0068 - val_loss: 0.0629 - val_mse: 1.3749e-04 - val_mae: 0.0077\n",
      "Epoch 50/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0563 - mse: 1.8982e-04 - mae: 0.0071\n",
      "Epoch 00050: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 424s 2s/step - loss: 0.0569 - mse: 1.9266e-04 - mae: 0.0072 - val_loss: 0.0818 - val_mse: 4.6883e-04 - val_mae: 0.0097\n",
      "Epoch 51/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0566 - mse: 2.0276e-04 - mae: 0.0071\n",
      "Epoch 00051: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 431s 2s/step - loss: 0.0567 - mse: 2.0241e-04 - mae: 0.0071 - val_loss: 0.0354 - val_mse: 8.0617e-05 - val_mae: 0.0066\n",
      "Epoch 52/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0721 - mse: 2.8136e-04 - mae: 0.0083\n",
      "Epoch 00052: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 418s 2s/step - loss: 0.0718 - mse: 2.8006e-04 - mae: 0.0083 - val_loss: 0.0699 - val_mse: 1.2855e-04 - val_mae: 0.0073\n",
      "Epoch 53/5000\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0636 - mse: 2.3292e-04 - mae: 0.0077\n",
      "Epoch 00053: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 411s 2s/step - loss: 0.0633 - mse: 2.3191e-04 - mae: 0.0077 - val_loss: 0.0861 - val_mse: 5.7607e-04 - val_mae: 0.0113\n",
      "Epoch 54/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0595 - mse: 2.0639e-04 - mae: 0.0075\n",
      "Epoch 00054: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 430s 2s/step - loss: 0.0601 - mse: 2.0997e-04 - mae: 0.0075 - val_loss: 0.0152 - val_mse: 3.0221e-05 - val_mae: 0.0032\n",
      "Epoch 55/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0661 - mse: 2.5825e-04 - mae: 0.0079\n",
      "Epoch 00055: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 431s 2s/step - loss: 0.0662 - mse: 2.5845e-04 - mae: 0.0079 - val_loss: 0.0953 - val_mse: 3.5015e-04 - val_mae: 0.0132\n",
      "Epoch 56/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0641 - mse: 2.2434e-04 - mae: 0.0079\n",
      "Epoch 00056: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 418s 2s/step - loss: 0.0645 - mse: 2.2597e-04 - mae: 0.0079 - val_loss: 0.0093 - val_mse: 1.2032e-05 - val_mae: 0.0022\n",
      "Epoch 57/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0630 - mse: 2.1952e-04 - mae: 0.0077\n",
      "Epoch 00057: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 427s 2s/step - loss: 0.0637 - mse: 2.2210e-04 - mae: 0.0077 - val_loss: 0.0326 - val_mse: 5.6539e-05 - val_mae: 0.0048\n",
      "Epoch 58/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0719 - mse: 2.7220e-04 - mae: 0.0086\n",
      "Epoch 00058: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 438s 2s/step - loss: 0.0716 - mse: 2.7076e-04 - mae: 0.0086 - val_loss: 0.0528 - val_mse: 1.1785e-04 - val_mae: 0.0060\n",
      "Epoch 59/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0681 - mse: 2.5271e-04 - mae: 0.0080\n",
      "Epoch 00059: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 429s 2s/step - loss: 0.0678 - mse: 2.5158e-04 - mae: 0.0080 - val_loss: 0.1316 - val_mse: 7.0212e-04 - val_mae: 0.0166\n",
      "Epoch 60/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0603 - mse: 2.1036e-04 - mae: 0.0072\n",
      "Epoch 00060: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 416s 2s/step - loss: 0.0601 - mse: 2.0987e-04 - mae: 0.0072 - val_loss: 0.0126 - val_mse: 1.7553e-05 - val_mae: 0.0030\n",
      "Epoch 61/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0670 - mse: 2.5465e-04 - mae: 0.0083\n",
      "Epoch 00061: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 435s 2s/step - loss: 0.0673 - mse: 2.5581e-04 - mae: 0.0083 - val_loss: 0.1012 - val_mse: 3.1616e-04 - val_mae: 0.0117\n",
      "Epoch 62/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0609 - mse: 2.1832e-04 - mae: 0.0073\n",
      "Epoch 00062: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 414s 2s/step - loss: 0.0608 - mse: 2.1755e-04 - mae: 0.0073 - val_loss: 0.0681 - val_mse: 3.8828e-04 - val_mae: 0.0088\n",
      "Epoch 63/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0594 - mse: 2.1312e-04 - mae: 0.0076\n",
      "Epoch 00063: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 427s 2s/step - loss: 0.0592 - mse: 2.1213e-04 - mae: 0.0075 - val_loss: 0.0318 - val_mse: 6.3487e-05 - val_mae: 0.0047\n",
      "Epoch 64/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0632 - mse: 2.3124e-04 - mae: 0.0076\n",
      "Epoch 00064: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 420s 2s/step - loss: 0.0638 - mse: 2.3281e-04 - mae: 0.0077 - val_loss: 0.0288 - val_mse: 4.6567e-05 - val_mae: 0.0042\n",
      "Epoch 65/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0707 - mse: 2.5898e-04 - mae: 0.0084\n",
      "Epoch 00065: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 418s 2s/step - loss: 0.0705 - mse: 2.5797e-04 - mae: 0.0084 - val_loss: 0.0362 - val_mse: 7.2720e-05 - val_mae: 0.0038\n",
      "Epoch 66/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0669 - mse: 2.3567e-04 - mae: 0.0081\n",
      "Epoch 00066: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 434s 2s/step - loss: 0.0666 - mse: 2.3461e-04 - mae: 0.0081 - val_loss: 0.0878 - val_mse: 2.2456e-04 - val_mae: 0.0103\n",
      "Epoch 67/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0784 - mse: 2.9014e-04 - mae: 0.0093\n",
      "Epoch 00067: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 438s 2s/step - loss: 0.0782 - mse: 2.8884e-04 - mae: 0.0093 - val_loss: 0.0498 - val_mse: 1.2003e-04 - val_mae: 0.0068\n",
      "Epoch 68/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0673 - mse: 2.6911e-04 - mae: 0.0082\n",
      "Epoch 00068: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 434s 2s/step - loss: 0.0670 - mse: 2.6765e-04 - mae: 0.0081 - val_loss: 0.0158 - val_mse: 2.0097e-05 - val_mae: 0.0032\n",
      "Epoch 69/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0579 - mse: 2.0818e-04 - mae: 0.0071\n",
      "Epoch 00069: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 424s 2s/step - loss: 0.0576 - mse: 2.0711e-04 - mae: 0.0071 - val_loss: 0.1046 - val_mse: 3.4000e-04 - val_mae: 0.0125\n",
      "Epoch 70/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0569 - mse: 1.9956e-04 - mae: 0.0069\n",
      "Epoch 00070: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 425s 2s/step - loss: 0.0568 - mse: 1.9891e-04 - mae: 0.0069 - val_loss: 0.0271 - val_mse: 4.7980e-05 - val_mae: 0.0043\n",
      "Epoch 71/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0612 - mse: 2.3481e-04 - mae: 0.0074\n",
      "Epoch 00071: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 424s 2s/step - loss: 0.0610 - mse: 2.3396e-04 - mae: 0.0074 - val_loss: 0.1299 - val_mse: 5.6212e-04 - val_mae: 0.0133\n",
      "Epoch 72/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0674 - mse: 2.6277e-04 - mae: 0.0081\n",
      "Epoch 00072: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 432s 2s/step - loss: 0.0671 - mse: 2.6154e-04 - mae: 0.0081 - val_loss: 0.0162 - val_mse: 2.3093e-05 - val_mae: 0.0034\n",
      "Epoch 73/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0642 - mse: 2.2437e-04 - mae: 0.0077\n",
      "Epoch 00073: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 423s 2s/step - loss: 0.0639 - mse: 2.2335e-04 - mae: 0.0076 - val_loss: 0.0628 - val_mse: 2.3790e-04 - val_mae: 0.0078\n",
      "Epoch 74/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0638 - mse: 2.3174e-04 - mae: 0.0077\n",
      "Epoch 00074: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 436s 2s/step - loss: 0.0645 - mse: 2.3461e-04 - mae: 0.0078 - val_loss: 0.0356 - val_mse: 1.0003e-04 - val_mae: 0.0053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0592 - mse: 2.0745e-04 - mae: 0.0073\n",
      "Epoch 00075: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 417s 2s/step - loss: 0.0590 - mse: 2.0647e-04 - mae: 0.0073 - val_loss: 0.1130 - val_mse: 5.7075e-04 - val_mae: 0.0138\n",
      "Epoch 76/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0684 - mse: 2.5107e-04 - mae: 0.0082\n",
      "Epoch 00076: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 428s 2s/step - loss: 0.0681 - mse: 2.4979e-04 - mae: 0.0082 - val_loss: 0.0307 - val_mse: 4.9395e-05 - val_mae: 0.0043\n",
      "Epoch 77/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0610 - mse: 2.3676e-04 - mae: 0.0073\n",
      "Epoch 00077: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 421s 2s/step - loss: 0.0610 - mse: 2.3630e-04 - mae: 0.0073 - val_loss: 0.1660 - val_mse: 9.2907e-04 - val_mae: 0.0151\n",
      "Epoch 78/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0738 - mse: 2.8873e-04 - mae: 0.0087\n",
      "Epoch 00078: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 432s 2s/step - loss: 0.0744 - mse: 2.9199e-04 - mae: 0.0087 - val_loss: 0.0064 - val_mse: 9.2506e-06 - val_mae: 0.0017\n",
      "Epoch 79/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0684 - mse: 2.7955e-04 - mae: 0.0079\n",
      "Epoch 00079: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 430s 2s/step - loss: 0.0682 - mse: 2.7814e-04 - mae: 0.0079 - val_loss: 0.0054 - val_mse: 4.3637e-06 - val_mae: 0.0013\n",
      "Epoch 80/5000\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0652 - mse: 2.1913e-04 - mae: 0.0077\n",
      "Epoch 00080: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 402s 2s/step - loss: 0.0649 - mse: 2.1797e-04 - mae: 0.0077 - val_loss: 0.0015 - val_mse: 9.0390e-07 - val_mae: 5.7019e-04\n",
      "Epoch 81/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0663 - mse: 2.4994e-04 - mae: 0.0078\n",
      "Epoch 00081: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 420s 2s/step - loss: 0.0660 - mse: 2.4887e-04 - mae: 0.0078 - val_loss: 0.0918 - val_mse: 2.6546e-04 - val_mae: 0.0109\n",
      "Epoch 82/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0661 - mse: 2.5770e-04 - mae: 0.0080\n",
      "Epoch 00082: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 435s 2s/step - loss: 0.0666 - mse: 2.5941e-04 - mae: 0.0081 - val_loss: 0.0331 - val_mse: 6.2319e-05 - val_mae: 0.0058\n",
      "Epoch 83/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0648 - mse: 2.2541e-04 - mae: 0.0078\n",
      "Epoch 00083: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 430s 2s/step - loss: 0.0645 - mse: 2.2443e-04 - mae: 0.0078 - val_loss: 0.1156 - val_mse: 5.5490e-04 - val_mae: 0.0120\n",
      "Epoch 84/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0610 - mse: 2.1062e-04 - mae: 0.0074\n",
      "Epoch 00084: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 420s 2s/step - loss: 0.0614 - mse: 2.1160e-04 - mae: 0.0074 - val_loss: 0.0142 - val_mse: 2.5749e-05 - val_mae: 0.0033\n",
      "Epoch 85/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0604 - mse: 2.1055e-04 - mae: 0.0074\n",
      "Epoch 00085: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 421s 2s/step - loss: 0.0609 - mse: 2.1276e-04 - mae: 0.0075 - val_loss: 0.0160 - val_mse: 1.8987e-05 - val_mae: 0.0028\n",
      "Epoch 86/5000\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0573 - mse: 1.8541e-04 - mae: 0.0069\n",
      "Epoch 00086: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 407s 2s/step - loss: 0.0571 - mse: 1.8472e-04 - mae: 0.0069 - val_loss: 0.1844 - val_mse: 8.7675e-04 - val_mae: 0.0205\n",
      "Epoch 87/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0655 - mse: 2.3034e-04 - mae: 0.0079\n",
      "Epoch 00087: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 415s 2s/step - loss: 0.0652 - mse: 2.2926e-04 - mae: 0.0079 - val_loss: 0.0535 - val_mse: 9.1236e-05 - val_mae: 0.0063\n",
      "Epoch 88/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0672 - mse: 2.4274e-04 - mae: 0.0079\n",
      "Epoch 00088: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 422s 2s/step - loss: 0.0682 - mse: 2.4889e-04 - mae: 0.0080 - val_loss: 0.1174 - val_mse: 4.4830e-04 - val_mae: 0.0139\n",
      "Epoch 89/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0607 - mse: 2.0553e-04 - mae: 0.0075\n",
      "Epoch 00089: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 425s 2s/step - loss: 0.0610 - mse: 2.0611e-04 - mae: 0.0075 - val_loss: 0.0046 - val_mse: 3.1505e-06 - val_mae: 0.0010\n",
      "Epoch 90/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0742 - mse: 2.6783e-04 - mae: 0.0089\n",
      "Epoch 00090: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 448s 2s/step - loss: 0.0739 - mse: 2.6652e-04 - mae: 0.0088 - val_loss: 0.0351 - val_mse: 5.8007e-05 - val_mae: 0.0043\n",
      "Epoch 91/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0706 - mse: 2.5099e-04 - mae: 0.0086\n",
      "Epoch 00091: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 425s 2s/step - loss: 0.0705 - mse: 2.5006e-04 - mae: 0.0086 - val_loss: 0.0638 - val_mse: 1.1816e-04 - val_mae: 0.0073\n",
      "Epoch 92/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0773 - mse: 2.8087e-04 - mae: 0.0088\n",
      "Epoch 00092: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 425s 2s/step - loss: 0.0770 - mse: 2.7957e-04 - mae: 0.0088 - val_loss: 0.0414 - val_mse: 1.2513e-04 - val_mae: 0.0060\n",
      "Epoch 93/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0666 - mse: 2.5222e-04 - mae: 0.0079\n",
      "Epoch 00093: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 431s 2s/step - loss: 0.0662 - mse: 2.5095e-04 - mae: 0.0079 - val_loss: 0.0731 - val_mse: 2.3330e-04 - val_mae: 0.0108\n",
      "Epoch 94/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0690 - mse: 2.8156e-04 - mae: 0.0080\n",
      "Epoch 00094: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 440s 2s/step - loss: 0.0686 - mse: 2.8036e-04 - mae: 0.0080 - val_loss: 0.0084 - val_mse: 7.8678e-06 - val_mae: 0.0020\n",
      "Epoch 95/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0748 - mse: 2.9905e-04 - mae: 0.0088\n",
      "Epoch 00095: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 421s 2s/step - loss: 0.0745 - mse: 2.9757e-04 - mae: 0.0087 - val_loss: 0.0173 - val_mse: 2.3037e-05 - val_mae: 0.0029\n",
      "Epoch 96/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0675 - mse: 2.3654e-04 - mae: 0.0081\n",
      "Epoch 00096: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 426s 2s/step - loss: 0.0671 - mse: 2.3550e-04 - mae: 0.0081 - val_loss: 0.0207 - val_mse: 2.9986e-05 - val_mae: 0.0033\n",
      "Epoch 97/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0613 - mse: 2.1226e-04 - mae: 0.0072\n",
      "Epoch 00097: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 413s 2s/step - loss: 0.0611 - mse: 2.1149e-04 - mae: 0.0072 - val_loss: 0.0834 - val_mse: 2.5611e-04 - val_mae: 0.0096\n",
      "Epoch 98/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0688 - mse: 2.5882e-04 - mae: 0.0081\n",
      "Epoch 00098: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 424s 2s/step - loss: 0.0689 - mse: 2.5844e-04 - mae: 0.0081 - val_loss: 0.0029 - val_mse: 2.2736e-06 - val_mae: 0.0011\n",
      "Epoch 99/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0641 - mse: 2.3521e-04 - mae: 0.0078\n",
      "Epoch 00099: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 438s 2s/step - loss: 0.0638 - mse: 2.3414e-04 - mae: 0.0078 - val_loss: 0.1089 - val_mse: 4.0085e-04 - val_mae: 0.0123\n",
      "Epoch 100/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0675 - mse: 2.3266e-04 - mae: 0.0080\n",
      "Epoch 00100: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 419s 2s/step - loss: 0.0672 - mse: 2.3154e-04 - mae: 0.0080 - val_loss: 0.0361 - val_mse: 8.6597e-05 - val_mae: 0.0055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0713 - mse: 2.6030e-04 - mae: 0.0084\n",
      "Epoch 00101: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 436s 2s/step - loss: 0.0710 - mse: 2.5907e-04 - mae: 0.0084 - val_loss: 0.0401 - val_mse: 7.2142e-05 - val_mae: 0.0051\n",
      "Epoch 102/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0535 - mse: 1.8843e-04 - mae: 0.0068\n",
      "Epoch 00102: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 422s 2s/step - loss: 0.0533 - mse: 1.8755e-04 - mae: 0.0068 - val_loss: 0.0950 - val_mse: 2.9185e-04 - val_mae: 0.0119\n",
      "Epoch 103/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0679 - mse: 2.3910e-04 - mae: 0.0081\n",
      "Epoch 00103: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 423s 2s/step - loss: 0.0679 - mse: 2.3845e-04 - mae: 0.0081 - val_loss: 0.0740 - val_mse: 2.6953e-04 - val_mae: 0.0099\n",
      "Epoch 104/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0791 - mse: 3.0217e-04 - mae: 0.0092\n",
      "Epoch 00104: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 418s 2s/step - loss: 0.0794 - mse: 3.0288e-04 - mae: 0.0092 - val_loss: 0.0733 - val_mse: 2.2383e-04 - val_mae: 0.0090\n",
      "Epoch 105/5000\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0739 - mse: 2.8763e-04 - mae: 0.0086\n",
      "Epoch 00105: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 412s 2s/step - loss: 0.0735 - mse: 2.8638e-04 - mae: 0.0085 - val_loss: 0.1024 - val_mse: 4.9259e-04 - val_mae: 0.0125\n",
      "Epoch 106/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0610 - mse: 2.1204e-04 - mae: 0.0074\n",
      "Epoch 00106: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 437s 2s/step - loss: 0.0616 - mse: 2.1432e-04 - mae: 0.0075 - val_loss: 0.0850 - val_mse: 2.5741e-04 - val_mae: 0.0094\n",
      "Epoch 107/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0662 - mse: 2.5459e-04 - mae: 0.0080\n",
      "Epoch 00107: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 443s 2s/step - loss: 0.0664 - mse: 2.5481e-04 - mae: 0.0081 - val_loss: 0.0299 - val_mse: 7.2691e-05 - val_mae: 0.0053\n",
      "Epoch 108/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0676 - mse: 2.3238e-04 - mae: 0.0081\n",
      "Epoch 00108: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 421s 2s/step - loss: 0.0672 - mse: 2.3110e-04 - mae: 0.0080 - val_loss: 0.0427 - val_mse: 1.3277e-04 - val_mae: 0.0063\n",
      "Epoch 109/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0622 - mse: 2.3731e-04 - mae: 0.0076\n",
      "Epoch 00109: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 430s 2s/step - loss: 0.0630 - mse: 2.4180e-04 - mae: 0.0077 - val_loss: 0.0115 - val_mse: 1.1974e-05 - val_mae: 0.0024\n",
      "Epoch 110/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0680 - mse: 2.6672e-04 - mae: 0.0081\n",
      "Epoch 00110: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 424s 2s/step - loss: 0.0677 - mse: 2.6529e-04 - mae: 0.0081 - val_loss: 0.0656 - val_mse: 1.3281e-04 - val_mae: 0.0067\n",
      "Epoch 111/5000\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0553 - mse: 1.9367e-04 - mae: 0.0068\n",
      "Epoch 00111: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 412s 2s/step - loss: 0.0552 - mse: 1.9290e-04 - mae: 0.0068 - val_loss: 0.1565 - val_mse: 8.6320e-04 - val_mae: 0.0164\n",
      "Epoch 112/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0590 - mse: 1.9715e-04 - mae: 0.0071\n",
      "Epoch 00112: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 421s 2s/step - loss: 0.0588 - mse: 1.9613e-04 - mae: 0.0071 - val_loss: 0.0040 - val_mse: 3.0312e-06 - val_mae: 0.0012\n",
      "Epoch 113/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0612 - mse: 1.9386e-04 - mae: 0.0075\n",
      "Epoch 00113: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 417s 2s/step - loss: 0.0609 - mse: 1.9290e-04 - mae: 0.0074 - val_loss: 0.0341 - val_mse: 8.0904e-05 - val_mae: 0.0060\n",
      "Epoch 114/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0664 - mse: 2.4227e-04 - mae: 0.0079\n",
      "Epoch 00114: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 421s 2s/step - loss: 0.0661 - mse: 2.4124e-04 - mae: 0.0079 - val_loss: 0.0539 - val_mse: 1.6320e-04 - val_mae: 0.0066\n",
      "Epoch 115/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0668 - mse: 2.3583e-04 - mae: 0.0080\n",
      "Epoch 00115: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 420s 2s/step - loss: 0.0672 - mse: 2.3820e-04 - mae: 0.0080 - val_loss: 0.0273 - val_mse: 6.4914e-05 - val_mae: 0.0044\n",
      "Epoch 116/5000\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0618 - mse: 2.0896e-04 - mae: 0.0074\n",
      "Epoch 00116: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 413s 2s/step - loss: 0.0615 - mse: 2.0801e-04 - mae: 0.0074 - val_loss: 0.0195 - val_mse: 3.8792e-05 - val_mae: 0.0042\n",
      "Epoch 117/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0669 - mse: 2.5739e-04 - mae: 0.0079\n",
      "Epoch 00117: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 433s 2s/step - loss: 0.0666 - mse: 2.5601e-04 - mae: 0.0079 - val_loss: 0.0398 - val_mse: 7.8010e-05 - val_mae: 0.0050\n",
      "Epoch 118/5000\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0588 - mse: 2.1348e-04 - mae: 0.0072\n",
      "Epoch 00118: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 412s 2s/step - loss: 0.0586 - mse: 2.1240e-04 - mae: 0.0072 - val_loss: 0.0235 - val_mse: 5.0205e-05 - val_mae: 0.0052\n",
      "Epoch 119/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0698 - mse: 2.9651e-04 - mae: 0.0083\n",
      "Epoch 00119: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 425s 2s/step - loss: 0.0699 - mse: 2.9625e-04 - mae: 0.0083 - val_loss: 0.0432 - val_mse: 6.8925e-05 - val_mae: 0.0055\n",
      "Epoch 120/5000\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0633 - mse: 2.1883e-04 - mae: 0.0074\n",
      "Epoch 00120: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 408s 2s/step - loss: 0.0637 - mse: 2.1955e-04 - mae: 0.0075 - val_loss: 0.1092 - val_mse: 5.9331e-04 - val_mae: 0.0120\n",
      "Epoch 121/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0678 - mse: 2.5678e-04 - mae: 0.0082\n",
      "Epoch 00121: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 438s 2s/step - loss: 0.0674 - mse: 2.5547e-04 - mae: 0.0082 - val_loss: 0.0319 - val_mse: 6.7914e-05 - val_mae: 0.0038\n",
      "Epoch 122/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0688 - mse: 2.6481e-04 - mae: 0.0082\n",
      "Epoch 00122: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 425s 2s/step - loss: 0.0686 - mse: 2.6365e-04 - mae: 0.0082 - val_loss: 0.0550 - val_mse: 1.0279e-04 - val_mae: 0.0058\n",
      "Epoch 123/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0689 - mse: 2.8304e-04 - mae: 0.0083\n",
      "Epoch 00123: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 431s 2s/step - loss: 0.0685 - mse: 2.8177e-04 - mae: 0.0083 - val_loss: 0.0397 - val_mse: 7.5028e-05 - val_mae: 0.0053\n",
      "Epoch 124/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0647 - mse: 2.4577e-04 - mae: 0.0080\n",
      "Epoch 00124: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 432s 2s/step - loss: 0.0644 - mse: 2.4475e-04 - mae: 0.0079 - val_loss: 0.0291 - val_mse: 7.6413e-05 - val_mae: 0.0052\n",
      "Epoch 125/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0562 - mse: 1.7688e-04 - mae: 0.0071\n",
      "Epoch 00125: mse improved from 0.00018 to 0.00018, saving model to weights/Homogeneous_Poisson_NN/Fourier_24x24modes_big.h5\n",
      "200/200 [==============================] - 414s 2s/step - loss: 0.0561 - mse: 1.7629e-04 - mae: 0.0071 - val_loss: 0.0073 - val_mse: 8.4095e-06 - val_mae: 0.0018\n",
      "Epoch 126/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0655 - mse: 2.4299e-04 - mae: 0.0080\n",
      "Epoch 00126: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 442s 2s/step - loss: 0.0653 - mse: 2.4196e-04 - mae: 0.0080 - val_loss: 0.0223 - val_mse: 7.1639e-05 - val_mae: 0.0042\n",
      "Epoch 127/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0674 - mse: 2.5777e-04 - mae: 0.0081\n",
      "Epoch 00127: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 422s 2s/step - loss: 0.0672 - mse: 2.5667e-04 - mae: 0.0081 - val_loss: 0.1405 - val_mse: 6.1433e-04 - val_mae: 0.0156\n",
      "Epoch 128/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0654 - mse: 2.3278e-04 - mae: 0.0078\n",
      "Epoch 00128: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 426s 2s/step - loss: 0.0656 - mse: 2.3318e-04 - mae: 0.0078 - val_loss: 0.0520 - val_mse: 1.0105e-04 - val_mae: 0.0055\n",
      "Epoch 129/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0646 - mse: 2.5587e-04 - mae: 0.0080\n",
      "Epoch 00129: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 435s 2s/step - loss: 0.0653 - mse: 2.6041e-04 - mae: 0.0081 - val_loss: 0.0492 - val_mse: 1.7143e-04 - val_mae: 0.0074\n",
      "Epoch 130/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0632 - mse: 2.0486e-04 - mae: 0.0076\n",
      "Epoch 00130: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 419s 2s/step - loss: 0.0629 - mse: 2.0373e-04 - mae: 0.0075 - val_loss: 0.0202 - val_mse: 3.4440e-05 - val_mae: 0.0028\n",
      "Epoch 131/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0627 - mse: 2.1858e-04 - mae: 0.0076\n",
      "Epoch 00131: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 426s 2s/step - loss: 0.0624 - mse: 2.1750e-04 - mae: 0.0076 - val_loss: 0.1033 - val_mse: 4.5220e-04 - val_mae: 0.0127\n",
      "Epoch 132/5000\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0609 - mse: 2.0301e-04 - mae: 0.0073\n",
      "Epoch 00132: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 411s 2s/step - loss: 0.0610 - mse: 2.0274e-04 - mae: 0.0074 - val_loss: 0.1452 - val_mse: 5.9095e-04 - val_mae: 0.0168\n",
      "Epoch 133/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0592 - mse: 2.2781e-04 - mae: 0.0074\n",
      "Epoch 00133: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 415s 2s/step - loss: 0.0590 - mse: 2.2705e-04 - mae: 0.0074 - val_loss: 0.0289 - val_mse: 5.2788e-05 - val_mae: 0.0038\n",
      "Epoch 134/5000\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0619 - mse: 2.0861e-04 - mae: 0.0074\n",
      "Epoch 00134: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 413s 2s/step - loss: 0.0630 - mse: 2.1574e-04 - mae: 0.0075 - val_loss: 0.1614 - val_mse: 6.9648e-04 - val_mae: 0.0175\n",
      "Epoch 135/5000\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0673 - mse: 2.7029e-04 - mae: 0.0081\n",
      "Epoch 00135: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 410s 2s/step - loss: 0.0672 - mse: 2.6939e-04 - mae: 0.0081 - val_loss: 0.0351 - val_mse: 8.8691e-05 - val_mae: 0.0053\n",
      "Epoch 136/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0576 - mse: 1.9819e-04 - mae: 0.0072\n",
      "Epoch 00136: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 419s 2s/step - loss: 0.0574 - mse: 1.9732e-04 - mae: 0.0071 - val_loss: 0.0967 - val_mse: 2.6515e-04 - val_mae: 0.0111\n",
      "Epoch 137/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0687 - mse: 2.4109e-04 - mae: 0.0082\n",
      "Epoch 00137: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 426s 2s/step - loss: 0.0685 - mse: 2.4008e-04 - mae: 0.0082 - val_loss: 0.0129 - val_mse: 1.5947e-05 - val_mae: 0.0024\n",
      "Epoch 138/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0660 - mse: 2.4173e-04 - mae: 0.0079\n",
      "Epoch 00138: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 411s 2s/step - loss: 0.0658 - mse: 2.4068e-04 - mae: 0.0078 - val_loss: 0.0106 - val_mse: 1.5142e-05 - val_mae: 0.0025\n",
      "Epoch 139/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0699 - mse: 2.6572e-04 - mae: 0.0083\n",
      "Epoch 00139: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 443s 2s/step - loss: 0.0696 - mse: 2.6422e-04 - mae: 0.0083 - val_loss: 0.0492 - val_mse: 8.7825e-05 - val_mae: 0.0052\n",
      "Epoch 140/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0646 - mse: 2.3355e-04 - mae: 0.0079\n",
      "Epoch 00140: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 436s 2s/step - loss: 0.0653 - mse: 2.3606e-04 - mae: 0.0080 - val_loss: 0.0345 - val_mse: 1.0655e-04 - val_mae: 0.0052\n",
      "Epoch 141/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0630 - mse: 2.3182e-04 - mae: 0.0076\n",
      "Epoch 00141: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 426s 2s/step - loss: 0.0628 - mse: 2.3083e-04 - mae: 0.0076 - val_loss: 0.0197 - val_mse: 2.4039e-05 - val_mae: 0.0034\n",
      "Epoch 142/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0691 - mse: 2.7584e-04 - mae: 0.0081\n",
      "Epoch 00142: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 420s 2s/step - loss: 0.0691 - mse: 2.7502e-04 - mae: 0.0081 - val_loss: 0.0364 - val_mse: 7.3479e-05 - val_mae: 0.0047\n",
      "Epoch 143/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0649 - mse: 2.5095e-04 - mae: 0.0078\n",
      "Epoch 00143: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 427s 2s/step - loss: 0.0650 - mse: 2.5072e-04 - mae: 0.0078 - val_loss: 0.0520 - val_mse: 1.1819e-04 - val_mae: 0.0070\n",
      "Epoch 144/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0620 - mse: 2.3965e-04 - mae: 0.0076\n",
      "Epoch 00144: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 429s 2s/step - loss: 0.0620 - mse: 2.3949e-04 - mae: 0.0076 - val_loss: 0.0400 - val_mse: 1.1441e-04 - val_mae: 0.0046\n",
      "Epoch 145/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0639 - mse: 2.4159e-04 - mae: 0.0077\n",
      "Epoch 00145: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 436s 2s/step - loss: 0.0637 - mse: 2.4058e-04 - mae: 0.0077 - val_loss: 0.0752 - val_mse: 3.0611e-04 - val_mae: 0.0090\n",
      "Epoch 146/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0661 - mse: 2.4120e-04 - mae: 0.0081\n",
      "Epoch 00146: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 438s 2s/step - loss: 0.0663 - mse: 2.4198e-04 - mae: 0.0082 - val_loss: 0.0461 - val_mse: 1.0469e-04 - val_mae: 0.0062\n",
      "Epoch 147/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0697 - mse: 2.5635e-04 - mae: 0.0081\n",
      "Epoch 00147: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 415s 2s/step - loss: 0.0704 - mse: 2.6178e-04 - mae: 0.0082 - val_loss: 2.7416e-04 - val_mse: 5.0013e-08 - val_mae: 1.5229e-04\n",
      "Epoch 148/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0680 - mse: 2.6073e-04 - mae: 0.0084\n",
      "Epoch 00148: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 437s 2s/step - loss: 0.0682 - mse: 2.6114e-04 - mae: 0.0085 - val_loss: 0.0288 - val_mse: 6.9840e-05 - val_mae: 0.0039\n",
      "Epoch 149/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0664 - mse: 2.5554e-04 - mae: 0.0078\n",
      "Epoch 00149: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 439s 2s/step - loss: 0.0661 - mse: 2.5435e-04 - mae: 0.0078 - val_loss: 0.1946 - val_mse: 7.9445e-04 - val_mae: 0.0185\n",
      "Epoch 150/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0637 - mse: 2.6304e-04 - mae: 0.0077\n",
      "Epoch 00150: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 430s 2s/step - loss: 0.0634 - mse: 2.6184e-04 - mae: 0.0077 - val_loss: 0.0261 - val_mse: 6.5824e-05 - val_mae: 0.0044\n",
      "Epoch 151/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0629 - mse: 2.3748e-04 - mae: 0.0074\n",
      "Epoch 00151: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 422s 2s/step - loss: 0.0628 - mse: 2.3654e-04 - mae: 0.0074 - val_loss: 0.1064 - val_mse: 4.7183e-04 - val_mae: 0.0131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 152/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0616 - mse: 2.2638e-04 - mae: 0.0075\n",
      "Epoch 00152: mse did not improve from 0.00018\n",
      "200/200 [==============================] - 426s 2s/step - loss: 0.0613 - mse: 2.2540e-04 - mae: 0.0075 - val_loss: 0.0076 - val_mse: 7.4171e-06 - val_mae: 0.0015\n",
      "Epoch 153/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0507 - mse: 1.7206e-04 - mae: 0.0066\n",
      "Epoch 00153: mse improved from 0.00018 to 0.00017, saving model to weights/Homogeneous_Poisson_NN/Fourier_24x24modes_big.h5\n",
      "200/200 [==============================] - 424s 2s/step - loss: 0.0513 - mse: 1.7438e-04 - mae: 0.0067 - val_loss: 0.1169 - val_mse: 3.5446e-04 - val_mae: 0.0141\n",
      "Epoch 154/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0624 - mse: 2.2265e-04 - mae: 0.0077\n",
      "Epoch 00154: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 416s 2s/step - loss: 0.0623 - mse: 2.2198e-04 - mae: 0.0076 - val_loss: 0.0503 - val_mse: 1.4160e-04 - val_mae: 0.0071\n",
      "Epoch 155/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0708 - mse: 2.5903e-04 - mae: 0.0083\n",
      "Epoch 00155: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 428s 2s/step - loss: 0.0708 - mse: 2.5903e-04 - mae: 0.0083 - val_loss: 0.0054 - val_mse: 5.1017e-06 - val_mae: 0.0014\n",
      "Epoch 156/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0618 - mse: 2.3251e-04 - mae: 0.0077\n",
      "Epoch 00156: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 436s 2s/step - loss: 0.0615 - mse: 2.3149e-04 - mae: 0.0076 - val_loss: 0.1343 - val_mse: 5.4234e-04 - val_mae: 0.0150\n",
      "Epoch 157/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0596 - mse: 2.1447e-04 - mae: 0.0074\n",
      "Epoch 00157: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 419s 2s/step - loss: 0.0594 - mse: 2.1353e-04 - mae: 0.0073 - val_loss: 0.2534 - val_mse: 0.0012 - val_mae: 0.0262\n",
      "Epoch 158/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0693 - mse: 2.6451e-04 - mae: 0.0084\n",
      "Epoch 00158: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 423s 2s/step - loss: 0.0690 - mse: 2.6302e-04 - mae: 0.0084 - val_loss: 0.1073 - val_mse: 4.3879e-04 - val_mae: 0.0123\n",
      "Epoch 159/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0779 - mse: 2.9413e-04 - mae: 0.0091\n",
      "Epoch 00159: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 419s 2s/step - loss: 0.0776 - mse: 2.9288e-04 - mae: 0.0090 - val_loss: 0.0575 - val_mse: 1.6923e-04 - val_mae: 0.0061\n",
      "Epoch 160/5000\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0676 - mse: 2.6187e-04 - mae: 0.0081\n",
      "Epoch 00160: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 406s 2s/step - loss: 0.0673 - mse: 2.6051e-04 - mae: 0.0080 - val_loss: 0.0093 - val_mse: 1.2085e-05 - val_mae: 0.0024\n",
      "Epoch 161/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0591 - mse: 2.0994e-04 - mae: 0.0074\n",
      "Epoch 00161: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 420s 2s/step - loss: 0.0588 - mse: 2.0887e-04 - mae: 0.0073 - val_loss: 0.0937 - val_mse: 3.5163e-04 - val_mae: 0.0117\n",
      "Epoch 162/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0578 - mse: 2.0126e-04 - mae: 0.0072\n",
      "Epoch 00162: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 441s 2s/step - loss: 0.0576 - mse: 2.0027e-04 - mae: 0.0072 - val_loss: 0.0541 - val_mse: 1.8392e-04 - val_mae: 0.0069\n",
      "Epoch 163/5000\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0483 - mse: 1.6836e-04 - mae: 0.0065\n",
      "Epoch 00163: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 409s 2s/step - loss: 0.0496 - mse: 1.7622e-04 - mae: 0.0066 - val_loss: 0.0885 - val_mse: 3.7298e-04 - val_mae: 0.0118\n",
      "Epoch 164/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0648 - mse: 2.3418e-04 - mae: 0.0078\n",
      "Epoch 00164: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 413s 2s/step - loss: 0.0658 - mse: 2.3976e-04 - mae: 0.0079 - val_loss: 0.0560 - val_mse: 1.3956e-04 - val_mae: 0.0053\n",
      "Epoch 165/5000\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0698 - mse: 2.6441e-04 - mae: 0.0083\n",
      "Epoch 00165: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 409s 2s/step - loss: 0.0694 - mse: 2.6321e-04 - mae: 0.0082 - val_loss: 0.0525 - val_mse: 1.3807e-04 - val_mae: 0.0071\n",
      "Epoch 166/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0618 - mse: 2.2474e-04 - mae: 0.0075\n",
      "Epoch 00166: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 425s 2s/step - loss: 0.0617 - mse: 2.2398e-04 - mae: 0.0075 - val_loss: 0.0270 - val_mse: 4.8111e-05 - val_mae: 0.0042\n",
      "Epoch 167/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0610 - mse: 2.1725e-04 - mae: 0.0076\n",
      "Epoch 00167: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 422s 2s/step - loss: 0.0610 - mse: 2.1677e-04 - mae: 0.0076 - val_loss: 0.0812 - val_mse: 2.8167e-04 - val_mae: 0.0107\n",
      "Epoch 168/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0595 - mse: 2.1657e-04 - mae: 0.0073\n",
      "Epoch 00168: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 429s 2s/step - loss: 0.0600 - mse: 2.1899e-04 - mae: 0.0074 - val_loss: 0.1403 - val_mse: 5.6208e-04 - val_mae: 0.0163\n",
      "Epoch 169/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0641 - mse: 2.3762e-04 - mae: 0.0076\n",
      "Epoch 00169: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 430s 2s/step - loss: 0.0638 - mse: 2.3652e-04 - mae: 0.0076 - val_loss: 0.2115 - val_mse: 0.0012 - val_mae: 0.0227\n",
      "Epoch 170/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0545 - mse: 1.7963e-04 - mae: 0.0069\n",
      "Epoch 00170: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 412s 2s/step - loss: 0.0542 - mse: 1.7864e-04 - mae: 0.0068 - val_loss: 0.0374 - val_mse: 6.5349e-05 - val_mae: 0.0046\n",
      "Epoch 171/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0622 - mse: 2.1739e-04 - mae: 0.0076\n",
      "Epoch 00171: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 422s 2s/step - loss: 0.0620 - mse: 2.1656e-04 - mae: 0.0076 - val_loss: 0.0022 - val_mse: 1.1600e-06 - val_mae: 5.9640e-04\n",
      "Epoch 172/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0645 - mse: 2.2739e-04 - mae: 0.0079\n",
      "Epoch 00172: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 419s 2s/step - loss: 0.0653 - mse: 2.3096e-04 - mae: 0.0080 - val_loss: 0.0121 - val_mse: 2.1530e-05 - val_mae: 0.0024\n",
      "Epoch 173/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0651 - mse: 2.5063e-04 - mae: 0.0078\n",
      "Epoch 00173: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 438s 2s/step - loss: 0.0648 - mse: 2.4942e-04 - mae: 0.0078 - val_loss: 0.1547 - val_mse: 6.7437e-04 - val_mae: 0.0159\n",
      "Epoch 174/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0611 - mse: 2.2878e-04 - mae: 0.0073\n",
      "Epoch 00174: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 423s 2s/step - loss: 0.0612 - mse: 2.2829e-04 - mae: 0.0073 - val_loss: 0.0214 - val_mse: 2.7826e-05 - val_mae: 0.0031\n",
      "Epoch 175/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0561 - mse: 2.0532e-04 - mae: 0.0070\n",
      "Epoch 00175: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 425s 2s/step - loss: 0.0564 - mse: 2.0624e-04 - mae: 0.0070 - val_loss: 0.0276 - val_mse: 6.6043e-05 - val_mae: 0.0048\n",
      "Epoch 176/5000\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0623 - mse: 2.2200e-04 - mae: 0.0075\n",
      "Epoch 00176: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 410s 2s/step - loss: 0.0621 - mse: 2.2106e-04 - mae: 0.0075 - val_loss: 0.0535 - val_mse: 2.2379e-04 - val_mae: 0.0088\n",
      "Epoch 177/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0655 - mse: 2.3084e-04 - mae: 0.0078\n",
      "Epoch 00177: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 423s 2s/step - loss: 0.0653 - mse: 2.2995e-04 - mae: 0.0078 - val_loss: 0.0977 - val_mse: 3.8229e-04 - val_mae: 0.0112\n",
      "Epoch 178/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0739 - mse: 2.7381e-04 - mae: 0.0087\n",
      "Epoch 00178: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 435s 2s/step - loss: 0.0735 - mse: 2.7239e-04 - mae: 0.0086 - val_loss: 0.0591 - val_mse: 1.6357e-04 - val_mae: 0.0075\n",
      "Epoch 179/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0730 - mse: 2.8553e-04 - mae: 0.0085\n",
      "Epoch 00179: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 422s 2s/step - loss: 0.0727 - mse: 2.8409e-04 - mae: 0.0085 - val_loss: 0.0800 - val_mse: 2.9125e-04 - val_mae: 0.0111\n",
      "Epoch 180/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0755 - mse: 2.7938e-04 - mae: 0.0088\n",
      "Epoch 00180: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 428s 2s/step - loss: 0.0756 - mse: 2.7917e-04 - mae: 0.0088 - val_loss: 0.0664 - val_mse: 2.0839e-04 - val_mae: 0.0081\n",
      "Epoch 181/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0639 - mse: 2.2959e-04 - mae: 0.0078\n",
      "Epoch 00181: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 431s 2s/step - loss: 0.0637 - mse: 2.2864e-04 - mae: 0.0077 - val_loss: 0.0467 - val_mse: 1.1508e-04 - val_mae: 0.0060\n",
      "Epoch 182/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0588 - mse: 1.9346e-04 - mae: 0.0071\n",
      "Epoch 00182: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 417s 2s/step - loss: 0.0585 - mse: 1.9261e-04 - mae: 0.0071 - val_loss: 0.0583 - val_mse: 2.1096e-04 - val_mae: 0.0072\n",
      "Epoch 183/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0637 - mse: 2.2948e-04 - mae: 0.0076\n",
      "Epoch 00183: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 420s 2s/step - loss: 0.0635 - mse: 2.2861e-04 - mae: 0.0076 - val_loss: 0.0812 - val_mse: 2.0562e-04 - val_mae: 0.0097\n",
      "Epoch 184/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0550 - mse: 1.9708e-04 - mae: 0.0066\n",
      "Epoch 00184: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 421s 2s/step - loss: 0.0547 - mse: 1.9606e-04 - mae: 0.0066 - val_loss: 0.0489 - val_mse: 1.7391e-04 - val_mae: 0.0066\n",
      "Epoch 185/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0678 - mse: 2.3569e-04 - mae: 0.0083\n",
      "Epoch 00185: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 432s 2s/step - loss: 0.0681 - mse: 2.3592e-04 - mae: 0.0083 - val_loss: 0.0305 - val_mse: 6.0283e-05 - val_mae: 0.0035\n",
      "Epoch 186/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0618 - mse: 2.3392e-04 - mae: 0.0076\n",
      "Epoch 00186: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 426s 2s/step - loss: 0.0615 - mse: 2.3284e-04 - mae: 0.0076 - val_loss: 0.0379 - val_mse: 8.0750e-05 - val_mae: 0.0062\n",
      "Epoch 187/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0679 - mse: 2.5593e-04 - mae: 0.0082\n",
      "Epoch 00187: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 417s 2s/step - loss: 0.0677 - mse: 2.5492e-04 - mae: 0.0081 - val_loss: 0.1221 - val_mse: 4.1833e-04 - val_mae: 0.0132\n",
      "Epoch 188/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0662 - mse: 2.5993e-04 - mae: 0.0079\n",
      "Epoch 00188: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 432s 2s/step - loss: 0.0661 - mse: 2.5920e-04 - mae: 0.0079 - val_loss: 0.0363 - val_mse: 6.7003e-05 - val_mae: 0.0055\n",
      "Epoch 189/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0645 - mse: 2.3582e-04 - mae: 0.0078\n",
      "Epoch 00189: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 426s 2s/step - loss: 0.0646 - mse: 2.3523e-04 - mae: 0.0078 - val_loss: 0.0271 - val_mse: 4.7165e-05 - val_mae: 0.0042\n",
      "Epoch 190/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0507 - mse: 1.6809e-04 - mae: 0.0065\n",
      "Epoch 00190: mse improved from 0.00017 to 0.00017, saving model to weights/Homogeneous_Poisson_NN/Fourier_24x24modes_big.h5\n",
      "200/200 [==============================] - 415s 2s/step - loss: 0.0505 - mse: 1.6743e-04 - mae: 0.0065 - val_loss: 0.0474 - val_mse: 1.3567e-04 - val_mae: 0.0066\n",
      "Epoch 191/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0651 - mse: 2.3769e-04 - mae: 0.0079\n",
      "Epoch 00191: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 431s 2s/step - loss: 0.0662 - mse: 2.4284e-04 - mae: 0.0080 - val_loss: 0.0948 - val_mse: 3.7720e-04 - val_mae: 0.0106\n",
      "Epoch 192/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0626 - mse: 2.2335e-04 - mae: 0.0078\n",
      "Epoch 00192: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 441s 2s/step - loss: 0.0629 - mse: 2.2429e-04 - mae: 0.0078 - val_loss: 0.0836 - val_mse: 4.8086e-04 - val_mae: 0.0097\n",
      "Epoch 193/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0626 - mse: 2.4300e-04 - mae: 0.0077\n",
      "Epoch 00193: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 433s 2s/step - loss: 0.0624 - mse: 2.4196e-04 - mae: 0.0076 - val_loss: 0.0099 - val_mse: 1.1275e-05 - val_mae: 0.0019\n",
      "Epoch 194/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0630 - mse: 2.1170e-04 - mae: 0.0076\n",
      "Epoch 00194: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 415s 2s/step - loss: 0.0631 - mse: 2.1147e-04 - mae: 0.0076 - val_loss: 0.0244 - val_mse: 4.2237e-05 - val_mae: 0.0042\n",
      "Epoch 195/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0675 - mse: 2.6886e-04 - mae: 0.0082\n",
      "Epoch 00195: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 433s 2s/step - loss: 0.0675 - mse: 2.6805e-04 - mae: 0.0082 - val_loss: 0.0170 - val_mse: 2.7216e-05 - val_mae: 0.0027\n",
      "Epoch 196/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0545 - mse: 1.9622e-04 - mae: 0.0068\n",
      "Epoch 00196: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 440s 2s/step - loss: 0.0557 - mse: 2.0183e-04 - mae: 0.0069 - val_loss: 0.0235 - val_mse: 5.6605e-05 - val_mae: 0.0040\n",
      "Epoch 197/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0633 - mse: 2.4034e-04 - mae: 0.0076\n",
      "Epoch 00197: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 426s 2s/step - loss: 0.0630 - mse: 2.3907e-04 - mae: 0.0076 - val_loss: 0.1003 - val_mse: 2.8672e-04 - val_mae: 0.0114\n",
      "Epoch 198/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0716 - mse: 2.7866e-04 - mae: 0.0084\n",
      "Epoch 00198: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 427s 2s/step - loss: 0.0717 - mse: 2.7806e-04 - mae: 0.0084 - val_loss: 0.0419 - val_mse: 1.1717e-04 - val_mae: 0.0074\n",
      "Epoch 199/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0553 - mse: 1.8375e-04 - mae: 0.0068\n",
      "Epoch 00199: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 417s 2s/step - loss: 0.0551 - mse: 1.8280e-04 - mae: 0.0067 - val_loss: 0.0808 - val_mse: 3.4728e-04 - val_mae: 0.0112\n",
      "Epoch 200/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0647 - mse: 2.1907e-04 - mae: 0.0080\n",
      "Epoch 00200: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 434s 2s/step - loss: 0.0644 - mse: 2.1789e-04 - mae: 0.0079 - val_loss: 0.0910 - val_mse: 2.0532e-04 - val_mae: 0.0089\n",
      "Epoch 201/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0554 - mse: 1.8043e-04 - mae: 0.0068\n",
      "Epoch 00201: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 423s 2s/step - loss: 0.0558 - mse: 1.8108e-04 - mae: 0.0068 - val_loss: 0.0477 - val_mse: 9.3825e-05 - val_mae: 0.0051\n",
      "Epoch 202/5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199/200 [============================>.] - ETA: 2s - loss: 0.0559 - mse: 1.8992e-04 - mae: 0.0070\n",
      "Epoch 00202: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 418s 2s/step - loss: 0.0559 - mse: 1.8976e-04 - mae: 0.0070 - val_loss: 0.0579 - val_mse: 1.5886e-04 - val_mae: 0.0083\n",
      "Epoch 203/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0602 - mse: 2.0519e-04 - mae: 0.0074\n",
      "Epoch 00203: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 419s 2s/step - loss: 0.0605 - mse: 2.0756e-04 - mae: 0.0074 - val_loss: 0.0458 - val_mse: 9.2887e-05 - val_mae: 0.0057\n",
      "Epoch 204/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0652 - mse: 2.5004e-04 - mae: 0.0081\n",
      "Epoch 00204: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 436s 2s/step - loss: 0.0653 - mse: 2.4987e-04 - mae: 0.0081 - val_loss: 0.0578 - val_mse: 1.0454e-04 - val_mae: 0.0059\n",
      "Epoch 205/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0612 - mse: 2.2922e-04 - mae: 0.0072\n",
      "Epoch 00205: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 421s 2s/step - loss: 0.0621 - mse: 2.3356e-04 - mae: 0.0073 - val_loss: 0.0412 - val_mse: 1.3632e-04 - val_mae: 0.0056\n",
      "Epoch 206/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0647 - mse: 2.5222e-04 - mae: 0.0078\n",
      "Epoch 00206: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 437s 2s/step - loss: 0.0644 - mse: 2.5085e-04 - mae: 0.0078 - val_loss: 0.0655 - val_mse: 2.4403e-04 - val_mae: 0.0092\n",
      "Epoch 207/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0662 - mse: 2.6451e-04 - mae: 0.0080\n",
      "Epoch 00207: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 424s 2s/step - loss: 0.0664 - mse: 2.6572e-04 - mae: 0.0081 - val_loss: 0.0059 - val_mse: 4.7018e-06 - val_mae: 0.0013\n",
      "Epoch 208/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0633 - mse: 2.2782e-04 - mae: 0.0076\n",
      "Epoch 00208: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 431s 2s/step - loss: 0.0635 - mse: 2.2796e-04 - mae: 0.0076 - val_loss: 0.1976 - val_mse: 6.9870e-04 - val_mae: 0.0195\n",
      "Epoch 209/5000\n",
      "199/200 [============================>.] - ETA: 1s - loss: 0.0589 - mse: 2.2680e-04 - mae: 0.0071\n",
      "Epoch 00209: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 412s 2s/step - loss: 0.0586 - mse: 2.2564e-04 - mae: 0.0071 - val_loss: 0.0217 - val_mse: 3.6051e-05 - val_mae: 0.0038\n",
      "Epoch 210/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0685 - mse: 2.4361e-04 - mae: 0.0081\n",
      "Epoch 00210: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 420s 2s/step - loss: 0.0699 - mse: 2.5107e-04 - mae: 0.0082 - val_loss: 0.0185 - val_mse: 3.7617e-05 - val_mae: 0.0036\n",
      "Epoch 211/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0593 - mse: 2.3184e-04 - mae: 0.0071\n",
      "Epoch 00211: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 415s 2s/step - loss: 0.0593 - mse: 2.3109e-04 - mae: 0.0071 - val_loss: 0.0682 - val_mse: 1.2898e-04 - val_mae: 0.0072\n",
      "Epoch 212/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0720 - mse: 2.8752e-04 - mae: 0.0087\n",
      "Epoch 00212: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 439s 2s/step - loss: 0.0720 - mse: 2.8670e-04 - mae: 0.0087 - val_loss: 0.0062 - val_mse: 4.9281e-06 - val_mae: 0.0014\n",
      "Epoch 213/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0655 - mse: 2.6073e-04 - mae: 0.0078\n",
      "Epoch 00213: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 418s 2s/step - loss: 0.0655 - mse: 2.5985e-04 - mae: 0.0078 - val_loss: 0.1265 - val_mse: 5.3195e-04 - val_mae: 0.0169\n",
      "Epoch 214/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0658 - mse: 2.6046e-04 - mae: 0.0080\n",
      "Epoch 00214: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 435s 2s/step - loss: 0.0656 - mse: 2.5942e-04 - mae: 0.0079 - val_loss: 0.0559 - val_mse: 1.2554e-04 - val_mae: 0.0070\n",
      "Epoch 215/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0687 - mse: 2.7174e-04 - mae: 0.0081\n",
      "Epoch 00215: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 429s 2s/step - loss: 0.0686 - mse: 2.7051e-04 - mae: 0.0081 - val_loss: 0.0915 - val_mse: 3.0435e-04 - val_mae: 0.0111\n",
      "Epoch 216/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0546 - mse: 1.9410e-04 - mae: 0.0069\n",
      "Epoch 00216: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 416s 2s/step - loss: 0.0544 - mse: 1.9324e-04 - mae: 0.0069 - val_loss: 0.0101 - val_mse: 1.3167e-05 - val_mae: 0.0016\n",
      "Epoch 217/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0493 - mse: 1.5995e-04 - mae: 0.0065\n",
      "Epoch 00217: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 435s 2s/step - loss: 0.0508 - mse: 1.6934e-04 - mae: 0.0066 - val_loss: 0.1546 - val_mse: 0.0011 - val_mae: 0.0189\n",
      "Epoch 218/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0608 - mse: 2.0135e-04 - mae: 0.0074\n",
      "Epoch 00218: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 415s 2s/step - loss: 0.0613 - mse: 2.0316e-04 - mae: 0.0075 - val_loss: 0.2681 - val_mse: 0.0014 - val_mae: 0.0248\n",
      "Epoch 219/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0661 - mse: 2.5100e-04 - mae: 0.0080\n",
      "Epoch 00219: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 421s 2s/step - loss: 0.0658 - mse: 2.4979e-04 - mae: 0.0080 - val_loss: 0.0828 - val_mse: 3.4969e-04 - val_mae: 0.0101\n",
      "Epoch 220/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0638 - mse: 2.2230e-04 - mae: 0.0077\n",
      "Epoch 00220: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 424s 2s/step - loss: 0.0638 - mse: 2.2190e-04 - mae: 0.0077 - val_loss: 0.0937 - val_mse: 3.9571e-04 - val_mae: 0.0107\n",
      "Epoch 221/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0563 - mse: 1.9189e-04 - mae: 0.0073\n",
      "Epoch 00221: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 438s 2s/step - loss: 0.0560 - mse: 1.9082e-04 - mae: 0.0072 - val_loss: 0.0515 - val_mse: 1.3401e-04 - val_mae: 0.0083\n",
      "Epoch 222/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0604 - mse: 2.1960e-04 - mae: 0.0074\n",
      "Epoch 00222: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 427s 2s/step - loss: 0.0604 - mse: 2.1899e-04 - mae: 0.0074 - val_loss: 0.0435 - val_mse: 1.0915e-04 - val_mae: 0.0044\n",
      "Epoch 223/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0545 - mse: 1.8030e-04 - mae: 0.0070\n",
      "Epoch 00223: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 435s 2s/step - loss: 0.0543 - mse: 1.7947e-04 - mae: 0.0070 - val_loss: 0.0158 - val_mse: 4.7168e-05 - val_mae: 0.0032\n",
      "Epoch 224/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0639 - mse: 2.4563e-04 - mae: 0.0078\n",
      "Epoch 00224: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 421s 2s/step - loss: 0.0644 - mse: 2.4666e-04 - mae: 0.0078 - val_loss: 0.0729 - val_mse: 2.5018e-04 - val_mae: 0.0101\n",
      "Epoch 225/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0735 - mse: 3.0212e-04 - mae: 0.0087\n",
      "Epoch 00225: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 434s 2s/step - loss: 0.0740 - mse: 3.0511e-04 - mae: 0.0087 - val_loss: 0.1202 - val_mse: 2.9260e-04 - val_mae: 0.0117\n",
      "Epoch 226/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0580 - mse: 2.1588e-04 - mae: 0.0072\n",
      "Epoch 00226: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 442s 2s/step - loss: 0.0577 - mse: 2.1469e-04 - mae: 0.0072 - val_loss: 0.0615 - val_mse: 1.2184e-04 - val_mae: 0.0062\n",
      "Epoch 227/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0619 - mse: 2.2190e-04 - mae: 0.0075\n",
      "Epoch 00227: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 416s 2s/step - loss: 0.0616 - mse: 2.2072e-04 - mae: 0.0075 - val_loss: 0.0796 - val_mse: 3.1273e-04 - val_mae: 0.0116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0660 - mse: 2.3661e-04 - mae: 0.0081\n",
      "Epoch 00228: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 446s 2s/step - loss: 0.0657 - mse: 2.3536e-04 - mae: 0.0081 - val_loss: 0.1158 - val_mse: 4.6054e-04 - val_mae: 0.0130\n",
      "Epoch 229/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0682 - mse: 2.4509e-04 - mae: 0.0083\n",
      "Epoch 00229: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 433s 2s/step - loss: 0.0685 - mse: 2.4511e-04 - mae: 0.0083 - val_loss: 0.0030 - val_mse: 1.9237e-06 - val_mae: 9.5195e-04\n",
      "Epoch 230/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0708 - mse: 2.5966e-04 - mae: 0.0086\n",
      "Epoch 00230: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 432s 2s/step - loss: 0.0705 - mse: 2.5830e-04 - mae: 0.0086 - val_loss: 0.0341 - val_mse: 9.7470e-05 - val_mae: 0.0048\n",
      "Epoch 231/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0693 - mse: 2.4763e-04 - mae: 0.0082\n",
      "Epoch 00231: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 431s 2s/step - loss: 0.0692 - mse: 2.4700e-04 - mae: 0.0082 - val_loss: 0.0517 - val_mse: 1.0497e-04 - val_mae: 0.0069\n",
      "Epoch 232/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0644 - mse: 2.2763e-04 - mae: 0.0077\n",
      "Epoch 00232: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 419s 2s/step - loss: 0.0645 - mse: 2.2749e-04 - mae: 0.0077 - val_loss: 0.0139 - val_mse: 2.0984e-05 - val_mae: 0.0029\n",
      "Epoch 233/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0618 - mse: 2.1227e-04 - mae: 0.0075\n",
      "Epoch 00233: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 437s 2s/step - loss: 0.0621 - mse: 2.1330e-04 - mae: 0.0075 - val_loss: 0.0368 - val_mse: 8.4803e-05 - val_mae: 0.0057\n",
      "Epoch 234/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0732 - mse: 2.9457e-04 - mae: 0.0084\n",
      "Epoch 00234: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 435s 2s/step - loss: 0.0729 - mse: 2.9300e-04 - mae: 0.0084 - val_loss: 0.0331 - val_mse: 6.8821e-05 - val_mae: 0.0038\n",
      "Epoch 235/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0685 - mse: 2.5317e-04 - mae: 0.0081\n",
      "Epoch 00235: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 434s 2s/step - loss: 0.0690 - mse: 2.5397e-04 - mae: 0.0081 - val_loss: 0.0804 - val_mse: 3.3145e-04 - val_mae: 0.0105\n",
      "Epoch 236/5000\n",
      "199/200 [============================>.] - ETA: 2s - loss: 0.0609 - mse: 2.1172e-04 - mae: 0.0074\n",
      "Epoch 00236: mse did not improve from 0.00017\n",
      "200/200 [==============================] - 414s 2s/step - loss: 0.0609 - mse: 2.1143e-04 - mae: 0.0074 - val_loss: 0.0354 - val_mse: 7.4498e-05 - val_mae: 0.0045\n",
      "Epoch 237/5000\n",
      "170/200 [========================>.....] - ETA: 1:04 - loss: 0.0677 - mse: 2.4845e-04 - mae: 0.0081"
     ]
    }
   ],
   "source": [
    "cb = [tf.keras.callbacks.ModelCheckpoint('weights/Homogeneous_Poisson_NN/Fourier_24x24modes_big.h5', monitor='mse', verbose=1, save_best_only=True, save_weights_only=True), tf.keras.callbacks.ReduceLROnPlateau(monitor='mse', min_lr = 1e-8, verbose = True, patience = 5)]\n",
    "mod.compile(loss = mod.integral_loss, optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-5), metrics = ['mse', 'mae'])\n",
    "mod.run_eagerly = True\n",
    "mod.fit_generator(generator=dataset_generator_2_rbg(), steps_per_epoch=200, epochs=5000, validation_data=dataset_generator_2_rbg(), validation_steps=3, callbacks=cb)\n",
    "#maybe incorporate delta x and domain size information? via [dx, dy, Lx, Ly] -> dense layers -> einsum into conv filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.save_weights('Homogeneous_Poisson_NN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = dataset_generator_2_rbg()\n",
    "inp, soln = next(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#p_r = np.random.randint(0,soln.shape[0])\n",
    "x, y = np.meshgrid(np.linspace(0, soln.shape[-2]*inp[1][p_r,0], soln.shape[-2]), np.linspace(0, soln.shape[-1]*inp[1][p_r,0], soln.shape[-1]), indexing = 'ij')\n",
    "z = soln[p_r,0,...]\n",
    "z = mod(inp)[p_r,0,...]\n",
    "#z = mod(inp)[p_r,0,...] - tf.cast(soln[p_r,0,...], tf.keras.backend.floatx())\n",
    "z_min, z_max = -np.abs(z).max(), np.abs(z).max()\n",
    "#z_min, z_max = -0.8,0.8\n",
    "fig, ax = plt.subplots()\n",
    "c = ax.pcolormesh(x, y, z, cmap='RdBu', vmin=z_min, vmax=z_max)\n",
    "ax.set_title('analytical')\n",
    "ax.axis([x.min(), x.max(), y.min(), y.max()])\n",
    "fig.colorbar(c, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#p_r = np.random.randint(0,soln.shape[0])\n",
    "x, y = np.meshgrid(np.linspace(0, soln.shape[-2]*inp[1][p_r,0], soln.shape[-2]), np.linspace(0, soln.shape[-1]*inp[1][p_r,0], soln.shape[-1]), indexing = 'ij')\n",
    "z = soln[p_r,0,...]\n",
    "#z = mod(inp)[p_r,0,...]\n",
    "#z = mod(inp)[p_r,0,...] - tf.cast(soln[p_r,0,...], tf.keras.backend.floatx())\n",
    "z_min, z_max = -np.abs(z).max(), np.abs(z).max()\n",
    "#z_min, z_max = -0.8,0.8\n",
    "fig, ax = plt.subplots()\n",
    "c = ax.pcolormesh(x, y, z, cmap='RdBu', vmin=z_min, vmax=z_max)\n",
    "ax.set_title('analytical')\n",
    "ax.axis([x.min(), x.max(), y.min(), y.max()])\n",
    "fig.colorbar(c, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conv_laplacian_loss import conv_laplacian_loss\n",
    "cll = conv_laplacian_loss((ny,nx), dx)\n",
    "#cll(tf.expand_dims(rhs, axis = 1), tf.expand_dims(soln, axis = 1))\n",
    "cll(rhs,soln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod.compile(loss = lf, optimizer = tf.keras.optimizers.Adam(learning_rate = 5e-5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=198252197, shape=(), dtype=float64, numpy=0.6228059530258179>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_max(tf.abs(soln))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([100, 1, 128, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rhs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(Homogeneous_Poisson_NN)\n",
    "mod = Homogeneous_Poisson_NN.Homogeneous_Poisson_NN_2(data_format = 'channels_first', Lp_norm_power=2)\n",
    "nx1 = 100\n",
    "nx2 = 100\n",
    "dx = 0.01\n",
    "Lx1 = 1.0\n",
    "Lx2 = 1.0\n",
    "batch_size = 1\n",
    "mod((tf.random.uniform((10,1,74,83), dtype = tf.keras.backend.floatx()), dx*tf.ones((batch_size,1), dtype = tf.keras.backend.floatx())))\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "x = np.array(np.meshgrid(np.linspace(0, Lx1, nx1),np.linspace(0, Lx2, nx2),indexing = 'xy'), dtype = np.float64).transpose((1,2,0))\n",
    "y = np.sin(x[:,:,0] - x[:,:,1])\n",
    "mod.integral_loss_2(y, tf.zeros((batch_size,1,nx1,nx2), dtype = tf.float64)) #ttf.ones((batch_size,1,nx1,nx2), dtype = tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"homogeneous__poisson_nn_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              multiple                  1360      \n",
      "_________________________________________________________________\n",
      "resnet_block (ResnetBlock)   multiple                  21648     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            multiple                  15504     \n",
      "_________________________________________________________________\n",
      "resnet_block_1 (ResnetBlock) multiple                  61984     \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            multiple                  31128     \n",
      "_________________________________________________________________\n",
      "resnet_block_2 (ResnetBlock) multiple                  93360     \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            multiple                  62240     \n",
      "_________________________________________________________________\n",
      "resnet_block_3 (ResnetBlock) multiple                  165952    \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           multiple                  103720    \n",
      "_________________________________________________________________\n",
      "resnet_block_4 (ResnetBlock) multiple                  259280    \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           multiple                  155568    \n",
      "_________________________________________________________________\n",
      "resnet_block_5 (ResnetBlock) multiple                  373344    \n",
      "_________________________________________________________________\n",
      "spatial_pyramid_pool (Spatia multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  400       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  1616      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  3244250   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  125500    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  513024    \n",
      "=================================================================\n",
      "Total params: 5,239,978\n",
      "Trainable params: 5,239,978\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mod.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3]\n"
     ]
    }
   ],
   "source": [
    "from sympy import factorint\n",
    "f = factorint(36*5)\n",
    "s = []\n",
    "for key in f.keys():\n",
    "    s += [key] * f[key]\n",
    "print(s[1::2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=102308, shape=(), dtype=float64, numpy=5.063595015141838e-07>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = dataset_generator_2()\n",
    "s = next(i)\n",
    "from conv_laplacian_loss import conv_laplacian_loss\n",
    "cll = conv_laplacian_loss((64,64), s[0][1][0,0])\n",
    "cll(s[0][0], s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "36\n",
      "7\n",
      "3\n",
      "1\n",
      "0\n",
      "[2, 5, 2, 2, 2, 0]\n",
      "73\n",
      "36\n",
      "7\n",
      "3\n",
      "1\n",
      "0\n",
      "tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "from sympy import factorint\n",
    "def get_factors(dim):\n",
    "    factorization = factorint(dim)\n",
    "    factors = []\n",
    "    for factor in factorization.keys():\n",
    "        factors += [factor] * factorization[factor]\n",
    "    return factors\n",
    "def pooling_strategy(dim, steps):\n",
    "    print(dim)\n",
    "    factors = sorted(get_factors(dim), reverse=False)\n",
    "    #print(factors)\n",
    "    exponents_excess = len(factors) - steps\n",
    "    if exponents_excess == 0:\n",
    "        return factors\n",
    "    if exponents_excess > 0:\n",
    "        strategy = []\n",
    "        for step in range(steps):\n",
    "            strategy += [int(tf.reduce_prod(factors[step::steps]))]\n",
    "        return strategy\n",
    "    if exponents_excess < 0:\n",
    "        try:\n",
    "            lossy_pooling_size = (dim-1)/int(tf.reduce_prod(get_factors(dim-1)[1:]))\n",
    "            #print(lossy_pooling_size)\n",
    "            lossy_pooling_size = int(lossy_pooling_size)\n",
    "            strategy = [lossy_pooling_size] + pooling_strategy(dim//lossy_pooling_size, steps-1)\n",
    "        except:\n",
    "            strategy = [2] + pooling_strategy(dim//2, steps-1)\n",
    "        return strategy\n",
    "\n",
    "n = 73\n",
    "steps = 6\n",
    "print(pooling_strategy(n,steps))\n",
    "print(tf.reduce_prod(pooling_strategy(n,steps)))\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
